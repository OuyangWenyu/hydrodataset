{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"hydrodataset","text":"<p>A Python package for accessing hydrological datasets with a unified API, optimized for deep learning workflows.</p> <ul> <li>\ud83c\udf0a Unified Interface: Consistent API across 20+ hydrological datasets</li> <li>\u26a1 Fast Access: NetCDF caching for instant data loading</li> <li>\ud83c\udfaf Standardized Variables: Common naming across all datasets</li> <li>\ud83d\udd17 Built on AquaFetch: Powered by the comprehensive AquaFetch backend</li> <li>\ud83d\udcca ML-Ready: Optimized for integration with torchhydro</li> </ul>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Core Philosophy</li> <li>Installation</li> <li>Quick Start</li> <li>Supported Datasets</li> <li>Key Features</li> <li>Project Status</li> <li>Credits</li> </ul>"},{"location":"#core-philosophy","title":"Core Philosophy","text":"<p>This library has been redesigned to serve as a powerful data-adapting layer on top of the AquaFetch package.</p> <p>While <code>AquaFetch</code> handles the complexities of downloading and reading numerous public hydrological datasets, <code>hydrodataset</code> takes the next step: it standardizes this data into a clean, consistent NetCDF (<code>.nc</code>) format. This format is specifically optimized for seamless integration with hydrological modeling libraries like torchhydro.</p> <p>The core workflow is: 1.  Fetch: Use a <code>hydrodataset</code> class for a specific dataset (e.g., <code>CamelsAus</code>). 2.  Standardize: It uses <code>AquaFetch</code> as the primary backend for fetching raw data, while maintaining a consistent, unified interface across all datasets. 3.  Cache: On the first run, <code>hydrodataset</code> processes the data into an <code>xarray.Dataset</code> and saves it as <code>.nc</code> files for timeseries and attributes separately in a specified local directory set in <code>hydro_setting.yml</code> in the user's home directory. 4.  Access: All subsequent data requests are read directly from the fast <code>.nc</code> cache, giving you analysis-ready data instantly.</p>"},{"location":"#installation","title":"Installation","text":"<p>We strongly recommend using a virtual environment to manage dependencies.</p>"},{"location":"#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>We recommend using uv for fast, reliable package and environment management:</p> <pre><code># Install uv if you haven't already\npip install uv\n\n# Install hydrodataset with uv\nuv pip install hydrodataset\n</code></pre> <p>For more advanced usage or to work on the project locally:</p> <pre><code># Clone the repository\ngit clone https://github.com/OuyangWenyu/hydrodataset.git\ncd hydrodataset\n\n# Create virtual environment and install all dependencies\nuv sync --all-extras\n</code></pre> <p>The <code>--all-extras</code> flag installs base dependencies plus all optional dependencies for development and documentation.</p>"},{"location":"#using-pip-alternative","title":"Using pip (Alternative)","text":"<p>If you prefer traditional pip:</p> <pre><code># Create and activate a virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install the package\npip install hydrodataset\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>The primary goal of <code>hydrodataset</code> is to provide a simple, unified API for accessing various hydrological datasets. Here's a complete example showing the core workflow:</p> <p>\u26a0\ufe0f Important Note on First-Time Data Download</p> <p>If you haven't pre-downloaded the datasets, the first access will trigger automatic downloads via AquaFetch, which can take considerable time depending on dataset size:</p> <ul> <li>Small datasets (&lt; 1GB, e.g., CAMELS-CL, CAMELS-COL): ~10-30 minutes</li> <li>Medium datasets (1-5GB, e.g., CAMELS-AUS, CAMELS-BR): ~30 minutes to 1 hour</li> <li>Large datasets (10-20GB, e.g., CAMELS-US, LamaH-CE): ~1-3 hours</li> <li>Very large datasets (&gt; 30GB, e.g., HYSETS): ~3-6 hours or more</li> </ul> <p>Download times vary based on your internet connection speed and server availability.</p> <p>We strongly recommend downloading datasets manually during off-peak hours if possible.</p> <p>After the initial download, all subsequent access will be fast thanks to NetCDF caching.</p>"},{"location":"#basic-example","title":"Basic Example","text":"<pre><code>from hydrodataset.camels_us import CamelsUs\nfrom hydrodataset import SETTING\nimport os\n\n# All datasets are expected to be in the directory defined in your hydro_setting.yml\n# A example of hydro_setting.yml in Windows is like this:\n# local_data_path:\n#   root: 'D:\\data\\waterism' # Update with your root data directory\n#   datasets-origin: 'D:\\data\\waterism\\datasets-origin'\n#   cache: 'D:\\data\\waterism\\cache'\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\n\n# Initialize the dataset class\nds = CamelsUs(data_path)\n\n# 1. Check which features are available\nprint(\"Available static features:\")\nprint(ds.available_static_features)\n\nprint(\"Available dynamic features:\")\nprint(ds.available_dynamic_features)\n\n# 2. Get a list of all basin IDs\nbasin_ids = ds.read_object_ids()\n\n# 3. Read static (attribute) data for a subset of basins\n# Note: We use standardized names like 'area' and 'p_mean'\nattr_data = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:2],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(\"Static attribute data:\")\nprint(attr_data)\n\n# 4. Read dynamic (time-series) data for the same basins\n# Note: We use standardized names like 'streamflow' and 'precipitation'\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:2],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(\"Time-series data:\")\nprint(ts_data)\n</code></pre>"},{"location":"#standardized-variable-names","title":"Standardized Variable Names","text":"<p>A key feature of the new architecture is the use of standardized variable names. This allows you to use the same variable name to fetch the same type of data across different datasets, without needing to know the specific, internal naming scheme of each one.</p> <p>For example, you can get streamflow from both CAMELS-US and CAMELS-AUS using the same variable name:</p> <pre><code># Get streamflow from CAMELS-US\nus_ds.read_ts_xrdataset(gage_id_lst=[\"01013500\"], var_lst=[\"streamflow\"], t_range=[\"1990-01-01\", \"1995-12-31\"])\n\n# Get streamflow from CAMELS-AUS\naus_ds.read_ts_xrdataset(gage_id_lst=[\"A4260522\"], var_lst=[\"streamflow\"], t_range=[\"1990-01-01\", \"1995-12-31\"])\n</code></pre> <p>Similarly, you can use <code>precipitation</code>, <code>temperature_max</code>, etc., across datasets. A comprehensive list of these standardized names and their coverage across all datasets is in progress and will be published soon.</p>"},{"location":"#supported-datasets","title":"Supported Datasets","text":"<p>hydrodataset currently provides unified access to 27 hydrological datasets across the globe. Below is a summary of all supported datasets:</p> Dataset Name Paper Temporal Resolution Data Version Region Basins Time Span Release Date Size BULL Paper / Code Daily Version 3 (code) / Version 2 (data) Spain 484 1951-01-02 to 2021-12-31 2024-03-10 2.2G CAMELS-AUS Paper (V1) / Paper (V2) Daily Version 1 / Version 2 Australia 561 1950-01-01 to 2022-03-31 2024-12 2.1G CAMELS-BR Paper Daily Version 1.2 / Version 1.1 Brazil 897 1980-01-01 to 2024-10-22 2025-03-21 1.4G CAMELS-CH Paper Daily Version 0.9 / Version 0.6 Switzerland 331 1981-01-01 to 2020-12-31 2025-03-14 793.1M CAMELS-CL Paper Daily Dataset Chile 516 1913-02-15 to 2018-03-09 2018-09-28 208M CAMELS-COL Paper Daily Version 2 Colombia 347 1981-05 to 2022-12 2025-05 80.9M CAMELS-DE Paper Daily Version 1.1 / Version 0.1 Germany 1582 1951-01-01 to 2020-12-31 2025-08-07 2.2G CAMELS-DK Paper Daily Version 6.0 Denmark 304 1989-01-02 to 2023-12-31 2025-02-14 1.41G CAMELS-FI Meeting Yearly/Daily Version 1.0.1 Finland 320 1961-01-01 to 2023-12-31 2025-07 382M CAMELS-FR Paper Daily/Monthly/Yearly Version 3.2 / Version 3 France 654 1970-01-01 to 2021-12-31 2025-08-12 364M CAMELS-GB Paper Daily Dataset United Kingdom 671 1970-10-01 to 2015-09-30 2025-05 (new data link) 244M CAMELS-IND Paper Daily Version 2.2 India 472 (242 sufficient flow) 1980-01-01 to 2020-12-31 2025-03-13 529.4M CAMELS-LUX Paper Hourly/Daily Version 1.1 Luxembourg 56 2004-11-01 to 2021-10-31 2024-09-27 1.4G CAMELS-NZ Paper Hourly/Daily Version 2 / Version 1 New Zealand 369 1972-01-01 to 2024-08-02 2025-08-05 4.81G CAMELS-SE Paper Daily Version 1 Sweden 50 1961-2020 2024-02 16.19M CAMELS-US Paper Daily Version 1.2 United States 671 1980-2014 2022-06-24 14.6G CAMELSH-KR - Hourly Version 1 South Korea 178 2000-2019 2025-03-23 3.1G CAMELSH Paper Hourly Version 6 + 3 + 2 United States 9008 1980-2024 2025-08-14 4.2G+3.57G+2.18G Caravan-DK Paper Daily Version 7 / Version 5 Denmark 308 1981-01-02 to 2020-12-31 2025-04-11 521.6M Caravan Paper / Code Daily Version 1.6 Global 16299 1950-2023 2025-05 24.8G EStream Paper / Code Daily (weekly, monthly, yearly available) Version 1.3 / Version 1.1 Europe 17130 1950-01-01 to 2023-06-30 2025-06-30 12.3G GRDC-Caravan Paper Daily Version 0.6 / Version 0.2 Global 5357 1950-2023 2025-05-06 16.4G HYPE Paper (draft) Daily/Monthly/Yearly Version 1.1 Costa Rica 605 1985-01-01 to 2019-12-31 2020-09-14 616.5M HYSETS Paper / Code Daily Dataset (dynamic attributes) North America 14425 1950-01-01 to 2023-12-31 2024-09 41.9G LamaH-CE Paper Daily/Hourly Version 1.0 Central Europe 859 1981-01-01 to 2019-12-31 2021-08-02 16.3G LamaH-Ice Paper Daily/Hourly Version 1.5 / old version Iceland 111 1950-01-01 to 2021-12-31 2025-08-12 9.6G Simbi Paper Daily/Monthly Version 6.0 Haiti 24 1920-01-01 to 2005-12-31 2024-07-02 125M &gt;"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#unified-api-across-all-datasets","title":"\ud83c\udfaf Unified API Across All Datasets","text":"<p>Access any dataset using the same method calls: <pre><code># Same API works for all datasets\nds.read_object_ids()                          # Get basin IDs\nds.read_attr_xrdataset(...)                   # Read attributes\nds.read_ts_xrdataset(...)                     # Read timeseries\n</code></pre></p>"},{"location":"#fast-netcdf-caching","title":"\u26a1 Fast NetCDF Caching","text":"<p>First access processes and caches data as NetCDF files. All subsequent reads are instant: - Timeseries data: <code>{dataset}_timeseries.nc</code> - Attribute data: <code>{dataset}_attributes.nc</code> - Configured via <code>~/hydro_setting.yml</code></p>"},{"location":"#standardized-variable-names_1","title":"\ud83d\udd04 Standardized Variable Names","text":"<p>Use common names across all datasets: - <code>streamflow</code> - River discharge - <code>precipitation</code> - Rainfall - <code>temperature_max</code> / <code>temperature_min</code> - Temperature extremes - <code>potential_evapotranspiration</code> - PET - And many more...</p>"},{"location":"#xarray-integration","title":"\ud83d\udcca xarray Integration","text":"<p>All data returned as <code>xarray.Dataset</code> objects: - Labeled dimensions and coordinates - Built-in metadata and units - Easy slicing, selection, and computation - Compatible with Dask for large datasets</p>"},{"location":"#project-status-future-work","title":"Project Status &amp; Future Work","text":"<p>The new, unified API architecture is currently in active development.</p> <ul> <li>Current Implementation: hydrodataset provides access to 27 hydrological datasets (see the Supported Datasets table above). The new unified architecture based on the <code>HydroDataset</code> base class has been fully implemented and tested for <code>camels_us</code> and <code>camels_aus</code> datasets, which serve as reference implementations.</li> <li>In Progress: We are in the process of migrating all other datasets supported by the library to this new architecture.</li> <li>Release Schedule: We plan to release new versions frequently in the short term as more datasets are integrated. Please check back for updates.</li> </ul>"},{"location":"#credits","title":"Credits","text":"<p>This package was created with Cookiecutter and the giswqs/pypackage project template. The data fetching and reading is now powered by AquaFetch.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/OuyangWenyu/hydrodataset/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>HydroDataset could always use more documentation, whether as part of the official HydroDataset docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/OuyangWenyu/hydrodataset/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up hydrodataset for local development.</p> <ol> <li> <p>Fork the hydrodataset repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/hydrodataset.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv hydrodataset\n$ cd hydrodataset/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 hydrodataset tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/OuyangWenyu/hydrodataset/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-hydrodataset","title":"What is hydrodataset?","text":"<p>hydrodataset is a Python package that provides a unified API for accessing 50+ hydrological datasets. It serves as a data-adapting layer on top of AquaFetch, standardizing diverse datasets into a consistent NetCDF format optimized for deep learning workflows.</p>"},{"location":"faq/#how-is-hydrodataset-different-from-aquafetch","title":"How is hydrodataset different from AquaFetch?","text":"<ul> <li>AquaFetch: Handles downloading and reading raw data from public hydrological datasets</li> <li>hydrodataset: Takes AquaFetch data and standardizes it into a consistent format with unified variable names, NetCDF caching, and ML-ready outputs</li> </ul> <p>Think of AquaFetch as the data fetcher and hydrodataset as the data standardizer.</p>"},{"location":"faq/#which-python-versions-are-supported","title":"Which Python versions are supported?","text":"<p>hydrodataset requires Python 3.10 or higher.</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#where-should-i-create-the-hydro_settingyml-file","title":"Where should I create the <code>hydro_setting.yml</code> file?","text":"<p>The <code>hydro_setting.yml</code> file should be placed in your home directory (<code>~/hydro_setting.yml</code>): - Windows: <code>C:\\Users\\YourUsername\\hydro_setting.yml</code> - Linux/Mac: <code>/home/username/hydro_setting.yml</code> or <code>~/hydro_setting.yml</code></p>"},{"location":"faq/#what-should-be-in-hydro_settingyml","title":"What should be in <code>hydro_setting.yml</code>?","text":"<pre><code>local_data_path:\n  root: 'D:\\data\\waterism'                    # Your root data directory\n  datasets-origin: 'D:\\data\\waterism\\datasets-origin'  # Raw data from AquaFetch\n  cache: 'D:\\data\\waterism\\cache'             # NetCDF cache files\n</code></pre> <p>Adjust paths according to your system and preferences.</p>"},{"location":"faq/#im-getting-an-error-about-missing-hydro_settingyml-what-should-i-do","title":"I'm getting an error about missing <code>hydro_setting.yml</code>. What should I do?","text":"<ol> <li>Create the file in your home directory (see above)</li> <li>Ensure the paths in the file exist and are writable</li> <li>Use absolute paths or proper forward slashes on Windows</li> </ol>"},{"location":"faq/#data-access","title":"Data Access","text":""},{"location":"faq/#how-do-i-know-which-datasets-are-available","title":"How do I know which datasets are available?","text":"<p>Check the Supported Datasets section in the README or browse the API documentation.</p>"},{"location":"faq/#what-are-standardized-variable-names","title":"What are standardized variable names?","text":"<p>Standardized variable names allow you to request the same type of data across different datasets using a common name: - <code>streamflow</code> - works for CAMELS-US, CAMELS-AUS, etc. - <code>precipitation</code> - consistent across all datasets - <code>temperature_max</code> / <code>temperature_min</code> - temperature extremes</p> <p>This eliminates the need to learn each dataset's specific naming conventions.</p>"},{"location":"faq/#how-do-i-see-what-variables-are-available-for-a-dataset","title":"How do I see what variables are available for a dataset?","text":"<pre><code>from hydrodataset.camels_us import CamelsUs\nds = CamelsUs(data_path)\n\nprint(ds.available_static_features)   # Static attributes\nprint(ds.available_dynamic_features)  # Timeseries variables\n</code></pre>"},{"location":"faq/#caching-performance","title":"Caching &amp; Performance","text":""},{"location":"faq/#where-are-the-netcdf-cache-files-stored","title":"Where are the NetCDF cache files stored?","text":"<p>Cache files are stored in the <code>cache</code> directory specified in your <code>hydro_setting.yml</code>: <pre><code>{cache_directory}/{dataset}_timeseries.nc\n{cache_directory}/{dataset}_attributes.nc\n</code></pre></p>"},{"location":"faq/#the-first-data-access-is-slow-is-this-normal","title":"The first data access is slow. Is this normal?","text":"<p>Yes! The first access: 1. Fetches raw data via AquaFetch 2. Standardizes variable names and units 3. Saves to NetCDF cache files</p> <p>All subsequent reads are instant as they load from the fast <code>.nc</code> cache.</p>"},{"location":"faq/#how-do-i-regenerate-the-cache","title":"How do I regenerate the cache?","text":"<p>Simply delete the corresponding <code>.nc</code> files in your cache directory: <pre><code># Delete cache for CAMELS-US\nrm ~/data/cache/camels_us_timeseries.nc\nrm ~/data/cache/camels_us_attributes.nc\n</code></pre></p> <p>Next access will regenerate the cache.</p>"},{"location":"faq/#usage-examples","title":"Usage &amp; Examples","text":""},{"location":"faq/#how-do-i-read-data-for-specific-basins","title":"How do I read data for specific basins?","text":"<pre><code>ds = CamelsUs(data_path)\nbasin_ids = ds.read_object_ids()\n\n# Read for first 5 basins\nattr_data = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\n</code></pre>"},{"location":"faq/#how-do-i-specify-a-time-range","title":"How do I specify a time range?","text":"<pre><code>ts_data = ds.read_ts_xrdataset(\n    gage_id_lst=[\"01013500\"],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],  # YYYY-MM-DD format\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\n</code></pre>"},{"location":"faq/#can-i-use-this-with-deep-learning-frameworks","title":"Can I use this with deep learning frameworks?","text":"<p>Yes! The data is returned as <code>xarray.Dataset</code> objects which can be easily converted to numpy arrays or PyTorch tensors:</p> <pre><code>import torch\n\nts_data = ds.read_ts_xrdataset(...)\nstreamflow_array = ts_data['streamflow'].values  # numpy array\nstreamflow_tensor = torch.from_numpy(streamflow_array)  # PyTorch tensor\n</code></pre> <p>For integration with deep learning workflows, check out torchhydro.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#im-getting-import-errors-what-should-i-check","title":"I'm getting import errors. What should I check?","text":"<ol> <li>Ensure hydrodataset is installed: <code>pip install hydrodataset</code></li> <li>Check your Python version: <code>python --version</code> (must be 3.10+)</li> <li>Try reinstalling: <code>pip install --upgrade hydrodataset</code></li> </ol>"},{"location":"faq/#data-is-not-being-cached-whats-wrong","title":"Data is not being cached. What's wrong?","text":"<ol> <li>Check that the <code>cache</code> path in <code>hydro_setting.yml</code> exists</li> <li>Verify write permissions for the cache directory</li> <li>Check disk space availability</li> </ol>"},{"location":"faq/#im-getting-filenotfounderror-when-reading-data-help","title":"I'm getting \"FileNotFoundError\" when reading data. Help!","text":"<ol> <li>Ensure raw data is downloaded to the <code>datasets-origin</code> directory</li> <li>Some datasets require manual download - check AquaFetch documentation</li> <li>Verify paths in <code>hydro_setting.yml</code> are correct</li> </ol>"},{"location":"faq/#where-can-i-get-help","title":"Where can I get help?","text":"<ul> <li>\ud83d\udcd6 Read the Documentation</li> <li>\ud83d\udc1b Check GitHub Issues</li> <li>\ud83d\udcac Open a new issue with your question</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>We strongly recommend using a virtual environment to manage dependencies and avoid package conflicts.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.10 or higher</li> <li>Operating System: Windows, Linux, or macOS</li> <li>Dependencies: Automatically installed with pip (xarray, netCDF4, pandas, numpy, pint, AquaFetch, etc.)</li> </ul>"},{"location":"installation/#for-users","title":"For Users","text":""},{"location":"installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>We recommend using uv for fast, reliable package and environment management:</p> <pre><code># Install uv if you haven't already\npip install uv\n\n# Install hydrodataset with uv\nuv pip install hydrodataset\n</code></pre> <p>This installs the latest stable release along with all required dependencies, significantly faster than traditional pip.</p>"},{"location":"installation/#using-pip-alternative","title":"Using pip (Alternative)","text":"<p>If you prefer traditional pip:</p> <pre><code># Create and activate a virtual environment (recommended)\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install hydrodataset\npip install hydrodataset\n</code></pre>"},{"location":"installation/#using-conda","title":"Using conda","text":"<p>If you prefer conda, you can install from conda-forge:</p> <pre><code># Create a new conda environment\nconda create -n hydro python=3.10\nconda activate hydro\n\n# Install from conda-forge\nconda install -c conda-forge hydrodataset\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify it works:</p> <pre><code>python -c \"import hydrodataset; print(hydrodataset.__version__)\"\n</code></pre>"},{"location":"installation/#for-developers","title":"For Developers","text":"<p>If you want to contribute to hydrodataset or modify the source code, follow these steps:</p>"},{"location":"installation/#using-uv-recommended_1","title":"Using uv (Recommended)","text":"<p>This project uses uv for fast, reliable package and environment management:</p> <pre><code># Clone the repository\ngit clone https://github.com/OuyangWenyu/hydrodataset.git\ncd hydrodataset\n\n# Install uv if you haven't already\npip install uv\n\n# Create virtual environment and install all dependencies\nuv sync --all-extras\n</code></pre> <p>The <code>--all-extras</code> flag installs: - Base dependencies (required for core functionality) - Development tools (pytest, black, flake8, etc.) - Documentation tools (mkdocs, mkdocstrings, etc.)</p>"},{"location":"installation/#using-pip-alternative_1","title":"Using pip (Alternative)","text":"<p>If you prefer traditional pip:</p> <pre><code># Clone the repository\ngit clone https://github.com/OuyangWenyu/hydrodataset.git\ncd hydrodataset\n\n# Create and activate virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in editable mode with all extras\npip install -e \".[dev,docs,lint]\"\n</code></pre>"},{"location":"installation/#verify-development-installation","title":"Verify Development Installation","text":"<pre><code># Run tests\npytest\n\n# Check code formatting\nblack hydrodataset tests\n\n# Run linting\nflake8 hydrodataset tests\n\n# Build documentation\nmkdocs serve\n</code></pre>"},{"location":"installation/#post-installation-setup","title":"Post-Installation Setup","text":""},{"location":"installation/#create-configuration-file","title":"Create Configuration File","text":"<p>After installation, create a <code>hydro_setting.yml</code> file in your home directory:</p> <p>Windows: <code>C:\\Users\\YourUsername\\hydro_setting.yml</code> Linux/Mac: <code>~/hydro_setting.yml</code></p> <p>Content: <pre><code>local_data_path:\n  root: 'D:\\data\\waterism'                    # Your root data directory\n  datasets-origin: 'D:\\data\\waterism\\datasets-origin'  # Raw data from AquaFetch\n  cache: 'D:\\data\\waterism\\cache'             # NetCDF cache files\n</code></pre></p> <p>Important: Update the paths according to your system. Ensure: - Directories exist or will be created - You have write permissions - Sufficient disk space (cache files can be several GB)</p>"},{"location":"installation/#download-data","title":"Download Data","text":"<p>hydrodataset uses AquaFetch to fetch raw data. Some datasets download automatically, while others require manual download. Check the AquaFetch documentation for dataset-specific instructions.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#pip-installation-fails","title":"pip installation fails","text":"<p>If you encounter errors during installation:</p> <pre><code># Upgrade pip, setuptools, and wheel\npip install --upgrade pip setuptools wheel\n\n# Try installing again\npip install hydrodataset\n</code></pre>"},{"location":"installation/#import-errors-after-installation","title":"Import errors after installation","text":"<pre><code># Ensure you're in the correct environment\nwhich python  # Should point to your virtual environment\n\n# Reinstall\npip uninstall hydrodataset\npip install hydrodataset\n</code></pre>"},{"location":"installation/#aquafetch-dependency-issues","title":"AquaFetch dependency issues","text":"<p>hydrodataset depends on the development version of AquaFetch. If you encounter issues:</p> <pre><code># Install AquaFetch directly from GitHub\npip install git+https://github.com/hyex-research/AquaFetch.git@dev\n</code></pre>"},{"location":"installation/#configuration-file-not-found","title":"Configuration file not found","text":"<p>Error: <code>FileNotFoundError: hydro_setting.yml not found</code></p> <p>Solution: Ensure <code>hydro_setting.yml</code> is in your home directory: <pre><code># Check home directory\necho $HOME  # Linux/Mac\necho %USERPROFILE%  # Windows\n\n# Create file\ntouch ~/hydro_setting.yml  # Linux/Mac\ntype nul &gt; %USERPROFILE%\\hydro_setting.yml  # Windows\n</code></pre></p>"},{"location":"installation/#upgrading","title":"Upgrading","text":""},{"location":"installation/#upgrade-to-latest-version","title":"Upgrade to Latest Version","text":"<pre><code>pip install --upgrade hydrodataset\n</code></pre>"},{"location":"installation/#upgrade-from-conda","title":"Upgrade from conda","text":"<pre><code>conda update -c conda-forge hydrodataset\n</code></pre>"},{"location":"installation/#uninstallation","title":"Uninstallation","text":"<pre><code># Using pip\npip uninstall hydrodataset\n\n# Using conda\nconda remove hydrodataset\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>After installation: 1. \u2705 Create <code>hydro_setting.yml</code> configuration file 2. \ud83d\udcd6 Read the Usage Guide 3. \ud83d\ude80 Try the Quick Start examples 4. \ud83d\udcda Browse the API Documentation</p> <p>If you encounter issues, check the FAQ or open an issue on GitHub.</p> <pre><code>git clone git://github.com/OuyangWenyu/hydrodataset\n</code></pre>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide provides comprehensive examples for using hydrodataset in your projects.</p>"},{"location":"usage/#basic-setup","title":"Basic Setup","text":""},{"location":"usage/#configuration-file","title":"Configuration File","text":"<p>First, ensure you have a <code>hydro_setting.yml</code> file in your home directory:</p> <pre><code>local_data_path:\n  root: 'D:\\data\\waterism'                    # Your root data directory\n  datasets-origin: 'D:\\data\\waterism\\datasets-origin'  # Raw data location\n  cache: 'D:\\data\\waterism\\cache'             # Cache directory for .nc files\n</code></pre>"},{"location":"usage/#import-and-initialize","title":"Import and Initialize","text":"<pre><code>from hydrodataset.camels_us import CamelsUs\nfrom hydrodataset import SETTING\n\n# Access configured paths\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\n\n# Initialize dataset\nds = CamelsUs(data_path)\n</code></pre>"},{"location":"usage/#exploring-available-data","title":"Exploring Available Data","text":""},{"location":"usage/#check-available-features","title":"Check Available Features","text":"<pre><code># List all static (attribute) features\nprint(\"Static features:\")\nprint(ds.available_static_features)\n\n# List all dynamic (timeseries) features\nprint(\"Dynamic features:\")\nprint(ds.available_dynamic_features)\n</code></pre>"},{"location":"usage/#get-basinstation-ids","title":"Get Basin/Station IDs","text":"<pre><code># Get all available basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Total basins: {len(basin_ids)}\")\nprint(f\"First 5 basins: {basin_ids[:5]}\")\n</code></pre>"},{"location":"usage/#reading-data","title":"Reading Data","text":""},{"location":"usage/#read-static-attributes","title":"Read Static Attributes","text":"<p>Static attributes are catchment characteristics that don't change over time:</p> <pre><code># Read specific attributes for selected basins\nattr_data = ds.read_attr_xrdataset(\n    gage_id_lst=[\"01013500\", \"01022500\"],  # Basin IDs\n    var_lst=[\"area\", \"p_mean\", \"elev_mean\"]  # Attribute names\n)\n\nprint(attr_data)\n# Output is an xarray.Dataset with dimensions [basin, variable]\n</code></pre>"},{"location":"usage/#read-timeseries-data","title":"Read Timeseries Data","text":"<p>Timeseries data includes streamflow, precipitation, temperature, etc.:</p> <pre><code># Read timeseries for specific basins and time period\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=[\"01013500\", \"01022500\"],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],  # Start and end dates\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\nprint(ts_data)\n# Output is an xarray.Dataset with dimensions [basin, time, variable]\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"usage/#using-multiple-data-sources","title":"Using Multiple Data Sources","text":"<p>Some datasets provide the same variable from different sources:</p> <pre><code>from hydrodataset.camels_aus import CamelsAus\n\nds_aus = CamelsAus(data_path)\n\n# Use default streamflow source (BOM)\nts_data_bom = ds_aus.read_ts_xrdataset(\n    gage_id_lst=[\"A4260522\"],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\"]  # Uses default source\n)\n\n# Explicitly specify GR4J model output\nts_data_gr4j = ds_aus.read_ts_xrdataset(\n    gage_id_lst=[\"A4260522\"],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[(\"streamflow\", \"gr4j\")]  # Tuple: (variable, source)\n)\n</code></pre>"},{"location":"usage/#reading-all-basins","title":"Reading All Basins","text":"<pre><code># Read all available basins\nall_basin_ids = ds.read_object_ids()\n\n# Read attributes for all basins\nall_attrs = ds.read_attr_xrdataset(\n    gage_id_lst=all_basin_ids,\n    var_lst=[\"area\", \"p_mean\"]\n)\n</code></pre>"},{"location":"usage/#selective-basin-filtering","title":"Selective Basin Filtering","text":"<pre><code>import numpy as np\n\n# Get all basins\nall_basins = ds.read_object_ids()\n\n# Read areas for all basins\nareas = ds.read_attr_xrdataset(\n    gage_id_lst=all_basins,\n    var_lst=[\"area\"]\n)\n\n# Filter basins by area (e.g., &gt; 1000 km\u00b2)\nlarge_basins = all_basins[areas['area'].values &gt; 1000]\n\n# Read timeseries only for large basins\nts_large = ds.read_ts_xrdataset(\n    gage_id_lst=large_basins.tolist(),\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\"streamflow\"]\n)\n</code></pre>"},{"location":"usage/#working-with-xarray-datasets","title":"Working with xarray Datasets","text":"<p>hydrodataset returns data as <code>xarray.Dataset</code> objects, which provide powerful data manipulation capabilities:</p>"},{"location":"usage/#basic-operations","title":"Basic Operations","text":"<pre><code># Select specific basin\nbasin_data = ts_data.sel(basin=\"01013500\")\n\n# Select time range\nperiod_data = ts_data.sel(time=slice(\"1992-01-01\", \"1993-12-31\"))\n\n# Access specific variable\nstreamflow = ts_data[\"streamflow\"]\n\n# Convert to numpy array\nstreamflow_array = streamflow.values\n\n# Convert to pandas DataFrame\nstreamflow_df = streamflow.to_dataframe()\n</code></pre>"},{"location":"usage/#computations","title":"Computations","text":"<pre><code># Calculate mean streamflow for each basin\nmean_flow = ts_data[\"streamflow\"].mean(dim=\"time\")\n\n# Calculate annual maximum streamflow\nannual_max = ts_data[\"streamflow\"].resample(time=\"1Y\").max()\n\n# Correlation between variables\nimport xarray as xr\ncorrelation = xr.corr(\n    ts_data[\"streamflow\"],\n    ts_data[\"precipitation\"],\n    dim=\"time\"\n)\n</code></pre>"},{"location":"usage/#integration-with-deep-learning","title":"Integration with Deep Learning","text":""},{"location":"usage/#converting-to-numpypytorch","title":"Converting to NumPy/PyTorch","text":"<pre><code>import torch\nimport numpy as np\n\n# Get timeseries data\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Convert to numpy\ndata_np = ts_data.to_array().values  # Shape: (variables, basins, time)\n\n# Convert to PyTorch tensor\ndata_tensor = torch.from_numpy(data_np).float()\n\n# Reshape for deep learning (e.g., [batch, time, features])\ndata_dl = data_tensor.permute(1, 2, 0)  # [basins, time, variables]\n</code></pre>"},{"location":"usage/#creating-trainingtest-splits","title":"Creating Training/Test Splits","text":"<pre><code># Split by time\ntrain_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids,\n    t_range=[\"1990-01-01\", \"2005-12-31\"],  # Training period\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\n\ntest_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids,\n    t_range=[\"2006-01-01\", \"2010-12-31\"],  # Test period\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\n</code></pre>"},{"location":"usage/#working-across-multiple-datasets","title":"Working Across Multiple Datasets","text":"<pre><code>from hydrodataset.camels_us import CamelsUs\nfrom hydrodataset.camels_aus import CamelsAus\n\n# Initialize multiple datasets\nds_us = CamelsUs(data_path)\nds_aus = CamelsAus(data_path)\n\n# Read the same variables from different datasets using standardized names\nus_data = ds_us.read_ts_xrdataset(\n    gage_id_lst=[\"01013500\"],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\n\naus_data = ds_aus.read_ts_xrdataset(\n    gage_id_lst=[\"A4260522\"],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\"]  # Same variable names!\n)\n</code></pre>"},{"location":"usage/#cache-management","title":"Cache Management","text":""},{"location":"usage/#understanding-the-cache","title":"Understanding the Cache","text":"<p>hydrodataset caches processed data as NetCDF files for faster subsequent access:</p> <pre><code># First access: slow (processes raw data and creates cache)\nts_data = ds.read_ts_xrdataset(...)  # May take minutes\n\n# Subsequent access: fast (reads from .nc cache)\nts_data = ds.read_ts_xrdataset(...)  # Instant!\n</code></pre>"},{"location":"usage/#regenerating-cache","title":"Regenerating Cache","text":"<p>If you need to regenerate the cache (e.g., after data updates):</p> <pre><code># Navigate to cache directory (from hydro_setting.yml)\ncd ~/data/cache\n\n# Remove cache files for CAMELS-US\nrm camels_us_timeseries.nc\nrm camels_us_attributes.nc\n\n# Next Python access will regenerate the cache\n</code></pre>"},{"location":"usage/#best-practices","title":"Best Practices","text":""},{"location":"usage/#1-start-small-scale-up","title":"1. Start Small, Scale Up","text":"<pre><code># Test with few basins first\ntest_basins = basin_ids[:5]\ntest_data = ds.read_ts_xrdataset(\n    gage_id_lst=test_basins,\n    t_range=[\"2000-01-01\", \"2000-12-31\"],\n    var_lst=[\"streamflow\"]\n)\n\n# Once working, scale to full dataset\n</code></pre>"},{"location":"usage/#2-check-data-availability","title":"2. Check Data Availability","text":"<pre><code># Always check what's available before requesting\nprint(ds.available_dynamic_features)\nprint(ds.available_static_features)\n\n# Check default time range\nprint(ds.default_t_range)\n</code></pre>"},{"location":"usage/#3-handle-missing-data","title":"3. Handle Missing Data","text":"<pre><code># Check for NaN values\nhas_nan = ts_data[\"streamflow\"].isnull().any()\n\n# Fill or drop NaN values\nfilled_data = ts_data.fillna(0)\ndropped_data = ts_data.dropna(dim=\"time\")\n</code></pre>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Reference for detailed method documentation</li> <li>Check the FAQ for common questions and troubleshooting</li> <li>See examples in the repository</li> <li>Integrate with torchhydro for deep learning workflows</li> </ul>"},{"location":"api/bull/","title":"BULL","text":""},{"location":"api/bull/#overview","title":"Overview","text":"<p>BULL is a hydrological dataset for Spain. Spanish hydrological dataset covering catchments in Spain with diverse climate conditions from Mediterranean to Atlantic influences.</p>"},{"location":"api/bull/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Spain</li> <li>Module: <code>hydrodataset.bull</code></li> <li>Class: <code>BULL</code></li> </ul>"},{"location":"api/bull/#features","title":"Features","text":""},{"location":"api/bull/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/bull/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available: - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - And additional variables depending on the dataset</p>"},{"location":"api/bull/#usage","title":"Usage","text":""},{"location":"api/bull/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.bull import BULL\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = BULL(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Check default time range\nprint(f\"Default time range: {ds.default_t_range}\")\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/bull/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"2020-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/bull/#working-with-multiple-data-sources","title":"Working with Multiple Data Sources","text":"<p>If the dataset provides multiple sources for variables:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/bull/#api-reference","title":"API Reference","text":""},{"location":"api/bull/#hydrodataset.bull.BULL","title":"<code>hydrodataset.bull.BULL</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>Bull dataset class extending RainfallRunoff.</p> <p>This class provides access to the Bull dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/bull.py</code> <pre><code>class BULL(HydroDataset):\n    \"\"\"Bull dataset class extending RainfallRunoff.\n\n    This class provides access to the Bull dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize Bull dataset.\n\n        Args:\n            data_path: Path to the Bull data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = Bull(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"bull_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"bull_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1951-01-02\", \"2021-12-31\"]\n\n    _subclass_static_definitions = {\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"streamflow\", \"unit\": \"m^3/s\"},\n                \"q_cms\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"aemet\",\n            \"sources\": {\n                \"aemet\": {\"specific_name\": \"pcp_mm_aemet\", \"unit\": \"mm/day\"},\n                \"bull\": {\"specific_name\": \"pcp_mm_bull\", \"unit\": \"mm/day\"},\n                \"era5land\": {\"specific_name\": \"pcp_mm_era5land\", \"unit\": \"mm/day\"},\n                \"emo1arc\": {\"specific_name\": \"pcp_mm_emo1arc\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"aemet\",\n            \"sources\": {\n                \"aemet\": {\"specific_name\": \"airtemp_c_aemet_max\", \"unit\": \"\u00b0C\"},\n                \"era5land\": {\"specific_name\": \"airtemp_c_era5land_max\", \"unit\": \"\u00b0C\"},\n                \"emo1arc\": {\"specific_name\": \"airtemp_c_emo1arc_max\", \"unit\": \"\u00b0C\"},\n                \"2m\": {\"specific_name\": \"airtemp_c_2m_max\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\"specific_name\": \"dptemp_c_max\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"aemet\",\n            \"sources\": {\n                \"aemet\": {\"specific_name\": \"airtemp_c_aemet_min\", \"unit\": \"\u00b0C\"},\n                \"era5land\": {\"specific_name\": \"airtemp_c_era5land_min\", \"unit\": \"\u00b0C\"},\n                \"emo1arc\": {\"specific_name\": \"airtemp_c_emo1arc_min\", \"unit\": \"\u00b0C\"},\n                \"2m\": {\"specific_name\": \"airtemp_c_2m_min\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\"specific_name\": \"dptemp_c_min\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"aemet\",\n            \"sources\": {\n                \"aemet\": {\"specific_name\": \"airtemp_c_mean_aemet\", \"unit\": \"\u00b0C\"},\n                \"era5land\": {\"specific_name\": \"airtemp_c_mean_era5land\", \"unit\": \"\u00b0C\"},\n                \"emo1arc\": {\"specific_name\": \"airtemp_c_mean_emo1arc\", \"unit\": \"\u00b0C\"},\n                \"2m\": {\"specific_name\": \"airtemp_c_mean_2m\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\"specific_name\": \"dptemp_c_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"aemet\",\n            \"sources\": {\n                \"aemet\": {\"specific_name\": \"pet_mm_aemet\", \"unit\": \"mm/day\"},\n                \"era5land\": {\"specific_name\": \"pet_mm_era5land\", \"unit\": \"mm/day\"},\n                \"emo1arc\": {\"specific_name\": \"pet_mm_emo1arc\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.EVAPORATION: {\n            \"default_source\": \"bull\",\n            \"sources\": {\"bull\": {\"specific_name\": \"pevap_mm\", \"unit\": \"mm/day\"}},\n        },\n        # Snow water equivalent - separate MIN and MAX as independent variables\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"observations\",\n            \"sources\": {\"observations\": {\"specific_name\": \"swe_mm\", \"unit\": \"mm\"}},\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\"observations\": {\"specific_name\": \"swe_mm_min\", \"unit\": \"mm\"}},\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\"observations\": {\"specific_name\": \"swe_mm_max\", \"unit\": \"mm\"}},\n        },\n        # Solar radiation - separate MIN and MAX as independent variables\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"}\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"solrad_wm2_min\", \"unit\": \"W/m^2\"}\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"solrad_wm2_max\", \"unit\": \"W/m^2\"}\n            },\n        },\n        # Thermal radiation - separate MIN and MAX as independent variables\n        StandardVariable.THERMAL_RADIATION: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"thermrad_wm2\", \"unit\": \"W/m^2\"}\n            },\n        },\n        StandardVariable.THERMAL_RADIATION_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"thermrad_wm2_min\", \"unit\": \"W/m^2\"}\n            },\n        },\n        StandardVariable.THERMAL_RADIATION_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"thermrad_wm2_max\", \"unit\": \"W/m^2\"}\n            },\n        },\n        # Surface pressure - separate MIN and MAX as independent variables\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"surface_pressure_mean_bull\",\n                    \"unit\": \"Pa\",\n                }\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"surface_pressure_min_bull\",\n                    \"unit\": \"Pa\",\n                }\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"surface_pressure_max_bull\",\n                    \"unit\": \"Pa\",\n                }\n            },\n        },\n        # U wind speed - separate MIN and MAX as independent variables\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"windspeedu_mps_mean_10m\",\n                    \"unit\": \"m/s\",\n                }\n            },\n        },\n        StandardVariable.U_WIND_SPEED_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"windspeedu_mps_min_10m\",\n                    \"unit\": \"m/s\",\n                }\n            },\n        },\n        StandardVariable.U_WIND_SPEED_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"windspeedu_mps_max_10m\",\n                    \"unit\": \"m/s\",\n                }\n            },\n        },\n        # V wind speed - separate MIN and MAX as independent variables\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"windspeedv_mps_mean_10m\",\n                    \"unit\": \"m/s\",\n                }\n            },\n        },\n        StandardVariable.V_WIND_SPEED_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"windspeedv_mps_min_10m\",\n                    \"unit\": \"m/s\",\n                }\n            },\n        },\n        StandardVariable.V_WIND_SPEED_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"windspeedv_mps_max_10m\",\n                    \"unit\": \"m/s\",\n                }\n            },\n        },\n        # Volumetric soil water layer 1 - separate MIN and MAX as independent variables\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_1_mean_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_1_min_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_1_max_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        # Volumetric soil water layer 2 - separate MIN and MAX as independent variables\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_2_mean_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_2_min_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_2_max_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        # Volumetric soil water layer 3 - separate MIN and MAX as independent variables\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_3_mean_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_3_min_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_3_max_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        # Volumetric soil water layer 4 - separate MIN and MAX as independent variables\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_4_mean_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_4_min_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_4_max_bull\",\n                    \"unit\": \"m^3/m^3\",\n                }\n            },\n        },\n    }\n</code></pre>"},{"location":"api/bull/#hydrodataset.bull.BULL.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/bull/#hydrodataset.bull.BULL.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize Bull dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the Bull data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/bull.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize Bull dataset.\n\n    Args:\n        data_path: Path to the Bull data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = Bull(data_path)\n</code></pre>"},{"location":"api/camels_aus/","title":"CAMELS-AUS","text":""},{"location":"api/camels_aus/#overview","title":"Overview","text":"<p>CAMELS-AUS is the Australia hydrological dataset implementation. Australian CAMELS dataset with comprehensive hydrological and meteorological data for catchments across Australia.</p>"},{"location":"api/camels_aus/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Australia</li> <li>Module: <code>hydrodataset.camels_aus</code></li> <li>Class: <code>CamelsAus</code></li> </ul>"},{"location":"api/camels_aus/#features","title":"Features","text":""},{"location":"api/camels_aus/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_aus/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_aus/#usage","title":"Usage","text":""},{"location":"api/camels_aus/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_aus import CamelsAus\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsAus(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_aus/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_aus/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_aus/#api-reference","title":"API Reference","text":""},{"location":"api/camels_aus/#hydrodataset.camels_aus.CamelsAus","title":"<code>hydrodataset.camels_aus.CamelsAus</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_AUS dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS_AUS dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camels_aus.py</code> <pre><code>class CamelsAus(HydroDataset):\n    \"\"\"CAMELS_AUS dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS_AUS dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_AUS dataset.\n\n        Args:\n            data_path: Path to the CAMELS_AUS data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = CAMELS_AUS(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_aus_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_aus_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1950-01-01\", \"2022-03-31\"]\n\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"bom\",\n            \"sources\": {\n                \"bom\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"mm^3/s\"},\n                \"gr4j\": {\n                    \"specific_name\": \"streamflow_mld_inclinfilled\",\n                    \"unit\": \"ML/day\",\n                },\n                \"depth_based\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"silo_morton\",\n            \"sources\": {\n                \"silo_morton\": {\n                    \"specific_name\": \"aet_mm_silo_morton\",\n                    \"unit\": \"mm/day\",\n                },\n            },\n        },\n        # For PET, AET and ET, the explanation is in the CAMELS_AUS paper, table 2.\n        # table 2 in https://essd.copernicus.org/articles/13/3847/2021/#&amp;gid=1&amp;pid=1\n        # But the specific names are not the same as the ones in the paper but same as the ones renamed by aqua_fetch.\n        # https://github.com/hyex-research/AquaFetch/blob/143c1578fcf18dd6f3a47ba1f2214b089e6e47a9/aqua_fetch/rr/_camels.py#L905C1-L908C93\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"silo_morton\",\n            \"sources\": {\n                \"silo_morton\": {\n                    \"specific_name\": \"et_morton_wet_silo\",\n                    \"unit\": \"mm/day\",\n                },\n                \"silo_morton_point\": {\n                    \"specific_name\": \"aet_mm_silo_morton_point\",\n                    \"unit\": \"mm/day\",\n                },\n                \"silo_short_crop\": {\n                    \"specific_name\": \"aet_mm_silo_short_crop\",\n                    \"unit\": \"mm/day\",\n                },\n                \"silo_tall_crop\": {\n                    \"specific_name\": \"aet_mm_silo_tall_crop\",\n                    \"unit\": \"mm/day\",\n                },\n            },\n        },\n        StandardVariable.EVAPORATION: {\n            \"default_source\": \"silo_morton_lake\",\n            \"sources\": {\n                \"silo_morton_lake\": {\n                    \"specific_name\": \"evap_morton_lake_silo\",\n                    \"unit\": \"mm/day\",\n                },\n                \"silo_pan\": {\"specific_name\": \"evap_pan_silo\", \"unit\": \"mm/day\"},\n                \"silo_syn\": {\"specific_name\": \"evap_syn_silo\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"agcd\",\n            \"sources\": {\n                \"agcd\": {\"specific_name\": \"pcp_mm_agcd\", \"unit\": \"mm/day\"},\n                \"silo\": {\"specific_name\": \"pcp_mm_silo\", \"unit\": \"mm/day\"},\n                # \"agcd_var\": {\n                #     \"specific_name\": \"precipitation_var_agcd\",\n                #     \"unit\": \"mm^2/day^2\",\n                # }, # May not be used\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"agcd\",\n            \"sources\": {\n                \"agcd\": {\"specific_name\": \"airtemp_c_agcd_max\", \"unit\": \"\u00b0C\"},\n                \"silo\": {\"specific_name\": \"airtemp_c_silo_max\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"agcd\",\n            \"sources\": {\n                \"agcd\": {\"specific_name\": \"airtemp_c_agcd_min\", \"unit\": \"\u00b0C\"},\n                \"silo\": {\"specific_name\": \"airtemp_c_silo_min\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"silo\",\n            \"sources\": {\n                \"silo\": {\"specific_name\": \"airtemp_c_mean_silo\", \"unit\": \"\u00b0C\"},\n                \"agcd\": {\"specific_name\": \"airtemp_c_mean_agcd\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.VAPOR_PRESSURE: {\n            \"default_source\": \"agcd_h09\",\n            \"sources\": {\n                \"agcd_h09\": {\"specific_name\": \"vp_hpa_agcd_h09\", \"unit\": \"hPa\"},\n                \"agcd_h15\": {\"specific_name\": \"vp_hpa_agcd_h15\", \"unit\": \"hPa\"},\n                \"silo\": {\"specific_name\": \"vp_hpa_silo\", \"unit\": \"hPa\"},\n                \"silo_deficit\": {\"specific_name\": \"vp_deficit_silo\", \"unit\": \"hPa\"},\n            },\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"silo\",\n            \"sources\": {\n                \"silo_tmax\": {\"specific_name\": \"rh__silo_tmax\", \"unit\": \"%\"},\n                \"silo_tmin\": {\"specific_name\": \"rh__silo_tmin\", \"unit\": \"%\"},\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"silo\",\n            \"sources\": {\"silo\": {\"specific_name\": \"mslp_silo\", \"unit\": \"hPa\"}},\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"silo\",\n            \"sources\": {\"silo\": {\"specific_name\": \"solrad_wm2_silo\", \"unit\": \"MJ/m^2\"}},\n        },\n    }\n</code></pre>"},{"location":"api/camels_aus/#hydrodataset.camels_aus.CamelsAus.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_aus/#hydrodataset.camels_aus.CamelsAus.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_AUS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_AUS data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camels_aus.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_AUS dataset.\n\n    Args:\n        data_path: Path to the CAMELS_AUS data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = CAMELS_AUS(data_path)\n</code></pre>"},{"location":"api/camels_br/","title":"CAMELS-BR","text":""},{"location":"api/camels_br/#overview","title":"Overview","text":"<p>CAMELS-BR is the Brazil hydrological dataset implementation. Brazilian CAMELS dataset containing hydrological data from Brazilian catchments with diverse climate conditions.</p>"},{"location":"api/camels_br/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Brazil</li> <li>Module: <code>hydrodataset.camels_br</code></li> <li>Class: <code>CamelsBr</code></li> </ul>"},{"location":"api/camels_br/#features","title":"Features","text":""},{"location":"api/camels_br/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_br/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_br/#usage","title":"Usage","text":""},{"location":"api/camels_br/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_br import CamelsBr\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsBr(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_br/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_br/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_br/#api-reference","title":"API Reference","text":""},{"location":"api/camels_br/#hydrodataset.camels_br.CamelsBr","title":"<code>hydrodataset.camels_br.CamelsBr</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_BR dataset class.</p> <p>This class uses a custom data reading implementation to support a newer dataset version than the one supported by the underlying aquafetch library. It overrides the download URLs and provides its own parsing and caching logic.</p> Source code in <code>hydrodataset/camels_br.py</code> <pre><code>class CamelsBr(HydroDataset):\n    \"\"\"CAMELS_BR dataset class.\n\n    This class uses a custom data reading implementation to support a newer\n    dataset version than the one supported by the underlying aquafetch library.\n    It overrides the download URLs and provides its own parsing and caching logic.\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_BR dataset.\n\n        Args:\n            data_path: Path to the CAMELS_BR data directory.\n            region: Geographic region identifier (optional, defaults to BR).\n            download: Whether to download data automatically (not used, handled by aqua_fetch).\n        \"\"\"\n        super().__init__(data_path)\n        self.region = \"BR\" if region is None else region\n\n        # Define the new URLs for the latest dataset version\n        new_url = \"https://zenodo.org/records/15025488\"\n        new_urls = {\n            \"01_CAMELS_BR_attributes.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"02_CAMELS_BR_streamflow_all_catchments.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"03_CAMELS_BR_streamflow_selected_catchments.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"04_CAMELS_BR_streamflow_simulated.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"05_CAMELS_BR_precipitation.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"06_CAMELS_BR_actual_evapotransp.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"07_CAMELS_BR_potential_evapotransp.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"08_CAMELS_BR_reference_evapotransp.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"09_CAMELS_BR_temperature.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"10_CAMELS_BR_soil_moisture.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"11_CAMELS_BR_precipitation_ana_gauges.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"12_CAMELS_BR_catchment_boundaries.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"13_CAMELS_BR_gauge_location.zip\": \"https://zenodo.org/records/15025488/files/\",\n            \"CAMELS_BR_readme.txt\": \"https://zenodo.org/records/15025488/files/\",\n        }\n        new_folders = {\n            \"streamflow_mm\": \"03_CAMELS_BR_streamflow_selected_catchments\",\n        }\n\n        def do_nothing(self, *args, **kwargs):\n            pass\n\n        class_attrs = {\n            \"url\": new_url,\n            \"urls\": new_urls,\n            \"folders\": new_folders,\n            \"_maybe_to_netcdf\": do_nothing,\n        }\n        CustomCamelsBr = type(\"CAMELS_BR\", (CAMELS_BR,), class_attrs)\n\n        # Instantiate our custom class to handle downloads, but note that the reading\n        # logic below is custom and does not rely on aquafetch's parsing.\n        self.aqua_fetch = CustomCamelsBr(data_path)\n\n        self.data_source_description = self.set_data_source_describe()\n\n        # Build the variable map for the custom reading logic\n        self._variable_map = self._build_variable_map()\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_br_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_br_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1980-01-01\", \"2024-07-31\"]\n\n    # get the information of features from dataset file\"CAMELS_BR_readme\"\n    _subclass_static_definitions = {\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"m3s\",\n            \"sources\": {\n                \"m3s\": {\"specific_name\": \"streamflow_m3s\", \"unit\": \"m^3/s\"},\n                \"mm\": {\"specific_name\": \"streamflow_mm\", \"unit\": \"mm/day\"},\n                \"simulated\": {\n                    \"specific_name\": \"simulated_streamflow_m3s\",\n                    \"unit\": \"m^3/s\",\n                },\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"p_era5land\", \"unit\": \"mm/day\"},\n                \"mswep\": {\"specific_name\": \"p_mswep\", \"unit\": \"mm/day\"},\n                \"cpc\": {\"specific_name\": \"p_cpc\", \"unit\": \"mm/day\"},\n                \"chirps\": {\"specific_name\": \"p_chirps\", \"unit\": \"mm/day\"},\n                \"brdwgd\": {\"specific_name\": \"p_brdwgd\", \"unit\": \"mm/day\"},\n                \"ana_gauges\": {\"specific_name\": \"p_ana_gauges\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"aet_era5land\", \"unit\": \"mm/day\"},\n                \"gleam\": {\"specific_name\": \"aet_gleam\", \"unit\": \"mm/day\"},\n                \"mgb\": {\"specific_name\": \"aet_mgb\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"pet_era5land\", \"unit\": \"mm/day\"},\n                \"gleam\": {\"specific_name\": \"pet_gleam\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"tmax_era5land\", \"unit\": \"\u00b0C\"},\n                \"cpc\": {\"specific_name\": \"tmax_cpc\", \"unit\": \"\u00b0C\"},\n                \"brdwgd\": {\"specific_name\": \"tmax_brdwgd\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"tmin_era5land\", \"unit\": \"\u00b0C\"},\n                \"cpc\": {\"specific_name\": \"tmin_cpc\", \"unit\": \"\u00b0C\"},\n                \"brdwgd\": {\"specific_name\": \"tmin_brdwgd\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"tmean_era5land\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.SOIL_MOISTURE: {\n            \"default_source\": \"surface_gleam\",\n            \"sources\": {\n                \"surface_gleam\": {\n                    \"specific_name\": \"sm_surface_gleam\",\n                    \"unit\": \"m^3/m^3\",\n                },\n                \"rootzone_gleam\": {\n                    \"specific_name\": \"sm_rootzone_gleam\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"sm_layer1_era5land\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"sm_layer2_era5land\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"sm_layer3_era5land\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4: {\n            \"default_source\": \"era5land\",\n            \"sources\": {\n                \"era5land\": {\"specific_name\": \"sm_layer4_era5land\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n    }\n\n    def _build_variable_map(self):\n        \"\"\"\n        Scans all time-series directories to build a map from each variable\n        to its parent directory path. This is done once at initialization.\n        \"\"\"\n        variable_map = {}\n        all_ts_dirs = (\n            self.data_source_description[\"CAMELS_FORCING_DIR\"]\n            + self.data_source_description[\"CAMELS_FLOW_DIR\"]\n        )\n\n        try:\n            sample_gage_id = self.read_object_ids()[0]\n        except IndexError:\n            # If there are no gages, we can't build the map.\n            return {}\n\n        for ts_dir in all_ts_dirs:\n            base_name = str(ts_dir).split(os.sep)[-1][13:]\n            # Handle special case for precipitation_ana_gauges\n            if base_name == \"precipitation_ana_gauges\":\n                variable_map[\"p_ana_gauges\"] = str(ts_dir)\n                continue\n\n            # Find a sample file to read its header\n            try:\n                files_for_gage = [\n                    f for f in os.listdir(ts_dir) if f.startswith(sample_gage_id)\n                ]\n                if not files_for_gage:\n                    continue\n                sample_file_path = os.path.join(ts_dir, files_for_gage[0])\n                df_header = pd.read_csv(sample_file_path, sep=r\"\\s+\", nrows=0)\n                internal_vars = df_header.columns[3:]\n                for var in internal_vars:\n                    if var in variable_map:\n                        logging.warning(\n                            f\"Duplicate variable '{var}' found. Overwriting mapping.\"\n                        )\n                    variable_map[var] = str(ts_dir)\n            except (FileNotFoundError, IndexError, pd.errors.EmptyDataError):\n                # If we can't read a sample file, just skip this directory\n                logging.warning(\n                    f\"Could not read sample file in {ts_dir} to map variables.\"\n                )\n                continue\n        return variable_map\n\n    def set_data_source_describe(self) -&gt; collections.OrderedDict:\n        \"\"\"\n        the files in the dataset and their location in file system\n\n        Returns\n        -------\n        collections.OrderedDict\n            the description for a CAMELS-BR dataset\n        \"\"\"\n        camels_db = self.data_source_dir.joinpath(\"CAMELS_BR\")\n\n        # attr\n        attr_dir = camels_db.joinpath(\n            \"01_CAMELS_BR_attributes\", \"01_CAMELS_BR_attributes\"\n        )\n        # we don't need the location attr file\n        attr_key_lst = [\n            \"climate\",\n            \"geology\",\n            \"human_intervention\",\n            \"hydrology\",\n            \"land_cover\",\n            \"quality_check\",\n            \"soil\",\n            \"topography\",\n        ]\n        # id and name, there are two types stations in CAMELS_BR, and we only chose the 897-stations version\n        gauge_id_file = attr_dir.joinpath(\"camels_br_topography.txt\")\n        # shp file of basins\n        camels_shp_file = camels_db.joinpath(\n            \"12_CAMELS_BR_catchment_boundaries\",\n            \"12_CAMELS_BR_catchment_boundaries\",\n            \"camels_br_catchments.gpkg\",\n        )\n        # config of flow data\n        flow_dir = camels_db.joinpath(\n            \"03_CAMELS_BR_streamflow_selected_catchments\",\n            \"03_CAMELS_BR_streamflow_selected_catchments\",\n        )\n        flow_dir_simulated = camels_db.joinpath(\n            \"04_CAMELS_BR_streamflow_simulated\",\n            \"04_CAMELS_BR_streamflow_simulated\",\n        )\n\n        # forcing\n        forcing_dir_precipitation = camels_db.joinpath(\n            \"05_CAMELS_BR_precipitation\",\n            \"05_CAMELS_BR_precipitation\",\n        )\n        forcing_dir_evapotransp = camels_db.joinpath(\n            \"06_CAMELS_BR_actual_evapotransp\",\n            \"06_CAMELS_BR_actual_evapotransp\",\n        )\n        forcing_dir_potential_evapotransp = camels_db.joinpath(\n            \"07_CAMELS_BR_potential_evapotransp\",\n            \"07_CAMELS_BR_potential_evapotransp\",\n        )\n        forcing_dir_reference_evap = camels_db.joinpath(\n            \"08_CAMELS_BR_reference_evapotransp\",\n            \"08_CAMELS_BR_reference_evapotransp\",\n        )\n        forcing_dir_temperature = camels_db.joinpath(\n            \"09_CAMELS_BR_temperature\",\n            \"09_CAMELS_BR_temperature\",\n        )\n        forcing_dir_soilmoisture = camels_db.joinpath(\n            \"10_CAMELS_BR_soil_moisture\",\n            \"10_CAMELS_BR_soil_moisture\",\n        )\n        forcing_dir_precipitation_ana_gauges = camels_db.joinpath(\n            \"11_CAMELS_BR_precipitation_ana_gauges\",\n            \"11_CAMELS_BR_precipitation_ana_gauges\",\n        )\n        base_url = \"https://zenodo.org/records/15025488\"\n        # NOTE: Now the CAMELS_BR is not supported by AquaFetch,\n        # Here, we only add download urls to be used for unzipping the dataset.\n        download_url_lst = [\n            f\"{base_url}/files/01_CAMELS_BR_attributes.zip\",\n            f\"{base_url}/files/02_CAMELS_BR_streamflow_all_catchments.zip\",\n            f\"{base_url}/files/03_CAMELS_BR_streamflow_selected_catchments.zip\",\n            f\"{base_url}/files/04_CAMELS_BR_streamflow_simulated.zip\",\n            f\"{base_url}/files/05_CAMELS_BR_precipitation.zip\",\n            f\"{base_url}/files/06_CAMELS_BR_actual_evapotransp.zip\",\n            f\"{base_url}/files/07_CAMELS_BR_potential_evapotransp.zip\",\n            f\"{base_url}/files/08_CAMELS_BR_reference_evapotransp.zip\",\n            f\"{base_url}/files/09_CAMELS_BR_temperature.zip\",\n            f\"{base_url}/files/10_CAMELS_BR_soil_moisture.zip\",\n            f\"{base_url}/files/11_CAMELS_BR_precipitation_ana_gauges.zip\",\n            f\"{base_url}/files/12_CAMELS_BR_catchment_boundaries.zip\",\n            f\"{base_url}/files/13_CAMELS_BR_gauge_location.zip\",\n            f\"{base_url}/files/CAMELS_BR_readme.txt\",\n        ]\n        return collections.OrderedDict(\n            CAMELS_DIR=camels_db,\n            CAMELS_FLOW_DIR=[\n                flow_dir,\n                flow_dir_simulated,\n            ],\n            CAMELS_FORCING_DIR=[\n                forcing_dir_precipitation,\n                forcing_dir_precipitation_ana_gauges,\n                forcing_dir_evapotransp,\n                forcing_dir_potential_evapotransp,\n                forcing_dir_reference_evap,\n                forcing_dir_temperature,\n                forcing_dir_soilmoisture,\n            ],\n            CAMELS_ATTR_DIR=attr_dir,\n            CAMELS_ATTR_KEY_LST=attr_key_lst,\n            CAMELS_GAUGE_FILE=gauge_id_file,\n            CAMELS_BASINS_SHP_FILE=camels_shp_file,\n            CAMELS_DOWNLOAD_URL_LST=download_url_lst,\n        )\n\n    def _get_constant_cols_some(self, data_folder, prefix, postfix, sep):\n        var_dict = {}\n        var_lst = []\n        key_lst = self.data_source_description[\"CAMELS_ATTR_KEY_LST\"]\n        for key in key_lst:\n            data_file = os.path.join(data_folder, prefix + key + postfix)\n            data_temp = pd.read_csv(data_file, sep=sep)\n            var_lst_temp = list(data_temp.columns[1:])\n            var_dict[key] = var_lst_temp\n            var_lst.extend(var_lst_temp)\n        return np.array(var_lst)\n\n    def _static_features(self) -&gt; list:\n        \"\"\"\n        all readable attrs in CAMELS-BR\n\n        Returns\n        -------\n        list\n            attribute types\n        \"\"\"\n        data_folder = self.data_source_description[\"CAMELS_ATTR_DIR\"]\n        return self._get_constant_cols_some(data_folder, \"camels_br_\", \".txt\", \"\\s+\")\n\n    def _dynamic_features(self):\n        \"Return all available time series variables.\"\n        return np.array(list(self._variable_map.keys()))\n\n    def _find_file_for_gage(self, directory, gage_id):\n        \"\"\"Finds the data file for a specific gage in a given directory.\"\"\"\n        if not os.path.isdir(directory):\n            return None\n        # Find any file in the directory for our sample gage\n        gage_files = [f for f in os.listdir(directory) if f.startswith(gage_id)]\n        if not gage_files:\n            return None\n        return os.path.join(directory, gage_files[0])\n\n    def read_relevant_cols(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        forcing_type: str = None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Read time series data for a list of variables, optimizing I/O by grouping variables by file.\n\n        Parameters\n        ----------\n        gage_id_lst\n            station ids\n        t_range\n            the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n        var_lst\n            time series variable types (e.g., [\"p_chirps\", \"t_mean\"])\n        Returns\n        -------\n        np.array\n            time series data\n        \"\"\"\n        if var_lst is None or len(var_lst) == 0:\n            return np.array([])\n        t_range_list = pd.date_range(start=t_range[0], end=t_range[1], freq=\"D\").values\n        nt = t_range_list.shape[0]\n        x = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n\n        for k, gage_id in enumerate(tqdm(gage_id_lst, desc=\"Reading basins\")):\n            # Group variables by the directory they belong to for the current basin\n            dir_to_vars_map = {}\n            for i, var in enumerate(var_lst):\n                directory = self._variable_map.get(var)\n                if not directory:\n                    logging.warning(f\"Could not find directory for variable: {var}\")\n                    continue\n                if directory not in dir_to_vars_map:\n                    dir_to_vars_map[directory] = []\n                dir_to_vars_map[directory].append((var, i))\n\n            # For this basin, iterate through directories, reading each file only once\n            for directory, vars_in_dir in dir_to_vars_map.items():\n                file_path = self._find_file_for_gage(directory, gage_id)\n                if not file_path:\n                    logging.warning(f\"No file found for gage {gage_id} in {directory}\")\n                    continue\n\n                try:\n                    data_temp = pd.read_csv(file_path, sep=r\"\\s+\")\n                except (FileNotFoundError, pd.errors.EmptyDataError):\n                    logging.warning(f\"Could not read or empty file: {file_path}\")\n                    continue\n\n                # Intersect time once per file\n                df_date = data_temp[[\"year\", \"month\", \"day\"]]\n                date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n                [c, file_indices, target_indices] = np.intersect1d(\n                    date, t_range_list, return_indices=True\n                )\n\n                # For each variable belonging to this file, extract its column\n                for var, var_index_in_x in vars_in_dir:\n                    if var in data_temp.columns:\n                        obs = data_temp[var].values\n                    else:  # Fallback for special cases like precipitation_ana_gauges\n                        obs = data_temp.iloc[:, 3].values\n\n                    # Convert to float to handle NaN values properly\n                    obs = obs.astype(float)\n                    obs[obs &lt; 0] = np.nan\n                    x[k, target_indices, var_index_in_x] = obs[file_indices]\n        return x\n\n    def _read_ts_dynamic(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        **kwargs,\n    ):\n        \"\"\"Helper function to dynamically read time series data without caching.\"\"\"\n        if var_lst is None:\n            return None\n        # read_relevant_cols is now the unified reader for any time-series variables\n        all_ts_data = self.read_relevant_cols(gage_id_lst, t_range, var_lst, **kwargs)\n\n        times = pd.date_range(start=t_range[0], end=t_range[1], freq=\"D\").values\n        data_vars = {}\n        for i, var in enumerate(var_lst):\n            data_vars[var] = ((\"basin\", \"time\"), all_ts_data[:, :, i])\n\n        ds = xr.Dataset(data_vars, coords={\"basin\": gage_id_lst, \"time\": times})\n        return ds\n\n    def cache_timeseries_xrdataset(self, **kwargs):\n        \"\"\"Read time series data from cache or generate it and return an xarray.Dataset\n        TODO: For p_ana_gauges, they are rainfall gauges, we need to calculate basin-averaged precipitation from them,\n        if we want to use them as basin-averaged precipitation.\n\n        \"\"\"\n        print(\"Creating cache for CAMELS-BR time series data... This may take a while.\")\n        all_basins = self.read_object_ids()\n        all_vars = self._dynamic_features()\n        # Define a canonical time range for the cache, e.g., 1980-2020\n        canonical_t_range = self.default_t_range\n        ds_full = self._read_ts_dynamic(\n            gage_id_lst=all_basins,\n            t_range=canonical_t_range,\n            var_lst=all_vars,\n            **kwargs,\n        )\n        ds_full.to_netcdf(self.cache_dir.joinpath(self._timeseries_cache_filename))\n</code></pre>"},{"location":"api/camels_br/#hydrodataset.camels_br.CamelsBr.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_br/#hydrodataset.camels_br.CamelsBr.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_BR dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_BR data directory.</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional, defaults to BR).</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (not used, handled by aqua_fetch).</p> <code>False</code> Source code in <code>hydrodataset/camels_br.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_BR dataset.\n\n    Args:\n        data_path: Path to the CAMELS_BR data directory.\n        region: Geographic region identifier (optional, defaults to BR).\n        download: Whether to download data automatically (not used, handled by aqua_fetch).\n    \"\"\"\n    super().__init__(data_path)\n    self.region = \"BR\" if region is None else region\n\n    # Define the new URLs for the latest dataset version\n    new_url = \"https://zenodo.org/records/15025488\"\n    new_urls = {\n        \"01_CAMELS_BR_attributes.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"02_CAMELS_BR_streamflow_all_catchments.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"03_CAMELS_BR_streamflow_selected_catchments.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"04_CAMELS_BR_streamflow_simulated.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"05_CAMELS_BR_precipitation.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"06_CAMELS_BR_actual_evapotransp.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"07_CAMELS_BR_potential_evapotransp.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"08_CAMELS_BR_reference_evapotransp.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"09_CAMELS_BR_temperature.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"10_CAMELS_BR_soil_moisture.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"11_CAMELS_BR_precipitation_ana_gauges.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"12_CAMELS_BR_catchment_boundaries.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"13_CAMELS_BR_gauge_location.zip\": \"https://zenodo.org/records/15025488/files/\",\n        \"CAMELS_BR_readme.txt\": \"https://zenodo.org/records/15025488/files/\",\n    }\n    new_folders = {\n        \"streamflow_mm\": \"03_CAMELS_BR_streamflow_selected_catchments\",\n    }\n\n    def do_nothing(self, *args, **kwargs):\n        pass\n\n    class_attrs = {\n        \"url\": new_url,\n        \"urls\": new_urls,\n        \"folders\": new_folders,\n        \"_maybe_to_netcdf\": do_nothing,\n    }\n    CustomCamelsBr = type(\"CAMELS_BR\", (CAMELS_BR,), class_attrs)\n\n    # Instantiate our custom class to handle downloads, but note that the reading\n    # logic below is custom and does not rely on aquafetch's parsing.\n    self.aqua_fetch = CustomCamelsBr(data_path)\n\n    self.data_source_description = self.set_data_source_describe()\n\n    # Build the variable map for the custom reading logic\n    self._variable_map = self._build_variable_map()\n</code></pre>"},{"location":"api/camels_ch/","title":"CAMELS-CH","text":""},{"location":"api/camels_ch/#overview","title":"Overview","text":"<p>CAMELS-CH is the Switzerland hydrological dataset implementation. Swiss CAMELS dataset covering Alpine and pre-Alpine catchments in Switzerland.</p>"},{"location":"api/camels_ch/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Switzerland</li> <li>Module: <code>hydrodataset.camels_ch</code></li> <li>Class: <code>CamelsCh</code></li> </ul>"},{"location":"api/camels_ch/#features","title":"Features","text":""},{"location":"api/camels_ch/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_ch/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_ch/#usage","title":"Usage","text":""},{"location":"api/camels_ch/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_ch import CamelsCh\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsCh(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_ch/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_ch/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_ch/#api-reference","title":"API Reference","text":""},{"location":"api/camels_ch/#hydrodataset.camels_ch.CamelsCh","title":"<code>hydrodataset.camels_ch.CamelsCh</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS-CH dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS-CH dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>This class overrides the default CSV reading methods from AquaFetch to use comma separators instead of semicolon separators, and updates the download URL to the latest Zenodo record.</p> Source code in <code>hydrodataset/camels_ch.py</code> <pre><code>class CamelsCh(HydroDataset):\n    \"\"\"CAMELS-CH dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS-CH dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    This class overrides the default CSV reading methods from AquaFetch to use\n    comma separators instead of semicolon separators, and updates the download\n    URL to the latest Zenodo record.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        region: Optional[str] = None,\n        download: bool = False,\n        version: str = \"v0.9\",\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS-CH dataset with custom URL and CSV reading methods.\n\n        Args:\n            data_path: Path to the CAMELS_CH data directory.\n            region: Geographic region identifier (optional).\n            download: Whether to download data automatically.\n            version: Dataset version (default: v0.9).\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.version = version\n\n        # Define updated URL for the new dataset version\n        new_url = \"https://zenodo.org/records/15025258\"\n\n        # Create custom methods that override the AquaFetch CSV reading\n        def custom_climate_attrs(self) -&gt; pd.DataFrame:\n            \"\"\"Returns 14 climate attributes of catchments with comma separator.\"\"\"\n            df = pd.read_csv(\n                self.clim_attr_path,\n                skiprows=1,\n                sep=\",\",  # Changed from ';' to ','\n                index_col=\"gauge_id\",\n                dtype={\n                    \"gauge_id\": str,\n                    \"p_mean\": float,\n                    \"aridity\": float,\n                    \"pet_mean\": float,\n                    \"p_seasonality\": float,\n                    \"frac_snow\": float,\n                    \"high_prec_freq\": float,\n                    \"high_prec_dur\": float,\n                    \"high_prec_timing\": str,\n                    \"low_prec_timing\": str,\n                },\n            )\n            return df\n\n        def custom_geol_attrs(self) -&gt; pd.DataFrame:\n            \"\"\"15 geological features with comma separator.\"\"\"\n            df = pd.read_csv(\n                self.geol_attr_path,\n                skiprows=1,\n                sep=\",\",  # Changed from ';' to ','\n                index_col=\"gauge_id\",\n                dtype=np.float32,\n            )\n            df.index = df.index.astype(int).astype(str)\n            return df\n\n        def custom_glacier_attrs(self) -&gt; pd.DataFrame:\n            \"\"\"Returns a dataframe with glacier attributes using comma separator.\"\"\"\n            df = pd.read_csv(\n                self.glacier_attr_path,\n                sep=\",\",  # Changed from ';' to ','\n                skiprows=1,\n                index_col=\"gauge_id\",\n                dtype=np.float32,\n            )\n            df.index = df.index.astype(int).astype(str)\n            return df\n\n        def custom_human_inf_attrs(self) -&gt; pd.DataFrame:\n            \"\"\"14 anthropogenic factors with comma separator.\"\"\"\n            df = pd.read_csv(\n                self.hum_inf_attr_path,\n                skiprows=1,\n                sep=\",\",  # Changed from ';' to ','\n                index_col=\"gauge_id\",\n                dtype={\n                    \"gauge_id\": str,\n                    \"n_inhabitants\": int,\n                    \"dens_inhabitants\": float,\n                    \"hp_count\": int,\n                    \"hp_qturb\": float,\n                    \"hp_inst_turb\": float,\n                    \"hp_max_power\": float,\n                    \"num_reservoir\": int,\n                    \"reservoir_cap\": float,\n                    \"reservoir_he\": float,\n                    \"reservoir_fs\": float,\n                    \"reservoir_irr\": float,\n                    \"reservoir_nousedata\": float,\n                },\n            )\n            return df\n\n        def custom_hydrogeol_attrs(self) -&gt; pd.DataFrame:\n            \"\"\"10 hydrogeological factors with comma separator.\"\"\"\n            df = pd.read_csv(\n                self.hydrogeol_attr_path,\n                skiprows=1,\n                sep=\",\",  # Changed from ';' to ','\n                index_col=\"gauge_id\",\n                dtype=float,\n            )\n            df.index = df.index.astype(int).astype(str)\n            return df\n\n        def custom_hydrol_attrs(self) -&gt; pd.DataFrame:\n            \"\"\"14 hydrological parameters + 2 useful infos with comma separator.\"\"\"\n            df = pd.read_csv(\n                self.hydrol_attr_path,\n                skiprows=1,\n                sep=\",\",  # Changed from ';' to ','\n                index_col=\"gauge_id\",\n                dtype={\n                    \"gauge_id\": str,\n                    \"sign_number_of_years\": int,\n                    \"q_mean\": float,\n                    \"runoff_ratio\": float,\n                    \"stream_elas\": float,\n                    \"slope_fdc\": float,\n                    \"baseflow_index_landson\": float,\n                    \"hfd_mean\": float,\n                    \"Q5\": float,\n                    \"Q95\": float,\n                    \"high_q_freq\": float,\n                    \"high_q_dur\": float,\n                    \"low_q_freq\": float,\n                },\n            )\n            return df\n\n        def custom_landcolover_attrs(self) -&gt; pd.DataFrame:\n            \"\"\"13 landcover parameters with comma separator.\"\"\"\n            return pd.read_csv(\n                self.lc_attr_path,\n                skiprows=1,\n                sep=\",\",  # Changed from ';' to ','\n                index_col=\"gauge_id\",\n                dtype={\n                    \"gauge_id\": str,\n                    \"crop_perc\": float,\n                    \"grass_perc\": float,\n                    \"scrub_perc\": float,\n                    \"dwood_perc\": float,\n                    \"mixed_wood_perc\": float,\n                    \"ewood_perc\": float,\n                    \"wetlands_perc\": float,\n                    \"inwater_perc\": float,\n                    \"ice_perc\": float,\n                    \"loose_rock_perc\": float,\n                    \"rock_perc\": float,\n                    \"urban_perc\": float,\n                    \"dom_land_cover\": str,\n                },\n            )\n\n        def custom_soil_attrs(self) -&gt; pd.DataFrame:\n            \"\"\"80 soil parameters with comma separator.\"\"\"\n            df = pd.read_csv(\n                self.soil_attr_path,\n                skiprows=1,\n                sep=\",\",  # Changed from ';' to ','\n                index_col=\"gauge_id\",\n            )\n            df.index = df.index.astype(int).astype(str)\n            return df\n\n        def custom_topo_attrs(self) -&gt; pd.DataFrame:\n            \"\"\"Topographic parameters with comma separator.\"\"\"\n            df = pd.read_csv(\n                self.topo_attr_path,\n                skiprows=1,\n                sep=\",\",  # Changed from ';' to ','\n                index_col=\"gauge_id\",\n                encoding=\"unicode_escape\",\n            )\n            df.index = df.index.astype(int).astype(str)\n            return df\n\n        def custom_static_data(self) -&gt; pd.DataFrame:\n            \"\"\"Concatenate all static attributes without supp_geol_attrs.\"\"\"\n            df = pd.concat(\n                [\n                    self.climate_attrs(),\n                    self.geol_attrs(),\n                    # self.supp_geol_attrs(),  # Removed as requested\n                    self.glacier_attrs(),\n                    self.human_inf_attrs(),\n                    self.hydrogeol_attrs(),\n                    self.hydrol_attrs(),\n                    self.landcolover_attrs(),\n                    self.soil_attrs(),\n                    self.topo_attrs(),\n                ],\n                axis=1,\n            )\n            df.index = df.index.astype(str)\n            df.rename(columns=self.static_map, inplace=True)\n            return df\n\n        def custom_read_stn_dyn(self, station: str) -&gt; pd.DataFrame:\n            \"\"\"Reads daily dynamic data for one catchment with comma separator.\"\"\"\n            df = pd.read_csv(\n                os.path.join(self.dynamic_path, f\"CAMELS_CH_obs_based_{station}.csv\"),\n                sep=\",\",  # Changed from ';' to ','\n                index_col=\"date\",\n                parse_dates=True,\n                dtype=np.float32,\n            )\n            df.rename(columns=self.dyn_map, inplace=True)\n            return df\n\n        def custom_stations(self) -&gt; list:\n            \"\"\"Returns station ids for catchments with comma separator.\"\"\"\n            stns = pd.read_csv(\n                self.glacier_attr_path, sep=\",\", skiprows=1  # Changed from ';' to ','\n            )[\"gauge_id\"].values.tolist()\n            return [str(stn) for stn in stns]\n\n        def custom_dynamic_path(self):\n            \"\"\"Return the correct path for dynamic data (timeseries not time_series).\"\"\"\n            return os.path.join(self.camels_path, \"timeseries\", \"observation_based\")\n\n        def do_nothing(self, *args, **kwargs):\n            \"\"\"Placeholder method to disable certain operations.\"\"\"\n            pass\n\n        # Create class attributes dictionary to override CAMELS_CH methods\n        class_attrs = {\n            \"url\": new_url,\n            \"dynamic_path\": property(custom_dynamic_path),\n            \"climate_attrs\": custom_climate_attrs,\n            \"geol_attrs\": custom_geol_attrs,\n            \"glacier_attrs\": custom_glacier_attrs,\n            \"human_inf_attrs\": custom_human_inf_attrs,\n            \"hydrogeol_attrs\": custom_hydrogeol_attrs,\n            \"hydrol_attrs\": custom_hydrol_attrs,\n            \"landcolover_attrs\": custom_landcolover_attrs,\n            \"soil_attrs\": custom_soil_attrs,\n            \"topo_attrs\": custom_topo_attrs,\n            \"_static_data\": custom_static_data,\n            \"_read_stn_dyn\": custom_read_stn_dyn,\n            \"stations\": custom_stations,\n            \"_maybe_to_netcdf\": do_nothing,\n        }\n\n        # Create custom CAMELS_CH class with overridden methods\n        CustomCamelsCh = type(\"CAMELS_CH\", (CAMELS_CH,), class_attrs)\n\n        try:\n            self.aqua_fetch = CustomCamelsCh(data_path)\n        except Exception as e:\n            print(e)\n            check_zip_extract = False\n            # The zip files that should be downloaded for CAMELS-CH\n            zip_files = [\"camels_ch.zip\", \"Caravan_extension_CH.zip\"]\n            for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n                # The extracted directory name (without .zip extension)\n                extracted_dir = self.data_source_dir.joinpath(\n                    \"CAMELS_CH\", filename[:-4]\n                )\n                if not extracted_dir.exists():\n                    check_zip_extract = True\n                    break\n            if check_zip_extract:\n                from hydroutils import hydro_file\n\n                hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_CH\"))\n            self.aqua_fetch = CustomCamelsCh(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_ch_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_ch_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1981-01-01\", \"2020-12-31\"]\n\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n    # get the information of features from dataset file\"camels_ch_data_description.pdf\"\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"vol\",\n            \"sources\": {\n                \"vol\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n                \"spec\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/d\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"sfo\",\n            \"sources\": {\n                \"sfo\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"sfo\",\n            \"sources\": {\"sfo\": {\"specific_name\": \"airtemp_C_max\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"sfo\",\n            \"sources\": {\n                \"sfo\": {\"specific_name\": \"airtemp_C_min\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"sfo\",\n            \"sources\": {\n                \"sfo\": {\"specific_name\": \"airtemp_C_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.RELATIVE_DAYLIGHT_DURATION: {\n            \"default_source\": \"sfo\",\n            \"sources\": {\n                \"sfo\": {\"specific_name\": \"rel_sun_dur(%)\", \"unit\": \"%\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"wsl\",\n            \"sources\": {\"wsl\": {\"specific_name\": \"swe_mm\", \"unit\": \"mm\"}},\n        },\n    }\n</code></pre>"},{"location":"api/camels_ch/#hydrodataset.camels_ch.CamelsCh.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_ch/#hydrodataset.camels_ch.CamelsCh.__init__","title":"<code>__init__(data_path, region=None, download=False, version='v0.9')</code>","text":"<p>Initialize CAMELS-CH dataset with custom URL and CSV reading methods.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_CH data directory.</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional).</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically.</p> <code>False</code> <code>version</code> <code>str</code> <p>Dataset version (default: v0.9).</p> <code>'v0.9'</code> Source code in <code>hydrodataset/camels_ch.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    region: Optional[str] = None,\n    download: bool = False,\n    version: str = \"v0.9\",\n) -&gt; None:\n    \"\"\"Initialize CAMELS-CH dataset with custom URL and CSV reading methods.\n\n    Args:\n        data_path: Path to the CAMELS_CH data directory.\n        region: Geographic region identifier (optional).\n        download: Whether to download data automatically.\n        version: Dataset version (default: v0.9).\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.version = version\n\n    # Define updated URL for the new dataset version\n    new_url = \"https://zenodo.org/records/15025258\"\n\n    # Create custom methods that override the AquaFetch CSV reading\n    def custom_climate_attrs(self) -&gt; pd.DataFrame:\n        \"\"\"Returns 14 climate attributes of catchments with comma separator.\"\"\"\n        df = pd.read_csv(\n            self.clim_attr_path,\n            skiprows=1,\n            sep=\",\",  # Changed from ';' to ','\n            index_col=\"gauge_id\",\n            dtype={\n                \"gauge_id\": str,\n                \"p_mean\": float,\n                \"aridity\": float,\n                \"pet_mean\": float,\n                \"p_seasonality\": float,\n                \"frac_snow\": float,\n                \"high_prec_freq\": float,\n                \"high_prec_dur\": float,\n                \"high_prec_timing\": str,\n                \"low_prec_timing\": str,\n            },\n        )\n        return df\n\n    def custom_geol_attrs(self) -&gt; pd.DataFrame:\n        \"\"\"15 geological features with comma separator.\"\"\"\n        df = pd.read_csv(\n            self.geol_attr_path,\n            skiprows=1,\n            sep=\",\",  # Changed from ';' to ','\n            index_col=\"gauge_id\",\n            dtype=np.float32,\n        )\n        df.index = df.index.astype(int).astype(str)\n        return df\n\n    def custom_glacier_attrs(self) -&gt; pd.DataFrame:\n        \"\"\"Returns a dataframe with glacier attributes using comma separator.\"\"\"\n        df = pd.read_csv(\n            self.glacier_attr_path,\n            sep=\",\",  # Changed from ';' to ','\n            skiprows=1,\n            index_col=\"gauge_id\",\n            dtype=np.float32,\n        )\n        df.index = df.index.astype(int).astype(str)\n        return df\n\n    def custom_human_inf_attrs(self) -&gt; pd.DataFrame:\n        \"\"\"14 anthropogenic factors with comma separator.\"\"\"\n        df = pd.read_csv(\n            self.hum_inf_attr_path,\n            skiprows=1,\n            sep=\",\",  # Changed from ';' to ','\n            index_col=\"gauge_id\",\n            dtype={\n                \"gauge_id\": str,\n                \"n_inhabitants\": int,\n                \"dens_inhabitants\": float,\n                \"hp_count\": int,\n                \"hp_qturb\": float,\n                \"hp_inst_turb\": float,\n                \"hp_max_power\": float,\n                \"num_reservoir\": int,\n                \"reservoir_cap\": float,\n                \"reservoir_he\": float,\n                \"reservoir_fs\": float,\n                \"reservoir_irr\": float,\n                \"reservoir_nousedata\": float,\n            },\n        )\n        return df\n\n    def custom_hydrogeol_attrs(self) -&gt; pd.DataFrame:\n        \"\"\"10 hydrogeological factors with comma separator.\"\"\"\n        df = pd.read_csv(\n            self.hydrogeol_attr_path,\n            skiprows=1,\n            sep=\",\",  # Changed from ';' to ','\n            index_col=\"gauge_id\",\n            dtype=float,\n        )\n        df.index = df.index.astype(int).astype(str)\n        return df\n\n    def custom_hydrol_attrs(self) -&gt; pd.DataFrame:\n        \"\"\"14 hydrological parameters + 2 useful infos with comma separator.\"\"\"\n        df = pd.read_csv(\n            self.hydrol_attr_path,\n            skiprows=1,\n            sep=\",\",  # Changed from ';' to ','\n            index_col=\"gauge_id\",\n            dtype={\n                \"gauge_id\": str,\n                \"sign_number_of_years\": int,\n                \"q_mean\": float,\n                \"runoff_ratio\": float,\n                \"stream_elas\": float,\n                \"slope_fdc\": float,\n                \"baseflow_index_landson\": float,\n                \"hfd_mean\": float,\n                \"Q5\": float,\n                \"Q95\": float,\n                \"high_q_freq\": float,\n                \"high_q_dur\": float,\n                \"low_q_freq\": float,\n            },\n        )\n        return df\n\n    def custom_landcolover_attrs(self) -&gt; pd.DataFrame:\n        \"\"\"13 landcover parameters with comma separator.\"\"\"\n        return pd.read_csv(\n            self.lc_attr_path,\n            skiprows=1,\n            sep=\",\",  # Changed from ';' to ','\n            index_col=\"gauge_id\",\n            dtype={\n                \"gauge_id\": str,\n                \"crop_perc\": float,\n                \"grass_perc\": float,\n                \"scrub_perc\": float,\n                \"dwood_perc\": float,\n                \"mixed_wood_perc\": float,\n                \"ewood_perc\": float,\n                \"wetlands_perc\": float,\n                \"inwater_perc\": float,\n                \"ice_perc\": float,\n                \"loose_rock_perc\": float,\n                \"rock_perc\": float,\n                \"urban_perc\": float,\n                \"dom_land_cover\": str,\n            },\n        )\n\n    def custom_soil_attrs(self) -&gt; pd.DataFrame:\n        \"\"\"80 soil parameters with comma separator.\"\"\"\n        df = pd.read_csv(\n            self.soil_attr_path,\n            skiprows=1,\n            sep=\",\",  # Changed from ';' to ','\n            index_col=\"gauge_id\",\n        )\n        df.index = df.index.astype(int).astype(str)\n        return df\n\n    def custom_topo_attrs(self) -&gt; pd.DataFrame:\n        \"\"\"Topographic parameters with comma separator.\"\"\"\n        df = pd.read_csv(\n            self.topo_attr_path,\n            skiprows=1,\n            sep=\",\",  # Changed from ';' to ','\n            index_col=\"gauge_id\",\n            encoding=\"unicode_escape\",\n        )\n        df.index = df.index.astype(int).astype(str)\n        return df\n\n    def custom_static_data(self) -&gt; pd.DataFrame:\n        \"\"\"Concatenate all static attributes without supp_geol_attrs.\"\"\"\n        df = pd.concat(\n            [\n                self.climate_attrs(),\n                self.geol_attrs(),\n                # self.supp_geol_attrs(),  # Removed as requested\n                self.glacier_attrs(),\n                self.human_inf_attrs(),\n                self.hydrogeol_attrs(),\n                self.hydrol_attrs(),\n                self.landcolover_attrs(),\n                self.soil_attrs(),\n                self.topo_attrs(),\n            ],\n            axis=1,\n        )\n        df.index = df.index.astype(str)\n        df.rename(columns=self.static_map, inplace=True)\n        return df\n\n    def custom_read_stn_dyn(self, station: str) -&gt; pd.DataFrame:\n        \"\"\"Reads daily dynamic data for one catchment with comma separator.\"\"\"\n        df = pd.read_csv(\n            os.path.join(self.dynamic_path, f\"CAMELS_CH_obs_based_{station}.csv\"),\n            sep=\",\",  # Changed from ';' to ','\n            index_col=\"date\",\n            parse_dates=True,\n            dtype=np.float32,\n        )\n        df.rename(columns=self.dyn_map, inplace=True)\n        return df\n\n    def custom_stations(self) -&gt; list:\n        \"\"\"Returns station ids for catchments with comma separator.\"\"\"\n        stns = pd.read_csv(\n            self.glacier_attr_path, sep=\",\", skiprows=1  # Changed from ';' to ','\n        )[\"gauge_id\"].values.tolist()\n        return [str(stn) for stn in stns]\n\n    def custom_dynamic_path(self):\n        \"\"\"Return the correct path for dynamic data (timeseries not time_series).\"\"\"\n        return os.path.join(self.camels_path, \"timeseries\", \"observation_based\")\n\n    def do_nothing(self, *args, **kwargs):\n        \"\"\"Placeholder method to disable certain operations.\"\"\"\n        pass\n\n    # Create class attributes dictionary to override CAMELS_CH methods\n    class_attrs = {\n        \"url\": new_url,\n        \"dynamic_path\": property(custom_dynamic_path),\n        \"climate_attrs\": custom_climate_attrs,\n        \"geol_attrs\": custom_geol_attrs,\n        \"glacier_attrs\": custom_glacier_attrs,\n        \"human_inf_attrs\": custom_human_inf_attrs,\n        \"hydrogeol_attrs\": custom_hydrogeol_attrs,\n        \"hydrol_attrs\": custom_hydrol_attrs,\n        \"landcolover_attrs\": custom_landcolover_attrs,\n        \"soil_attrs\": custom_soil_attrs,\n        \"topo_attrs\": custom_topo_attrs,\n        \"_static_data\": custom_static_data,\n        \"_read_stn_dyn\": custom_read_stn_dyn,\n        \"stations\": custom_stations,\n        \"_maybe_to_netcdf\": do_nothing,\n    }\n\n    # Create custom CAMELS_CH class with overridden methods\n    CustomCamelsCh = type(\"CAMELS_CH\", (CAMELS_CH,), class_attrs)\n\n    try:\n        self.aqua_fetch = CustomCamelsCh(data_path)\n    except Exception as e:\n        print(e)\n        check_zip_extract = False\n        # The zip files that should be downloaded for CAMELS-CH\n        zip_files = [\"camels_ch.zip\", \"Caravan_extension_CH.zip\"]\n        for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n            # The extracted directory name (without .zip extension)\n            extracted_dir = self.data_source_dir.joinpath(\n                \"CAMELS_CH\", filename[:-4]\n            )\n            if not extracted_dir.exists():\n                check_zip_extract = True\n                break\n        if check_zip_extract:\n            from hydroutils import hydro_file\n\n            hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_CH\"))\n        self.aqua_fetch = CustomCamelsCh(data_path)\n</code></pre>"},{"location":"api/camels_cl/","title":"CAMELS-CL","text":""},{"location":"api/camels_cl/#overview","title":"Overview","text":"<p>CAMELS-CL is the Chile hydrological dataset implementation. Chilean CAMELS dataset spanning from arid to humid climates across Chile.</p>"},{"location":"api/camels_cl/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Chile</li> <li>Module: <code>hydrodataset.camels_cl</code></li> <li>Class: <code>CamelsCl</code></li> </ul>"},{"location":"api/camels_cl/#features","title":"Features","text":""},{"location":"api/camels_cl/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_cl/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_cl/#usage","title":"Usage","text":""},{"location":"api/camels_cl/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_cl import CamelsCl\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsCl(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_cl/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_cl/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_cl/#api-reference","title":"API Reference","text":""},{"location":"api/camels_cl/#hydrodataset.camels_cl.CamelsCl","title":"<code>hydrodataset.camels_cl.CamelsCl</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_CL dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS_CL dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camels_cl.py</code> <pre><code>class CamelsCl(HydroDataset):\n    \"\"\"CAMELS_CL dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS_CL dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_CL dataset.\n\n        Args:\n            data_path: Path to the CAMELS_CL data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = CAMELS_CL(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_cl_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_cl_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1913-02-15\", \"2018-03-09\"]\n\n    def cache_attributes_xrdataset(self):\n        \"\"\"Override base method to add calculated p_mean from precipitation timeseries.\n\n        This method:\n        1. Calls parent method to create base attribute cache\n        2. Reads precipitation timeseries data\n        3. Calculates mean precipitation (p_mean) for each basin\n        4. Adds p_mean to the attribute dataset\n        5. Saves the updated cache\n        \"\"\"\n        # Step 1: Create base attribute cache using parent method\n        print(\"Creating base attribute cache...\")\n        super().cache_attributes_xrdataset()\n\n        # Step 2: Load the base cache file\n        cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n        with xr.open_dataset(cache_file) as ds_attr:\n            ds_attr = ds_attr.load()  # Load into memory\n\n        print(\"Calculating p_mean from precipitation timeseries...\")\n\n        # Step 3: Read precipitation timeseries for all basins\n        # Use the default precipitation source (cr2met)\n        basin_ids = self.read_object_ids().tolist()\n\n        try:\n            # Read full precipitation timeseries\n            prcp_ts = self.read_ts_xrdataset(\n                gage_id_lst=basin_ids,\n                t_range=self.default_t_range,\n                var_lst=[\"precipitation\"],\n            )\n\n            # Step 4: Calculate temporal mean for each basin\n            # The result is a DataArray with dimension (basin,)\n            p_mean_values = prcp_ts[\"precipitation\"].mean(dim=\"time\")\n\n            # Add units attribute\n            p_mean_values.attrs[\"units\"] = \"mm/day\"\n            p_mean_values.attrs[\"description\"] = (\n                \"Mean daily precipitation (calculated from timeseries)\"\n            )\n\n            # Step 5: Add p_mean to the attribute dataset\n            ds_attr[\"p_mean\"] = p_mean_values\n\n            print(f\"Successfully calculated p_mean for {len(basin_ids)} basins\")\n\n        except Exception as e:\n            print(f\"Warning: Could not calculate p_mean from precipitation data: {e}\")\n            print(\"Creating p_mean with NaN values as placeholder\")\n            # Create p_mean with NaN values if calculation fails\n            p_mean_nan = xr.DataArray(\n                np.full(len(basin_ids), np.nan),\n                coords={\"basin\": basin_ids},\n                dims=[\"basin\"],\n                attrs={\n                    \"units\": \"mm/day\",\n                    \"description\": \"Mean daily precipitation (not available)\",\n                },\n            )\n            ds_attr[\"p_mean\"] = p_mean_nan\n\n        # Step 6: Save the updated cache file\n        print(f\"Saving updated attribute cache with p_mean to: {cache_file}\")\n        ds_attr.to_netcdf(cache_file, mode=\"w\")\n        print(\"Successfully saved attribute cache with p_mean\")\n\n    # get the information of features from table3 in \"https://hess.copernicus.org/articles/22/5817/2018/\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n                \"depth_based\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"cr2met\",\n            \"sources\": {\n                \"cr2met\": {\"specific_name\": \"pcp_mm_cr2met\", \"unit\": \"mm/day\"},\n                \"chirps\": {\"specific_name\": \"pcp_mm_chirps\", \"unit\": \"mm/day\"},\n                \"mswep\": {\"specific_name\": \"pcp_mm_mswep\", \"unit\": \"mm/day\"},\n                \"tmpa\": {\"specific_name\": \"pcp_mm_tmpa\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_C_min\", \"unit\": \"\u00b0C\"}\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_C_max\", \"unit\": \"\u00b0C\"}\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_C_mean\", \"unit\": \"\u00b0C\"}\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"modis\",\n            \"sources\": {\n                \"modis\": {\"specific_name\": \"pet_mm_modis\", \"unit\": \"mm/day\"},\n                \"hargreaves\": {\"specific_name\": \"pet_mm_hargreaves\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"observations\",\n            \"sources\": {\"observations\": {\"specific_name\": \"swe\", \"unit\": \"mm\"}},\n        },\n    }\n</code></pre>"},{"location":"api/camels_cl/#hydrodataset.camels_cl.CamelsCl.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_cl/#hydrodataset.camels_cl.CamelsCl.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_CL dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_CL data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camels_cl.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_CL dataset.\n\n    Args:\n        data_path: Path to the CAMELS_CL data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = CAMELS_CL(data_path)\n</code></pre>"},{"location":"api/camels_col/","title":"CAMELS-COL","text":""},{"location":"api/camels_col/#overview","title":"Overview","text":"<p>CAMELS-COL is the Colombia hydrological dataset implementation. Colombian CAMELS dataset with tropical catchments from Colombia.</p>"},{"location":"api/camels_col/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Colombia</li> <li>Module: <code>hydrodataset.camels_col</code></li> <li>Class: <code>CamelsCol</code></li> </ul>"},{"location":"api/camels_col/#features","title":"Features","text":""},{"location":"api/camels_col/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_col/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_col/#usage","title":"Usage","text":""},{"location":"api/camels_col/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_col import CamelsCol\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsCol(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_col/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_col/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_col/#api-reference","title":"API Reference","text":""},{"location":"api/camels_col/#hydrodataset.camels_col.CamelsCol","title":"<code>hydrodataset.camels_col.CamelsCol</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_COL dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS_COL dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camels_col.py</code> <pre><code>class CamelsCol(HydroDataset):\n    \"\"\"CAMELS_COL dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS_COL dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_COL dataset.\n\n        Args:\n            data_path: Path to the CAMELS_COL data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = CAMELS_COL(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_col_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_col_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1981-05-21\", \"2022-12-31\"]\n\n    def cache_attributes_xrdataset(self):\n        \"\"\"Override base method to add calculated p_mean from precipitation timeseries.\n\n        This method:\n        1. Calls parent method to create base attribute cache\n        2. Reads precipitation timeseries data\n        3. Calculates mean precipitation (p_mean) for each basin\n        4. Adds p_mean to the attribute dataset\n        5. Saves the updated cache\n        \"\"\"\n        # Step 1: Create base attribute cache using parent method\n        print(\"Creating base attribute cache...\")\n        super().cache_attributes_xrdataset()\n\n        # Step 2: Load the base cache file\n        cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n        with xr.open_dataset(cache_file) as ds_attr:\n            ds_attr = ds_attr.load()  # Load into memory\n\n        print(\"Calculating p_mean from precipitation timeseries...\")\n\n        # Step 3: Read precipitation timeseries for all basins\n        basin_ids = self.read_object_ids().tolist()\n\n        try:\n            # Read full precipitation timeseries\n            prcp_ts = self.read_ts_xrdataset(\n                gage_id_lst=basin_ids,\n                t_range=self.default_t_range,\n                var_lst=[\"precipitation\"],\n            )\n\n            # Step 4: Calculate temporal mean for each basin\n            p_mean_values = prcp_ts[\"precipitation\"].mean(dim=\"time\")\n\n            # Add units attribute\n            p_mean_values.attrs[\"units\"] = \"mm/day\"\n            p_mean_values.attrs[\"description\"] = (\n                \"Mean daily precipitation (calculated from timeseries)\"\n            )\n\n            # Step 5: Add p_mean to the attribute dataset\n            ds_attr[\"p_mean\"] = p_mean_values\n\n            print(f\"Successfully calculated p_mean for {len(basin_ids)} basins\")\n\n        except Exception as e:\n            print(f\"Warning: Could not calculate p_mean from precipitation data: {e}\")\n            print(\"Creating p_mean with NaN values as placeholder\")\n            # Create p_mean with NaN values if calculation fails\n            p_mean_nan = xr.DataArray(\n                np.full(len(basin_ids), np.nan),\n                coords={\"basin\": basin_ids},\n                dims=[\"basin\"],\n                attrs={\n                    \"units\": \"mm/day\",\n                    \"description\": \"Mean daily precipitation (not available)\",\n                },\n            )\n            ds_attr[\"p_mean\"] = p_mean_nan\n\n        # Step 6: Save the updated cache file\n        print(f\"Saving updated attribute cache with p_mean to: {cache_file}\")\n        ds_attr.to_netcdf(cache_file, mode=\"w\")\n        print(\"Successfully saved attribute cache with p_mean\")\n\n    # get the information of features from dataset file \"00_CAMELS-COL  Description\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"}\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"pet_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/camels_col/#hydrodataset.camels_col.CamelsCol.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_col/#hydrodataset.camels_col.CamelsCol.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_COL dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_COL data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camels_col.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_COL dataset.\n\n    Args:\n        data_path: Path to the CAMELS_COL data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = CAMELS_COL(data_path)\n</code></pre>"},{"location":"api/camels_de/","title":"CAMELS-DE","text":""},{"location":"api/camels_de/#overview","title":"Overview","text":"<p>CAMELS-DE is the Germany hydrological dataset implementation. German CAMELS dataset covering diverse German catchments.</p>"},{"location":"api/camels_de/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Germany</li> <li>Module: <code>hydrodataset.camels_de</code></li> <li>Class: <code>CamelsDe</code></li> </ul>"},{"location":"api/camels_de/#features","title":"Features","text":""},{"location":"api/camels_de/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_de/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_de/#usage","title":"Usage","text":""},{"location":"api/camels_de/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_de import CamelsDe\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsDe(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_de/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_de/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_de/#api-reference","title":"API Reference","text":""},{"location":"api/camels_de/#hydrodataset.camels_de.CamelsDe","title":"<code>hydrodataset.camels_de.CamelsDe</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS-DE dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS-DE dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> Source code in <code>hydrodataset/camels_de.py</code> <pre><code>class CamelsDe(HydroDataset):\n    \"\"\"CAMELS-DE dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS-DE dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n    \"\"\"\n\n    def __init__(self, data_path, region=None, download=False, cache_path=None):\n        \"\"\"Initialize CAMELS-DE dataset.\"\"\"\n        super().__init__(data_path, cache_path=cache_path)\n        self.region = region\n        self.download = download\n        try:\n            self.aqua_fetch = CAMELS_DE(data_path)\n        except Exception:\n            check_zip_extract = False\n            zip_files = [\"camels_de.zip\"]\n            for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n                extracted_dir = self.data_source_dir.joinpath(\n                    \"CAMELS_DE\", filename[:-4]\n                )\n                if not extracted_dir.exists():\n                    check_zip_extract = True\n                    break\n            if check_zip_extract:\n                hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_DE\"))\n            self.aqua_fetch = CAMELS_DE(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_de_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_de_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1951-01-01\", \"2020-12-31\"]\n\n    # get the information of features from dataset file\"CAMELS_DE_Data_Description.pdf\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"gauge_lat\": {\"specific_name\": \"lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"long\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"elev_mean\", \"unit\": \"m\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"vol\",\n            \"sources\": {\n                \"vol\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n                \"specific\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.WATER_LEVEL: {\n            \"default_source\": \"federal\",\n            \"sources\": {\n                \"federal\": {\"specific_name\": \"water_level\", \"unit\": \"m\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"pcp_mm_mean\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION_MIN: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"pcp_mm_min\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION_MAX: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"pcp_mm_max\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION_MEDIAN: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"pcp_mm_median\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"solrad_wm2_mean\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MIN: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"solrad_wm2_min\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MAX: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"solrad_wm2_max\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MEDIAN: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"solrad_wm2_med\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"rh_\", \"unit\": \"%\"},\n            },\n        },\n        StandardVariable.RELATIVE_HUMIDITY_MIN: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"rh__min\", \"unit\": \"%\"},\n            },\n        },\n        StandardVariable.RELATIVE_HUMIDITY_MAX: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"rh__max\", \"unit\": \"%\"},\n            },\n        },\n        StandardVariable.RELATIVE_HUMIDITY_MEDIAN: {\n            \"default_source\": \"dwd\",\n            \"sources\": {\n                \"dwd\": {\"specific_name\": \"rh__med\", \"unit\": \"%\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/camels_de/#hydrodataset.camels_de.CamelsDe.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_de/#hydrodataset.camels_de.CamelsDe.__init__","title":"<code>__init__(data_path, region=None, download=False, cache_path=None)</code>","text":"<p>Initialize CAMELS-DE dataset.</p> Source code in <code>hydrodataset/camels_de.py</code> <pre><code>def __init__(self, data_path, region=None, download=False, cache_path=None):\n    \"\"\"Initialize CAMELS-DE dataset.\"\"\"\n    super().__init__(data_path, cache_path=cache_path)\n    self.region = region\n    self.download = download\n    try:\n        self.aqua_fetch = CAMELS_DE(data_path)\n    except Exception:\n        check_zip_extract = False\n        zip_files = [\"camels_de.zip\"]\n        for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n            extracted_dir = self.data_source_dir.joinpath(\n                \"CAMELS_DE\", filename[:-4]\n            )\n            if not extracted_dir.exists():\n                check_zip_extract = True\n                break\n        if check_zip_extract:\n            hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_DE\"))\n        self.aqua_fetch = CAMELS_DE(data_path)\n</code></pre>"},{"location":"api/camels_dk/","title":"CAMELS-DK","text":""},{"location":"api/camels_dk/#overview","title":"Overview","text":"<p>CAMELS-DK is the Denmark hydrological dataset implementation. Danish CAMELS dataset for catchments in Denmark.</p>"},{"location":"api/camels_dk/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Denmark</li> <li>Module: <code>hydrodataset.camels_dk</code></li> <li>Class: <code>CamelsDk</code></li> </ul>"},{"location":"api/camels_dk/#features","title":"Features","text":""},{"location":"api/camels_dk/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_dk/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_dk/#usage","title":"Usage","text":""},{"location":"api/camels_dk/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_dk import CamelsDk\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsDk(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_dk/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_dk/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_dk/#api-reference","title":"API Reference","text":""},{"location":"api/camels_dk/#hydrodataset.camels_dk.CamelsDk","title":"<code>hydrodataset.camels_dk.CamelsDk</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_DK dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS_DK dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camels_dk.py</code> <pre><code>class CamelsDk(HydroDataset):\n    \"\"\"CAMELS_DK dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS_DK dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_DK dataset.\n\n        Args:\n            data_path: Path to the CAMELS_DK data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = CAMELS_DK(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_dk_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_dk_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1989-01-02\", \"2023-12-31\"]\n\n    # get the information of features from dataset file\"Data_description.pdf\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"gauge_lat\": {\"specific_name\": \"lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"long\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"dem_mean\", \"unit\": \"m\"},\n        \"pet_mean\": {\"specific_name\": \"pet_mean\", \"unit\": \"mm/day\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"dkmodel\",\n            \"sources\": {\n                \"dkmodel\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"dmi\",\n            \"sources\": {\n                \"dmi\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"dmi\",\n            \"sources\": {\n                \"dmi\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"dmi\",\n            \"sources\": {\n                \"dmi\": {\"specific_name\": \"pet_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"dkm_model\",\n            \"sources\": {\n                \"dkm_model\": {\"specific_name\": \"aet_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/camels_dk/#hydrodataset.camels_dk.CamelsDk.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_dk/#hydrodataset.camels_dk.CamelsDk.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_DK dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_DK data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camels_dk.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_DK dataset.\n\n    Args:\n        data_path: Path to the CAMELS_DK data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = CAMELS_DK(data_path)\n</code></pre>"},{"location":"api/camels_fi/","title":"CAMELS-FI","text":""},{"location":"api/camels_fi/#overview","title":"Overview","text":"<p>CAMELS-FI is the Finland hydrological dataset implementation. Finnish CAMELS dataset covering boreal climate catchments.</p>"},{"location":"api/camels_fi/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Finland</li> <li>Module: <code>hydrodataset.camels_fi</code></li> <li>Class: <code>CamelsFi</code></li> </ul>"},{"location":"api/camels_fi/#features","title":"Features","text":""},{"location":"api/camels_fi/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_fi/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_fi/#usage","title":"Usage","text":""},{"location":"api/camels_fi/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_fi import CamelsFi\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsFi(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_fi/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_fi/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_fi/#api-reference","title":"API Reference","text":""},{"location":"api/camels_fi/#hydrodataset.camels_fi.CamelsFi","title":"<code>hydrodataset.camels_fi.CamelsFi</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_FI dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS_FI dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camels_fi.py</code> <pre><code>class CamelsFi(HydroDataset):\n    \"\"\"CAMELS_FI dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS_FI dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_FI dataset.\n\n        Args:\n            data_path: Path to the CAMELS_FI data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = CAMELS_FI(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_fi_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_fi_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1961-01-01\", \"2023-12-31\"]\n\n    # get the information of features from dataset file\"support_document.pdf\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"gauge_lat\": {\"specific_name\": \"lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"long\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"elev_mean\", \"unit\": \"m\"},\n        \"pet_mean\": {\"specific_name\": \"pet_mean\", \"unit\": \"mm/day\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"SYKE\",\n            \"sources\": {\n                \"SYKE\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n                \"depth_based\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"fmi\",\n            \"sources\": {\n                \"fmi\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"default\",\n            \"sources\": {\n                \"default\": {\"specific_name\": \"pet_mm\", \"unit\": \"mm/day\"},\n                \"era5_land\": {\"specific_name\": \"pe_era5_land\", \"unit\": \"mm/day\"},\n                \"fmi\": {\"specific_name\": \"pet_fmi\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"swe_mm_era5\", \"unit\": \"mm\"},\n                \"cci\": {\"specific_name\": \"swe_mm_cci3-1\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_C_min\", \"unit\": \"\u00b0C\"},\n                \"ground_min\": {\"specific_name\": \"temperature_gmin\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_C_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_C_max\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"rh_%\", \"unit\": \"%\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"radiation_global\", \"unit\": \"KJ/m^2\"},\n            },\n        },\n        StandardVariable.SNOW_DEPTH: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"snowdepth_m\", \"unit\": \"cm\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/camels_fi/#hydrodataset.camels_fi.CamelsFi.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_fi/#hydrodataset.camels_fi.CamelsFi.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_FI dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_FI data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camels_fi.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_FI dataset.\n\n    Args:\n        data_path: Path to the CAMELS_FI data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = CAMELS_FI(data_path)\n</code></pre>"},{"location":"api/camels_fr/","title":"CAMELS-FR","text":""},{"location":"api/camels_fr/#overview","title":"Overview","text":"<p>CAMELS-FR is the France hydrological dataset implementation. French CAMELS dataset for diverse catchments across France.</p>"},{"location":"api/camels_fr/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: France</li> <li>Module: <code>hydrodataset.camels_fr</code></li> <li>Class: <code>CamelsFr</code></li> </ul>"},{"location":"api/camels_fr/#features","title":"Features","text":""},{"location":"api/camels_fr/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_fr/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_fr/#usage","title":"Usage","text":""},{"location":"api/camels_fr/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_fr import CamelsFr\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsFr(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_fr/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_fr/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_fr/#api-reference","title":"API Reference","text":""},{"location":"api/camels_fr/#hydrodataset.camels_fr.CamelsFr","title":"<code>hydrodataset.camels_fr.CamelsFr</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_FR dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS_FR dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camels_fr.py</code> <pre><code>class CamelsFr(HydroDataset):\n    \"\"\"CAMELS_FR dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS_FR dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        region: Optional[str] = None,\n        download: bool = False,\n        cache_path: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_FR dataset.\n\n        Args:\n            data_path: Path to the CAMELS_FR data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n            cache_path: Path to the cache directory\n        \"\"\"\n        super().__init__(data_path, cache_path=cache_path)\n        self.region = region\n        self.download = download\n        try:\n            self.aqua_fetch = CAMELS_FR(data_path)\n        except Exception as e:\n            print(e)\n            check_zip_extract = False\n            # The zip files that should be downloaded for CAMELS-CH\n            zip_files = [\n                \"ADDITIONAL_LICENSES.zip\",\n                \"CAMELS_FR_attributes.zip\",\n                \"CAMELS_FR_geography.zip\",\n                \"CAMELS_FR_time_series.zip\",\n            ]\n            for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n                # The extracted directory name (without .zip extension)\n                extracted_dir = self.data_source_dir.joinpath(\n                    \"CAMELS_FR\", filename[:-4]\n                )\n                if not extracted_dir.exists():\n                    check_zip_extract = True\n                    break\n            if check_zip_extract:\n                hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_FR\"))\n            self.aqua_fetch = CAMELS_FR(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_fr_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_fr_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1970-01-01\", \"2021-12-31\"]\n\n    def cache_attributes_xrdataset(self):\n        \"\"\"Override base method to add calculated p_mean from precipitation timeseries.\n\n        This method:\n        1. Calls parent method to create base attribute cache\n        2. Reads precipitation timeseries data\n        3. Calculates mean precipitation (p_mean) for each basin\n        4. Adds p_mean to the attribute dataset\n        5. Saves the updated cache\n        \"\"\"\n        # Step 1: Create base attribute cache using parent method\n        print(\"Creating base attribute cache...\")\n        super().cache_attributes_xrdataset()\n\n        # Step 2: Load the base cache file\n        cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n        with xr.open_dataset(cache_file) as ds_attr:\n            ds_attr = ds_attr.load()  # Load into memory\n\n        print(\"Calculating p_mean from precipitation timeseries...\")\n\n        # Step 3: Read precipitation timeseries for all basins\n        basin_ids = self.read_object_ids().tolist()\n\n        try:\n            # Read full precipitation timeseries\n            prcp_ts = self.read_ts_xrdataset(\n                gage_id_lst=basin_ids,\n                t_range=self.default_t_range,\n                var_lst=[\"precipitation\"],\n            )\n\n            # Step 4: Calculate temporal mean for each basin\n            p_mean_values = prcp_ts[\"precipitation\"].mean(dim=\"time\")\n\n            # Add units attribute\n            p_mean_values.attrs[\"units\"] = \"mm/day\"\n            p_mean_values.attrs[\"description\"] = (\n                \"Mean daily precipitation (calculated from timeseries)\"\n            )\n\n            # Step 5: Add p_mean to the attribute dataset\n            ds_attr[\"p_mean\"] = p_mean_values\n\n            print(f\"Successfully calculated p_mean for {len(basin_ids)} basins\")\n\n        except Exception as e:\n            print(f\"Warning: Could not calculate p_mean from precipitation data: {e}\")\n            print(\"Creating p_mean with NaN values as placeholder\")\n            # Create p_mean with NaN values if calculation fails\n            p_mean_nan = xr.DataArray(\n                np.full(len(basin_ids), np.nan),\n                coords={\"basin\": basin_ids},\n                dims=[\"basin\"],\n                attrs={\n                    \"units\": \"mm/day\",\n                    \"description\": \"Mean daily precipitation (not available)\",\n                },\n            )\n            ds_attr[\"p_mean\"] = p_mean_nan\n\n        # Step 6: Save the updated cache file\n        print(f\"Saving updated attribute cache with p_mean to: {cache_file}\")\n        ds_attr.to_netcdf(cache_file, mode=\"w\")\n        print(\"Successfully saved attribute cache with p_mean\")\n\n    # get the information of features from dataset file\"CAMELS-FR_description\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"gauge_lat\": {\"specific_name\": \"lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"long\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"elev_mean\", \"unit\": \"m\"},\n        \"pet_mean\": {\"specific_name\": \"pet_mean\", \"unit\": \"mm/day\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"hydroportail\",\n            \"sources\": {\n                \"hydroportail\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"L/s\"},\n                \"camelsfr\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"SIM2-SAFRAN\",\n            \"sources\": {\n                \"SIM2-SAFRAN\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"SIM2-SAFRAN\",\n            \"sources\": {\n                \"SIM2-SAFRAN\": {\"specific_name\": \"airtemp_C_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"SIM2-SAFRAN\",\n            \"sources\": {\n                \"SIM2-SAFRAN\": {\"specific_name\": \"airtemp_C_min\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"SIM2-SAFRAN\",\n            \"sources\": {\n                \"SIM2-SAFRAN\": {\"specific_name\": \"airtemp_C_max\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"oudin\",\n            \"sources\": {\n                \"oudin\": {\"specific_name\": \"pet_mm_ou\", \"unit\": \"mm/day\"},\n                \"penman\": {\"specific_name\": \"pet_mm_pe\", \"unit\": \"mm/day\"},\n                \"penman_monteith\": {\"specific_name\": \"pet_mm_pm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.WIND_SPEED: {\n            \"default_source\": \"SIM2-SAFRAN\",\n            \"sources\": {\n                \"SIM2-SAFRAN\": {\"specific_name\": \"windspeed_mps\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"SIM2-SAFRAN\",\n            \"sources\": {\n                \"SIM2-SAFRAN\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"J/cm^2\"},\n            },\n        },\n        StandardVariable.LONGWAVE_SOLAR_RADIATION: {\n            \"default_source\": \"SIM2-SAFRAN\",\n            \"sources\": {\n                \"SIM2-SAFRAN\": {\"specific_name\": \"lwdownrad_wm2\", \"unit\": \"J/cm^2\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"isba_model\",\n            \"sources\": {\n                \"isba_model\": {\"specific_name\": \"tsd_swe_isba\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.SOIL_MOISTURE: {\n            \"default_source\": \"gr\",\n            \"sources\": {\n                \"gr\": {\"specific_name\": \"tsd_swi_gr\", \"unit\": \"mm/day\"},\n                \"isba\": {\"specific_name\": \"tsd_swi_isba\", \"unit\": \"mm/day\"},\n            },\n        },      \n        StandardVariable.SPECIFIC_HUMIDITY: {\n            \"default_source\": \"isba_model\",\n            \"sources\": {\n                \"isba_model\": {\"specific_name\": \"spechum_gkg\", \"unit\": \"mm/day\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/camels_fr/#hydrodataset.camels_fr.CamelsFr.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_fr/#hydrodataset.camels_fr.CamelsFr.__init__","title":"<code>__init__(data_path, region=None, download=False, cache_path=None)</code>","text":"<p>Initialize CAMELS_FR dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_FR data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> <code>cache_path</code> <code>Optional[str]</code> <p>Path to the cache directory</p> <code>None</code> Source code in <code>hydrodataset/camels_fr.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    region: Optional[str] = None,\n    download: bool = False,\n    cache_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize CAMELS_FR dataset.\n\n    Args:\n        data_path: Path to the CAMELS_FR data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n        cache_path: Path to the cache directory\n    \"\"\"\n    super().__init__(data_path, cache_path=cache_path)\n    self.region = region\n    self.download = download\n    try:\n        self.aqua_fetch = CAMELS_FR(data_path)\n    except Exception as e:\n        print(e)\n        check_zip_extract = False\n        # The zip files that should be downloaded for CAMELS-CH\n        zip_files = [\n            \"ADDITIONAL_LICENSES.zip\",\n            \"CAMELS_FR_attributes.zip\",\n            \"CAMELS_FR_geography.zip\",\n            \"CAMELS_FR_time_series.zip\",\n        ]\n        for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n            # The extracted directory name (without .zip extension)\n            extracted_dir = self.data_source_dir.joinpath(\n                \"CAMELS_FR\", filename[:-4]\n            )\n            if not extracted_dir.exists():\n                check_zip_extract = True\n                break\n        if check_zip_extract:\n            hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_FR\"))\n        self.aqua_fetch = CAMELS_FR(data_path)\n</code></pre>"},{"location":"api/camels_gb/","title":"CAMELS-GB","text":""},{"location":"api/camels_gb/#overview","title":"Overview","text":"<p>CAMELS-GB is the Great Britain hydrological dataset implementation. Great Britain CAMELS dataset covering catchments in England, Scotland, and Wales.</p>"},{"location":"api/camels_gb/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Great Britain</li> <li>Module: <code>hydrodataset.camels_gb</code></li> <li>Class: <code>CamelsGb</code></li> </ul>"},{"location":"api/camels_gb/#features","title":"Features","text":""},{"location":"api/camels_gb/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_gb/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_gb/#usage","title":"Usage","text":""},{"location":"api/camels_gb/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_gb import CamelsGb\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsGb(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_gb/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_gb/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_gb/#api-reference","title":"API Reference","text":""},{"location":"api/camels_gb/#hydrodataset.camels_gb.CamelsGb","title":"<code>hydrodataset.camels_gb.CamelsGb</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_GB dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS_GB dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camels_gb.py</code> <pre><code>class CamelsGb(HydroDataset):\n    \"\"\"CAMELS_GB dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS_GB dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_GB dataset.\n\n        Args:\n            data_path: Path to the CAMELS_GB data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = CAMELS_GB(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_gb_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_gb_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1970-10-01\", \"2015-09-30\"]\n\n    # get the information of features from dataset file\"CAMELSGB_EIDC_SupportingDocumentation\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"gauge_lat\": {\"specific_name\": \"lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"long\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"elev_mean\", \"unit\": \"m\"},\n        \"pet_mean\": {\"specific_name\": \"pet_mean\", \"unit\": \"mm/day\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"hydrological\",\n            \"sources\": {\n                \"hydrological\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n                \"depth_based\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"meteorological\",\n            \"sources\": {\n                \"meteorological\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"meteorological\",\n            \"sources\": {\n                \"meteorological\": {\"specific_name\": \"airtemp_C_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"meteorological\",\n            \"sources\": {\n                \"meteorological\": {\"specific_name\": \"pet_mm\", \"unit\": \"mm/day\"},\n                \"with_interception\": {\n                    \"specific_name\": \"pet_mm_intercep\",\n                    \"unit\": \"mm/day\",\n                },\n            },\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"meteorological\",\n            \"sources\": {\n                \"meteorological\": {\"specific_name\": \"rh_%\", \"unit\": \"g/kg\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"meteorological\",\n            \"sources\": {\n                \"meteorological\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.LONGWAVE_SOLAR_RADIATION: {\n            \"default_source\": \"meteorological\",\n            \"sources\": {\n                \"meteorological\": {\"specific_name\": \"lwsolrad_wm2\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.WIND_SPEED: {\n            \"default_source\": \"meteorological\",\n            \"sources\": {\n                \"meteorological\": {\"specific_name\": \"windspeed_mps\", \"unit\": \"m/s\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/camels_gb/#hydrodataset.camels_gb.CamelsGb.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_gb/#hydrodataset.camels_gb.CamelsGb.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_GB dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_GB data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camels_gb.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_GB dataset.\n\n    Args:\n        data_path: Path to the CAMELS_GB data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = CAMELS_GB(data_path)\n</code></pre>"},{"location":"api/camels_ind/","title":"CAMELS-IND","text":""},{"location":"api/camels_ind/#overview","title":"Overview","text":"<p>CAMELS-IND is the India hydrological dataset implementation. Indian CAMELS dataset with monsoon-influenced catchments.</p>"},{"location":"api/camels_ind/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: India</li> <li>Module: <code>hydrodataset.camels_ind</code></li> <li>Class: <code>CamelsInd</code></li> </ul>"},{"location":"api/camels_ind/#features","title":"Features","text":""},{"location":"api/camels_ind/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_ind/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_ind/#usage","title":"Usage","text":""},{"location":"api/camels_ind/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_ind import CamelsInd\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsInd(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_ind/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_ind/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_ind/#api-reference","title":"API Reference","text":""},{"location":"api/camels_ind/#hydrodataset.camels_ind.CamelsInd","title":"<code>hydrodataset.camels_ind.CamelsInd</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_IND dataset class extending HydroDataset.</p> <p>This class provides access to the CAMELS_IND dataset, which contains hydrological and meteorological data for various watersheds in India. It uses a custom implementation to support the latest dataset version.</p> <p>The class relies on AquaFetch for data reading but overrides certain methods to support the new file structure in the latest Zenodo release.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>aqua_fetch</code> <p>CustomCAMELS_IND instance for data access</p> Source code in <code>hydrodataset/camels_ind.py</code> <pre><code>class CamelsInd(HydroDataset):\n    \"\"\"CAMELS_IND dataset class extending HydroDataset.\n\n    This class provides access to the CAMELS_IND dataset, which contains\n    hydrological and meteorological data for various watersheds in India.\n    It uses a custom implementation to support the latest dataset version.\n\n    The class relies on AquaFetch for data reading but overrides certain\n    methods to support the new file structure in the latest Zenodo release.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        aqua_fetch: CustomCAMELS_IND instance for data access\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_IND dataset.\n\n        Args:\n            data_path: Path to the CAMELS_IND data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n\n        try:\n            # Use custom class that supports the latest dataset version\n            self.aqua_fetch = CustomCAMELS_IND(data_path)\n        except Exception as e:\n            print(e)\n            # If initialization fails, try to extract zip files\n            check_zip_extract = False\n            zip_files = [\n                \"CAMELS_IND_All_Catchments.zip\",\n                \"CAMELS_IND_Catchments_Streamflow_Sufficient.zip\",\n            ]\n            for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n                extracted_dir = self.data_source_dir.joinpath(\n                    \"CAMELS_IND\", filename[:-4]\n                )\n                if not extracted_dir.exists():\n                    check_zip_extract = True\n                    break\n            if check_zip_extract:\n                hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_IND\"))\n            # Retry initialization after extraction\n            self.aqua_fetch = CustomCAMELS_IND(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_ind_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_ind_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1980-01-01\", \"2020-12-31\"]\n\n    # get the information of features from dataset file\"00_CAMELS_IND_Data_Description.pdf\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"obs\",\n            \"sources\": {\"obs\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"}},\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"imd\",\n            \"sources\": {\"imd\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"imd\",\n            \"sources\": {\"imd\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"imd\",\n            \"sources\": {\"imd\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"imd\",\n            \"sources\": {\"imd\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"}},\n        },\n        StandardVariable.LONGWAVE_SOLAR_RADIATION: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"lwdownrad_wm2\", \"unit\": \"W/m^2\"}},\n        },\n        StandardVariable.WIND_SPEED: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"windspeed_mps\", \"unit\": \"m/s\"}},\n        },\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"windspeedv_mps\", \"unit\": \"m/s\"}},\n        },\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"windspeedu_mps\", \"unit\": \"m/s\"}},\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"rh_\", \"unit\": \"%\"}},\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"default\",\n            \"sources\": {\n                \"default\": {\"specific_name\": \"pet_mm\", \"unit\": \"mm/day\"},\n                \"gleam\": {\"specific_name\": \"pet_mm_gleam\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"gleam\",\n            \"sources\": {\"gleam\": {\"specific_name\": \"aet_mm_gleam\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.EVAPORATION: {\n            \"default_source\": \"canopy\",\n            \"sources\": {\n                \"canopy\": {\"specific_name\": \"evap_canopy\", \"unit\": \"mm/day\"},\n                \"surface\": {\"specific_name\": \"evap_surface\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"sm_lvl1\", \"unit\": \"kg/m^2\"}},\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"sm_lvl2\", \"unit\": \"kg/m^2\"}},\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"sm_lvl3\", \"unit\": \"kg/m^2\"}},\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4: {\n            \"default_source\": \"imdaa\",\n            \"sources\": {\"imdaa\": {\"specific_name\": \"sm_lvl4\", \"unit\": \"kg/m^2\"}},\n        },\n    }\n</code></pre>"},{"location":"api/camels_ind/#hydrodataset.camels_ind.CamelsInd.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_ind/#hydrodataset.camels_ind.CamelsInd.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_IND dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_IND data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camels_ind.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_IND dataset.\n\n    Args:\n        data_path: Path to the CAMELS_IND data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n\n    try:\n        # Use custom class that supports the latest dataset version\n        self.aqua_fetch = CustomCAMELS_IND(data_path)\n    except Exception as e:\n        print(e)\n        # If initialization fails, try to extract zip files\n        check_zip_extract = False\n        zip_files = [\n            \"CAMELS_IND_All_Catchments.zip\",\n            \"CAMELS_IND_Catchments_Streamflow_Sufficient.zip\",\n        ]\n        for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n            extracted_dir = self.data_source_dir.joinpath(\n                \"CAMELS_IND\", filename[:-4]\n            )\n            if not extracted_dir.exists():\n                check_zip_extract = True\n                break\n        if check_zip_extract:\n            hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_IND\"))\n        # Retry initialization after extraction\n        self.aqua_fetch = CustomCAMELS_IND(data_path)\n</code></pre>"},{"location":"api/camels_lux/","title":"CAMELS-LUX","text":""},{"location":"api/camels_lux/#overview","title":"Overview","text":"<p>CAMELS-LUX is the Luxembourg hydrological dataset implementation. Luxembourg CAMELS dataset for small European catchments.</p>"},{"location":"api/camels_lux/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Luxembourg</li> <li>Module: <code>hydrodataset.camels_lux</code></li> <li>Class: <code>CamelsLux</code></li> </ul>"},{"location":"api/camels_lux/#features","title":"Features","text":""},{"location":"api/camels_lux/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_lux/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_lux/#usage","title":"Usage","text":""},{"location":"api/camels_lux/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_lux import CamelsLux\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsLux(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_lux/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_lux/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_lux/#api-reference","title":"API Reference","text":""},{"location":"api/camels_lux/#hydrodataset.camels_lux.CamelsLux","title":"<code>hydrodataset.camels_lux.CamelsLux</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_LUX dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS_LUX dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camels_lux.py</code> <pre><code>class CamelsLux(HydroDataset):\n    \"\"\"CAMELS_LUX dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS_LUX dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_LUX dataset.\n\n        Args:\n            data_path: Path to the CAMELS_LUX data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        try:\n            self.aqua_fetch = CAMELS_LUX(data_path)\n        except Exception as e:\n            print(e)\n            check_zip_extract = False\n            # The zip files that should be downloaded for CAMELS-LUX\n            zip_files = [\"CAMELS-LUX.zip\", \"CAMELS-LUX_shapefiles.zip\"]\n            for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n                extracted_dir = self.data_source_dir.joinpath(\n                    \"CAMELS_LUX\", filename[:-4]\n                )\n                if not extracted_dir.exists():\n                    check_zip_extract = True\n                    break\n            if check_zip_extract:\n                hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_LUX\"))\n            self.aqua_fetch = CAMELS_LUX(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_lux_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_lux_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"2004-01-01\", \"2021-12-31\"]\n\n    def cache_attributes_xrdataset(self):\n        \"\"\"Override base method to add calculated p_mean from precipitation timeseries.\n\n        This method:\n        1. Calls parent method to create base attribute cache\n        2. Reads precipitation timeseries data\n        3. Calculates mean precipitation (p_mean) for each basin\n        4. Adds p_mean to the attribute dataset\n        5. Saves the updated cache\n        \"\"\"\n        # Step 1: Create base attribute cache using parent method\n        print(\"Creating base attribute cache...\")\n        super().cache_attributes_xrdataset()\n\n        # Step 2: Load the base cache file\n        cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n        with xr.open_dataset(cache_file) as ds_attr:\n            ds_attr = ds_attr.load()  # Load into memory\n\n        print(\"Calculating p_mean from precipitation timeseries...\")\n\n        # Step 3: Read precipitation timeseries for all basins\n        basin_ids = self.read_object_ids().tolist()\n\n        try:\n            # Read full precipitation timeseries\n            prcp_ts = self.read_ts_xrdataset(\n                gage_id_lst=basin_ids,\n                t_range=self.default_t_range,\n                var_lst=[\"precipitation\"],\n            )\n\n            # Step 4: Calculate temporal mean for each basin\n            p_mean_values = prcp_ts[\"precipitation\"].mean(dim=\"time\")\n\n            # Add units attribute\n            p_mean_values.attrs[\"units\"] = \"mm/day\"\n            p_mean_values.attrs[\"description\"] = (\n                \"Mean daily precipitation (calculated from timeseries)\"\n            )\n\n            # Step 5: Add p_mean to the attribute dataset\n            ds_attr[\"p_mean\"] = p_mean_values\n\n            print(f\"Successfully calculated p_mean for {len(basin_ids)} basins\")\n\n        except Exception as e:\n            print(f\"Warning: Could not calculate p_mean from precipitation data: {e}\")\n            print(\"Creating p_mean with NaN values as placeholder\")\n            # Create p_mean with NaN values if calculation fails\n            p_mean_nan = xr.DataArray(\n                np.full(len(basin_ids), np.nan),\n                coords={\"basin\": basin_ids},\n                dims=[\"basin\"],\n                attrs={\n                    \"units\": \"mm/day\",\n                    \"description\": \"Mean daily precipitation (not available)\",\n                },\n            )\n            ds_attr[\"p_mean\"] = p_mean_nan\n\n        # Step 6: Save the updated cache file\n        print(f\"Saving updated attribute cache with p_mean to: {cache_file}\")\n        ds_attr.to_netcdf(cache_file, mode=\"w\")\n        print(\"Successfully saved attribute cache with p_mean\")\n\n    # get the information of features from dataset file\"CAMELS-LUX_data-description.pdf\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"gauge_lat\": {\"specific_name\": \"lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"long\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"elev_mean\", \"unit\": \"m\"},\n        \"pet_mean\": {\"specific_name\": \"pet_mean\", \"unit\": \"mm/day\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n                \"depth_based\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"radar\",\n            \"sources\": {\n                \"radar\": {\"specific_name\": \"pcp_mm_radar\", \"unit\": \"mm\"},\n                \"station\": {\"specific_name\": \"pcp_mm_station\", \"unit\": \"mm\"},\n                \"era5\": {\"specific_name\": \"pcp_mm_era5\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \" era5\",\n            \"sources\": {\n                \" era5\": {\"specific_name\": \"airtemp_C_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"oudin\",\n            \"sources\": {\n                \"oudin\": {\"specific_name\": \"pet_mm_oudin\", \"unit\": \"mm\"},\n                \"penman_monteith\": {\"specific_name\": \"pet_mm_pm\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"rh_\", \"unit\": \"%\"},\n            },\n        },\n        StandardVariable.SPECIFIC_HUMIDITY: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"spechum_gkg\", \"unit\": \"kg/kg\"},\n            },\n        },\n        StandardVariable.WIND_SPEED: {\n            \"default_source\": \"hersbach\",\n            \"sources\": {\n                \"hersbach\": {\"specific_name\": \"windspeed_mps\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.LOW_LEVEL_WIND_SHEAR: {\n            \"default_source\": \"hersbach\",\n            \"sources\": {\n                \"hersbach\": {\"specific_name\": \"lls\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.DEEP_LEVEL_WIND_SHEAR: {\n            \"default_source\": \"hersbach\",\n            \"sources\": {\n                \"hersbach\": {\"specific_name\": \"dls\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.CAPE: {\n            \"default_source\": \"hersbach\",\n            \"sources\": {\n                \"hersbach\": {\"specific_name\": \"cape\", \"unit\": \"J/kg\"},\n            },\n        },\n        StandardVariable.CIN: {\n            \"default_source\": \"hersbach\",\n            \"sources\": {\n                \"hersbach\": {\"specific_name\": \"cin\", \"unit\": \"J/kg\"},\n            },\n        },\n        StandardVariable.MAX_RAIN_RATE: {\n            \"default_source\": \"radar\",\n            \"sources\": {\n                \"radar\": {\"specific_name\": \"rr_max_rad\", \"unit\": \"mm/5Min/1x1km\"},\n            },\n        },\n        StandardVariable.MIN_RAIN_RATE: {\n            \"default_source\": \"radar\",\n            \"sources\": {\n                \"radar\": {\"specific_name\": \"rr_min_rad\", \"unit\": \"mm/5Min/1x1km\"},\n            },\n        },  \n        StandardVariable.CIN: {\n            \"default_source\": \"hersbach\",\n            \"sources\": {\n                \"hersbach\": {\"specific_name\": \"cin\", \"unit\": \"J/kg\"},\n            },\n        },\n        StandardVariable.TOTAL_COLUMN_WATER_VAPOUR: {\n            \"default_source\": \"hersbach\",\n            \"sources\": {\n                \"hersbach\": {\"specific_name\": \"tcwv\", \"unit\": \"J/kg\"},\n            },\n        },\n        StandardVariable.TOTAL_COLUMN_WATER_VAPOUR: {\n            \"default_source\": \"hersbach\",\n            \"sources\": {\n                \"hersbach\": {\"specific_name\": \"tcwv\", \"unit\": \"J/kg\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1: {\n            \"default_source\": \"Mu\u00f1oz_Sabater\",\n            \"sources\": {\n                \"Mu\u00f1oz_Sabater\": {\"specific_name\": \"sml1\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2: {\n            \"default_source\": \"Mu\u00f1oz_Sabater\",\n            \"sources\": {\n                \"Mu\u00f1oz_Sabater\": {\"specific_name\": \"sml2\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3: {\n            \"default_source\": \"Mu\u00f1oz_Sabater\",\n            \"sources\": {\n                \"Mu\u00f1oz_Sabater\": {\"specific_name\": \"sml3\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4: {\n            \"default_source\": \"Mu\u00f1oz_Sabater\",\n            \"sources\": {\n                \"Mu\u00f1oz_Sabater\": {\"specific_name\": \"sml4\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/camels_lux/#hydrodataset.camels_lux.CamelsLux.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_lux/#hydrodataset.camels_lux.CamelsLux.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_LUX dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_LUX data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camels_lux.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_LUX dataset.\n\n    Args:\n        data_path: Path to the CAMELS_LUX data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    try:\n        self.aqua_fetch = CAMELS_LUX(data_path)\n    except Exception as e:\n        print(e)\n        check_zip_extract = False\n        # The zip files that should be downloaded for CAMELS-LUX\n        zip_files = [\"CAMELS-LUX.zip\", \"CAMELS-LUX_shapefiles.zip\"]\n        for filename in tqdm(zip_files, desc=\"Checking zip files\"):\n            extracted_dir = self.data_source_dir.joinpath(\n                \"CAMELS_LUX\", filename[:-4]\n            )\n            if not extracted_dir.exists():\n                check_zip_extract = True\n                break\n        if check_zip_extract:\n            hydro_file.zip_extract(self.data_source_dir.joinpath(\"CAMELS_LUX\"))\n        self.aqua_fetch = CAMELS_LUX(data_path)\n</code></pre>"},{"location":"api/camels_nz/","title":"CAMELS-NZ","text":""},{"location":"api/camels_nz/#overview","title":"Overview","text":"<p>CAMELS-NZ is the New Zealand hydrological dataset implementation. New Zealand CAMELS dataset covering diverse climates from subtropical to alpine.</p>"},{"location":"api/camels_nz/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: New Zealand</li> <li>Module: <code>hydrodataset.camels_nz</code></li> <li>Class: <code>CamelsNz</code></li> </ul>"},{"location":"api/camels_nz/#features","title":"Features","text":""},{"location":"api/camels_nz/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_nz/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_nz/#usage","title":"Usage","text":""},{"location":"api/camels_nz/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_nz import CamelsNz\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsNz(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_nz/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_nz/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_nz/#api-reference","title":"API Reference","text":""},{"location":"api/camels_nz/#hydrodataset.camels_nz.CamelsNz","title":"<code>hydrodataset.camels_nz.CamelsNz</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_NZ dataset class.</p> <p>This class uses a custom data reading implementation to support a newer dataset version than the one supported by the underlying aquafetch library. It overrides the download URLs and provides its own parsing and caching logic.</p> <p>The dataset supports both hourly ('H') and daily ('D') timesteps.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>timestep</code> <p>Time step for the data ('H' for hourly, 'D' for daily)</p> Source code in <code>hydrodataset/camels_nz.py</code> <pre><code>class CamelsNz(HydroDataset):\n    \"\"\"CAMELS_NZ dataset class.\n\n    This class uses a custom data reading implementation to support a newer\n    dataset version than the one supported by the underlying aquafetch library.\n    It overrides the download URLs and provides its own parsing and caching logic.\n\n    The dataset supports both hourly ('H') and daily ('D') timesteps.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        timestep: Time step for the data ('H' for hourly, 'D' for daily)\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        region: Optional[str] = None,\n        download: bool = False,\n        timestep: str = \"H\",\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_NZ dataset.\n\n        Args:\n            data_path: Path to the CAMELS_NZ data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n            timestep: Time step for the data ('H' for hourly, 'D' for daily, default: 'H')\n        \"\"\"\n        super().__init__(data_path)\n        self.region = \"NZ\" if region is None else region\n        self.download = download\n        self.timestep = timestep\n\n        # Instantiate our custom CAMELS_NZ class with timestep support\n        self.aqua_fetch = CAMELS_NZ(data_path, timestep=timestep)\n\n    @property\n    def _attributes_cache_filename(self):\n        return f\"camels_nz_{self.timestep.lower()}_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return f\"camels_nz_{self.timestep.lower()}_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1972-01-01\", \"2024-08-02\"]\n\n    # Static variable definitions for CAMELS-NZ\n    # Note: specific_name should be the cleaned version (lowercase, no spaces)\n    # as stored in the cache file after _clean_feature_names() processing\n    _subclass_static_definitions = {\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"meanannualrainfall\", \"unit\": \"mm\"},\n    }\n\n    # Dynamic variable mapping for CAMELS-NZ\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"obs\",\n            \"sources\": {\"obs\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"}},\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"default\",\n            \"sources\": {\"default\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"default\",\n            \"sources\": {\"default\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"default\",\n            \"sources\": {\"default\": {\"specific_name\": \"pet_mm\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"default\",\n            \"sources\": {\"default\": {\"specific_name\": \"rh_\", \"unit\": \"%\"}},\n        },\n    }\n</code></pre>"},{"location":"api/camels_nz/#hydrodataset.camels_nz.CamelsNz.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_nz/#hydrodataset.camels_nz.CamelsNz.__init__","title":"<code>__init__(data_path, region=None, download=False, timestep='H')</code>","text":"<p>Initialize CAMELS_NZ dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_NZ data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> <code>timestep</code> <code>str</code> <p>Time step for the data ('H' for hourly, 'D' for daily, default: 'H')</p> <code>'H'</code> Source code in <code>hydrodataset/camels_nz.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    region: Optional[str] = None,\n    download: bool = False,\n    timestep: str = \"H\",\n) -&gt; None:\n    \"\"\"Initialize CAMELS_NZ dataset.\n\n    Args:\n        data_path: Path to the CAMELS_NZ data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n        timestep: Time step for the data ('H' for hourly, 'D' for daily, default: 'H')\n    \"\"\"\n    super().__init__(data_path)\n    self.region = \"NZ\" if region is None else region\n    self.download = download\n    self.timestep = timestep\n\n    # Instantiate our custom CAMELS_NZ class with timestep support\n    self.aqua_fetch = CAMELS_NZ(data_path, timestep=timestep)\n</code></pre>"},{"location":"api/camels_se/","title":"CAMELS-SE","text":""},{"location":"api/camels_se/#overview","title":"Overview","text":"<p>CAMELS-SE is the Sweden hydrological dataset implementation. Swedish CAMELS dataset for Scandinavian catchments.</p>"},{"location":"api/camels_se/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Sweden</li> <li>Module: <code>hydrodataset.camels_se</code></li> <li>Class: <code>CamelsSe</code></li> </ul>"},{"location":"api/camels_se/#features","title":"Features","text":""},{"location":"api/camels_se/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_se/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_se/#usage","title":"Usage","text":""},{"location":"api/camels_se/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_se import CamelsSe\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsSe(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_se/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_se/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_se/#api-reference","title":"API Reference","text":""},{"location":"api/camels_se/#hydrodataset.camels_se.CamelsSe","title":"<code>hydrodataset.camels_se.CamelsSe</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_SE dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELS_SE dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camels_se.py</code> <pre><code>class CamelsSe(HydroDataset):\n    \"\"\"CAMELS_SE dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELS_SE dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_SE dataset.\n\n        Args:\n            data_path: Path to the CAMELS_SE data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = CAMELS_SE(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_se_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_se_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1961-01-01\", \"2020-12-31\"]\n\n    # get the information of features from dataset file\"Documentation_2024-01-02.pdf\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"pmean_mm_year\", \"unit\": \"mm/year\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"obs_cms\",\n            \"sources\": {\n                \"obs_cms\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n                \"obs_mm\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"default\",\n            \"sources\": {\n                \"default\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"default\",\n            \"sources\": {\n                \"default\": {\"specific_name\": \"airtemp_C_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/camels_se/#hydrodataset.camels_se.CamelsSe.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_se/#hydrodataset.camels_se.CamelsSe.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_SE dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_SE data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camels_se.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_SE dataset.\n\n    Args:\n        data_path: Path to the CAMELS_SE data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = CAMELS_SE(data_path)\n</code></pre>"},{"location":"api/camels_us/","title":"CAMELS-US","text":""},{"location":"api/camels_us/#overview","title":"Overview","text":"<p>CAMELS-US is the United States hydrological dataset implementation. United States CAMELS dataset, one of the largest and most comprehensive hydrological datasets.</p>"},{"location":"api/camels_us/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: United States</li> <li>Module: <code>hydrodataset.camels_us</code></li> <li>Class: <code>CamelsUs</code></li> </ul>"},{"location":"api/camels_us/#features","title":"Features","text":""},{"location":"api/camels_us/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camels_us/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available (varies by dataset): - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camels_us/#usage","title":"Usage","text":""},{"location":"api/camels_us/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camels_us import CamelsUs\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsUs(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camels_us/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camels_us/#data-sources","title":"Data Sources","text":"<p>The dataset supports multiple data sources for certain variables. Check the class documentation for available sources and use tuple notation to specify:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify ERA5-Land source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/camels_us/#api-reference","title":"API Reference","text":""},{"location":"api/camels_us/#hydrodataset.camels_us.CamelsUs","title":"<code>hydrodataset.camels_us.CamelsUs</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_US dataset class.</p> <p>This class is a wrapper around the CAMELS_US class from the <code>aqua_fetch</code> package. It standardizes the dataset into a NetCDF format for easy use with hydrological models. It also includes custom logic to read the PET variable from model output files.</p> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>class CamelsUs(HydroDataset):\n    \"\"\"CAMELS_US dataset class.\n\n    This class is a wrapper around the CAMELS_US class from the `aqua_fetch` package.\n    It standardizes the dataset into a NetCDF format for easy use with hydrological models.\n    It also includes custom logic to read the PET variable from model output files.\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_US dataset.\n\n        Args:\n            data_path: Path to the CAMELS_US data directory. This is where the data will be stored.\n            region: Geographic region identifier (optional, defaults to US).\n            download: Whether to download data automatically (not used, handled by aqua_fetch).\n        \"\"\"\n        super().__init__(data_path)\n        self.region = \"US\" if region is None else region\n\n        # Instantiate the custom class defined at module level\n        self.aqua_fetch = CAMELS_US(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_us_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_us_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1980-01-01\", \"2014-12-31\"]\n\n    def _dynamic_features(self) -&gt; list:\n        \"\"\"\n        Overrides the base method to include 'PET' as a dynamic feature.\n        \"\"\"\n        # Get the default features from the parent class (from aquafetch)\n        features = super()._dynamic_features()\n        # Add the custom PET and ET variables\n        features.extend([\"PET\", \"ET\"])\n        return features\n\n    def read_camels_us_model_output_data(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        forcing_type=\"daymet\",\n    ) -&gt; np.array:\n        \"\"\"\n        Read model output data of CAMELS-US, including PET.\n        This is a legacy function migrated from the old camels.py.\n        \"\"\"\n        # Fetch HUC codes for the requested basins on-the-fly\n        try:\n            huc_ds = self.read_attr_xrdataset(\n                gage_id_lst=gage_id_lst, var_lst=[\"huc_02\"], to_numeric=False\n            )\n            huc_df = huc_ds.to_dataframe()\n        except Exception as e:\n            raise RuntimeError(\n                f\"Could not read HUC attributes to get model output data: {e}\"\n            )\n\n        t_range_list = pd.date_range(start=t_range[0], end=t_range[1], freq=\"D\").values\n        model_out_put_var_lst = [\n            \"SWE\",\n            \"PRCP\",\n            \"RAIM\",\n            \"TAIR\",\n            \"PET\",\n            \"ET\",\n            \"MOD_RUN\",\n            \"OBS_RUN\",\n        ]\n        if not set(var_lst).issubset(set(model_out_put_var_lst)):\n            raise RuntimeError(\n                f\"Requested variables not in model output list: {var_lst}\"\n            )\n\n        nt = len(t_range_list)\n        chosen_camels_mods = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n\n        for i, usgs_id in enumerate(\n            tqdm(gage_id_lst, desc=\"Read model output data (PET and ET) for CAMELS-US\")\n        ):\n            try:\n                huc02_ = huc_df.loc[usgs_id, \"huc_02\"]\n                # Convert to 2-digit string with leading zeros if needed\n                huc02_ = f\"{int(huc02_):02d}\"\n            except KeyError:\n                print(\n                    f\"Warning: No HUC attribute found for {usgs_id}, skipping PET and ET reading.\"\n                )\n                continue\n\n            # Construct path to model output files\n            file_path_dir = os.path.join(\n                self.data_source_dir,\n                \"CAMELS_US\",\n                \"basin_timeseries_v1p2_modelOutput_\" + forcing_type,\n                \"model_output_\" + forcing_type,\n                \"model_output\",\n                \"flow_timeseries\",\n                forcing_type,\n                huc02_,\n            )\n\n            if not os.path.isdir(file_path_dir):\n                # This warning is kept for cases where the directory might be missing for a valid HUC\n                # print(f\"Warning: Model output directory not found: {file_path_dir}\")\n                continue\n\n            sac_random_seeds = [\n                \"05\",\n                \"11\",\n                \"27\",\n                \"33\",\n                \"48\",\n                \"59\",\n                \"66\",\n                \"72\",\n                \"80\",\n                \"94\",\n            ]\n            files = [\n                os.path.join(file_path_dir, f\"{usgs_id}_{seed}_model_output.txt\")\n                for seed in sac_random_seeds\n            ]\n\n            results = []\n            for file in files:\n                if not os.path.exists(file):\n                    continue\n                try:\n                    result = pd.read_csv(file, sep=r\"\\s+\")\n                    df_date = result[[\"YR\", \"MNTH\", \"DY\"]]\n                    df_date.columns = [\"year\", \"month\", \"day\"]\n                    date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n\n                    c, ind1, ind2 = np.intersect1d(\n                        date, t_range_list, return_indices=True\n                    )\n                    if len(c) &gt; 0:\n                        temp_data = np.full([nt, len(var_lst)], np.nan)\n                        temp_data[ind2, :] = result[var_lst].values[ind1]\n                        results.append(temp_data)\n                except Exception as e:\n                    print(f\"Warning: Failed to read {file}: {e}\")\n\n            if results:\n                result_np = np.array(results)\n                # Calculate mean across different random seeds\n                with np.errstate(\n                    invalid=\"ignore\"\n                ):  # Ignore warnings from all-NaN slices\n                    chosen_camels_mods[i, :, :] = np.nanmean(result_np, axis=0)\n\n        return chosen_camels_mods\n\n    def cache_timeseries_xrdataset(self):\n        \"\"\"\n        Overrides the base method to create a complete cache file including PET.\n\n        This method first calls the parent implementation to create the base cache\n        from aquafetch data, then reads the custom PET data and merges it into the\n        same cache file.\n        \"\"\"\n        # First, create the base cache file using the parent method\n        print(\"Creating base time-series cache from aquafetch...\")\n        super().cache_timeseries_xrdataset()\n\n        # Now, read the PET and ET data for all basins for the default time range\n        print(\"Reading PET and ET data to add to the cache...\")\n        gage_id_lst = self.read_object_ids().tolist()\n        model_output_data = self.read_camels_us_model_output_data(\n            gage_id_lst=gage_id_lst, t_range=self.default_t_range, var_lst=[\"PET\", \"ET\"]\n        )\n\n        cache_file = self.cache_dir.joinpath(self._timeseries_cache_filename)\n\n        # Use a with statement to ensure the dataset is closed before writing\n        with xr.open_dataset(cache_file) as ds:\n            print(f\"Variables in base cache: {list(ds.data_vars.keys())}\")\n            # Create xarray.DataArrays for PET and ET\n            pet_da = xr.DataArray(\n                model_output_data[:, :, 0],  # PET data\n                coords={\"basin\": gage_id_lst, \"time\": ds.time},\n                dims=[\"basin\", \"time\"],\n                attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n                name=\"PET\",\n            )\n            et_da = xr.DataArray(\n                model_output_data[:, :, 1],  # ET data\n                coords={\"basin\": gage_id_lst, \"time\": ds.time},\n                dims=[\"basin\", \"time\"],\n                attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n                name=\"ET\",\n            )\n            # Merge PET and ET into the main dataset\n            # Load the dataset into memory to avoid issues with lazy loading\n            merged_ds = ds.load().merge(pet_da).merge(et_da)\n\n        # Now that the original file is closed, we can safely overwrite it\n        print(\"Saving final cache file with merged PET and ET data...\")\n        print(f\"Variables in merged dataset: {list(merged_ds.data_vars.keys())}\")\n        merged_ds.to_netcdf(cache_file, mode=\"w\")\n        print(f\"Successfully saved final cache to: {cache_file}\")\n\n    _subclass_static_definitions = {\n        \"huc_02\": {\"specific_name\": \"huc_02\", \"unit\": \"dimensionless\"},\n        \"gauge_lat\": {\"specific_name\": \"lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"long\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"elev_mean\", \"unit\": \"m\"},\n        \"slope_mean\": {\"specific_name\": \"slope_mkm1\", \"unit\": \"m/km\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"geol_1st_class\": {\"specific_name\": \"geol_1st_class\", \"unit\": \"dimensionless\"},\n        \"geol_2nd_class\": {\"specific_name\": \"geol_2nd_class\", \"unit\": \"dimensionless\"},\n        \"geol_porostiy\": {\"specific_name\": \"geol_porostiy\", \"unit\": \"dimensionless\"},\n        \"geol_permeability\": {\"specific_name\": \"geol_permeability\", \"unit\": \"m^2\"},\n        \"frac_forest\": {\"specific_name\": \"frac_forest\", \"unit\": \"dimensionless\"},\n        \"lai_max\": {\"specific_name\": \"lai_max\", \"unit\": \"dimensionless\"},\n        \"lai_diff\": {\"specific_name\": \"lai_diff\", \"unit\": \"dimensionless\"},\n        \"dom_land_cover_frac\": {\n            \"specific_name\": \"dom_land_cover_frac\",\n            \"unit\": \"dimensionless\",\n        },\n        \"dom_land_cover\": {\"specific_name\": \"dom_land_cover\", \"unit\": \"dimensionless\"},\n        \"root_depth_50\": {\"specific_name\": \"root_depth_50\", \"unit\": \"m\"},\n        \"root_depth_99\": {\"specific_name\": \"root_depth_99\", \"unit\": \"m\"},\n        \"soil_depth_statsgo\": {\"specific_name\": \"soil_depth_statsgo\", \"unit\": \"m\"},\n        \"soil_porosity\": {\"specific_name\": \"soil_porosity\", \"unit\": \"dimensionless\"},\n        \"soil_conductivity\": {\"specific_name\": \"soil_conductivity\", \"unit\": \"cm/hr\"},\n        \"max_water_content\": {\"specific_name\": \"max_water_content\", \"unit\": \"m\"},\n        \"pet_mean\": {\"specific_name\": \"pet_mean\", \"unit\": \"mm/day\"},\n    }\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"usgs\",\n            \"sources\": {\"usgs\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"}},\n        },\n        # TODO: For maurer and nldas, we have not checked the specific names and units.\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n                \"maurer\": {\"specific_name\": \"prcp_maurer\", \"unit\": \"mm/day\"},\n                \"nldas\": {\"specific_name\": \"prcp_nldas\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"},\n                \"maurer\": {\"specific_name\": \"tmax_maurer\", \"unit\": \"\u00b0C\"},\n                \"nldas\": {\"specific_name\": \"tmax_nldas\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"},\n                \"maurer\": {\"specific_name\": \"tmin_maurer\", \"unit\": \"\u00b0C\"},\n                \"nldas\": {\"specific_name\": \"tmin_nldas\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.DAYLIGHT_DURATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"dayl\", \"unit\": \"s\"},\n                \"maurer\": {\"specific_name\": \"dayl_maurer\", \"unit\": \"s\"},\n                \"nldas\": {\"specific_name\": \"dayl_nldas\", \"unit\": \"s\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"},\n                \"maurer\": {\"specific_name\": \"srad_maurer\", \"unit\": \"W/m^2\"},\n                \"nldas\": {\"specific_name\": \"srad_nldas\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"swe_mm\", \"unit\": \"mm/day\"},\n                \"maurer\": {\"specific_name\": \"swe_maurer\", \"unit\": \"mm/day\"},\n                \"nldas\": {\"specific_name\": \"swe_nldas\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.VAPOR_PRESSURE: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"vp_hpa\", \"unit\": \"hPa\"},\n                \"maurer\": {\"specific_name\": \"vp_maurer\", \"unit\": \"hPa\"},\n                \"nldas\": {\"specific_name\": \"vp_nldas\", \"unit\": \"hPa\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"sac-sma\",\n            \"sources\": {\"sac-sma\": {\"specific_name\": \"PET\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"sac-sma\",\n            \"sources\": {\"sac-sma\": {\"specific_name\": \"ET\", \"unit\": \"mm/day\"}},\n        },\n    }\n</code></pre>"},{"location":"api/camels_us/#hydrodataset.camels_us.CamelsUs.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camels_us/#hydrodataset.camels_us.CamelsUs.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_US dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_US data directory. This is where the data will be stored.</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional, defaults to US).</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (not used, handled by aqua_fetch).</p> <code>False</code> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_US dataset.\n\n    Args:\n        data_path: Path to the CAMELS_US data directory. This is where the data will be stored.\n        region: Geographic region identifier (optional, defaults to US).\n        download: Whether to download data automatically (not used, handled by aqua_fetch).\n    \"\"\"\n    super().__init__(data_path)\n    self.region = \"US\" if region is None else region\n\n    # Instantiate the custom class defined at module level\n    self.aqua_fetch = CAMELS_US(data_path)\n</code></pre>"},{"location":"api/camelsh/","title":"CAMELSH","text":""},{"location":"api/camelsh/#overview","title":"Overview","text":"<p>CAMELSH is the United States hourly hydrological dataset. Hourly resolution hydrological dataset for US catchments, providing high-temporal-resolution data for detailed hydrological analysis.</p>"},{"location":"api/camelsh/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: United States</li> <li>Temporal Resolution: Hourly</li> <li>Module: <code>hydrodataset.camelsh</code></li> <li>Class: <code>Camelsh</code></li> </ul>"},{"location":"api/camelsh/#key-features","title":"Key Features","text":""},{"location":"api/camelsh/#hourly-resolution","title":"Hourly Resolution","text":"<p>Unlike daily CAMELS datasets, CAMELSH provides hourly timeseries data, enabling: - Sub-daily hydrological process analysis - Flash flood and storm event studies - High-frequency streamflow dynamics - Detailed precipitation event analysis</p>"},{"location":"api/camelsh/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camelsh/#dynamic-variables","title":"Dynamic Variables","text":"<p>Hourly timeseries variables available: - Streamflow (hourly) - Precipitation (hourly) - Temperature - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camelsh/#usage","title":"Usage","text":""},{"location":"api/camelsh/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camelsh import Camelsh\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = Camelsh(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read hourly timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"2015-01-01\", \"2015-01-31\"],  # One month of hourly data\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camelsh/#analyzing-storm-events","title":"Analyzing Storm Events","text":"<pre><code># Read hourly data for a specific storm event\nstorm_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:3],\n    t_range=[\"2015-06-15 00:00:00\", \"2015-06-20 23:00:00\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Analyze sub-daily patterns\nimport xarray as xr\nhourly_precip = storm_data[\"precipitation\"]\ndaily_total = hourly_precip.resample(time=\"1D\").sum()\nprint(\"Daily precipitation totals:\", daily_total)\n</code></pre>"},{"location":"api/camelsh/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range (note hourly timestamps)\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"2015-01-01 00:00:00\", \"2015-12-31 23:00:00\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camelsh/#data-considerations","title":"Data Considerations","text":""},{"location":"api/camelsh/#large-data-volumes","title":"Large Data Volumes","text":"<p>Hourly data results in significantly larger datasets compared to daily data: - 24x more data points per day - Larger cache files - Longer initial cache generation time</p>"},{"location":"api/camelsh/#time-range-selection","title":"Time Range Selection","text":"<p>When working with hourly data: <pre><code># Specify full datetime for hourly data\nt_range = [\"2015-01-01 00:00:00\", \"2015-01-31 23:00:00\"]\n\n# Or use date strings (defaults to 00:00:00)\nt_range = [\"2015-01-01\", \"2015-01-31\"]\n</code></pre></p>"},{"location":"api/camelsh/#api-reference","title":"API Reference","text":""},{"location":"api/camelsh/#hydrodataset.camelsh.Camelsh","title":"<code>hydrodataset.camelsh.Camelsh</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELSH (CAMELS-Hourly) dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELSH dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camelsh.py</code> <pre><code>class Camelsh(HydroDataset):\n    \"\"\"CAMELSH (CAMELS-Hourly) dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELSH dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELSH dataset.\n\n        Args:\n            data_path: Path to the CAMELSH data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n\n        # Use the fixed CAMELSH class with corrected directory paths\n        self.aqua_fetch = CAMELSH(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camelsh_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camelsh_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1980-01-01\", \"2024-12-31\"]\n\n    _subclass_static_definitions = {\n        # Basic station information\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"p_seasonality\": {\"specific_name\": \"p_seasonality\", \"unit\": \"none\"},\n        \"frac_snow\": {\"specific_name\": \"frac_snow\", \"unit\": \"none\"},\n        \"aridity\": {\"specific_name\": \"aridity_index\", \"unit\": \"none\"},\n    }\n    _dynamic_variable_mapping = {\n        # unit in aquafetch is m^3/s.in paper is kg/m^2\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\"nldas\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"kg/m^2\"}},\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.LONGWAVE_SOLAR_RADIATION: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"lwdown\", \"unit\": \"W/m^2\"},\n            },\n        },\n        # Shortwave radiation flux downwards (surface)\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"swdown\", \"unit\": \"W/m^2\"},\n            },\n        },\n        # unit in aquafetch is mm/day.in paper is kg/m^2\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\"nldas\": {\"specific_name\": \"pet_mm\", \"unit\": \"kg/m^2\"}},\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"psurf\", \"unit\": \"Pa\"},\n            },\n        },\n        # 10-meter above ground Zonal wind speed(east to west)\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"wind_e\", \"unit\": \"m/s\"},\n            },\n        },\n        # 10-meter above ground Meridional wind speed(north to south)\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"wind_n\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"qair\", \"unit\": \"kg/kg\"},\n            },\n        },\n        StandardVariable.WATER_LEVEL: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"water_level\", \"unit\": \"m\"},\n            },\n        },\n        StandardVariable.CAPE: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"cape\", \"unit\": \"J/kg\"},\n            },\n        },\n        StandardVariable.CRAINF_FRAC: {\n            \"default_source\": \"nldas\",\n            \"sources\": {\n                \"nldas\": {\"specific_name\": \"crainf_frac\", \"unit\": \"Fraction\"},\n            },\n        },\n    }\n\n    def cache_timeseries_xrdataset(self, batch_size=100):\n        \"\"\"\n        Cache timeseries data to NetCDF files in batches, each batch saved as a separate file\n\n        Args:\n            batch_size: Number of stations to process per batch, default is 100 stations\n        \"\"\"\n        if not hasattr(self, \"aqua_fetch\"):\n            raise NotImplementedError(\"aqua_fetch attribute is required\")\n\n        # Build mapping from variable names to units\n        unit_lookup = {}\n        if hasattr(self, \"_dynamic_variable_mapping\"):\n            for std_name, mapping_info in self._dynamic_variable_mapping.items():\n                for source, source_info in mapping_info[\"sources\"].items():\n                    unit_lookup[source_info[\"specific_name\"]] = source_info[\"unit\"]\n\n        # Get all station IDs\n        gage_id_lst = self.read_object_ids().tolist()\n        total_stations = len(gage_id_lst)\n\n        # Get original variable list and clean\n        original_var_lst = self.aqua_fetch.dynamic_features\n        cleaned_var_lst = self._clean_feature_names(original_var_lst)\n        var_name_mapping = dict(zip(original_var_lst, cleaned_var_lst))\n\n        print(\n            f\"Start batch processing {total_stations} stations, {batch_size} stations per batch\"\n        )\n        print(\n            f\"Total number of batches: {(total_stations + batch_size - 1)//batch_size}\"\n        )\n\n        # Ensure cache directory exists\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n        # Process stations in batches and save independently\n        batch_num = 1\n        for batch_idx in range(0, total_stations, batch_size):\n            batch_end = min(batch_idx + batch_size, total_stations)\n            batch_stations = gage_id_lst[batch_idx:batch_end]\n\n            print(\n                f\"\\nProcessing batch {batch_num}/{(total_stations + batch_size - 1)//batch_size}\"\n            )\n            print(\n                f\"Station range: {batch_idx} - {batch_end-1} (total {len(batch_stations)} stations)\"\n            )\n\n            try:\n                # Get data for this batch\n                batch_data = self.aqua_fetch.fetch_stations_features(\n                    stations=batch_stations,\n                    dynamic_features=original_var_lst,\n                    static_features=None,\n                    st=self.default_t_range[0],\n                    en=self.default_t_range[1],\n                    as_dataframe=False,\n                )\n\n                dynamic_data = (\n                    batch_data[1] if isinstance(batch_data, tuple) else batch_data\n                )\n\n                # Process variables\n                new_data_vars = {}\n                time_coord = dynamic_data.coords[\"time\"]\n\n                for original_var in tqdm(\n                    original_var_lst,\n                    desc=f\"Processing variables (batch {batch_num})\",\n                    total=len(original_var_lst),\n                ):\n                    cleaned_var = var_name_mapping[original_var]\n                    var_data = []\n                    for station in batch_stations:\n                        if station in dynamic_data.data_vars:\n                            station_data = dynamic_data[station].sel(\n                                dynamic_features=original_var\n                            )\n                            if \"dynamic_features\" in station_data.coords:\n                                station_data = station_data.drop(\"dynamic_features\")\n                            var_data.append(station_data)\n\n                    if var_data:\n                        combined = xr.concat(var_data, dim=\"basin\")\n                        combined[\"basin\"] = batch_stations\n                        combined.attrs[\"units\"] = unit_lookup.get(\n                            cleaned_var, \"unknown\"\n                        )\n                        new_data_vars[cleaned_var] = combined\n\n                # Create Dataset for this batch\n                batch_ds = xr.Dataset(\n                    data_vars=new_data_vars,\n                    coords={\n                        \"basin\": batch_stations,\n                        \"time\": time_coord,\n                    },\n                )\n\n                # Save this batch to independent file\n                batch_filename = f\"batch{batch_num:03d}_camelsh_timeseries.nc\"\n                batch_filepath = self.cache_dir.joinpath(batch_filename)\n\n                print(f\"Saving batch {batch_num} to: {batch_filepath}\")\n                batch_ds.to_netcdf(batch_filepath)\n                print(f\"Batch {batch_num} saved successfully\")\n\n            except Exception as e:\n                print(f\"Batch {batch_num} processing failed: {e}\")\n                import traceback\n\n                traceback.print_exc()\n                continue\n\n            batch_num += 1\n\n        print(f\"\\nAll batches processed! Total {batch_num - 1} batch files saved\")\n\n    def read_ts_xrdataset(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        sources: dict = None,\n        **kwargs,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read timeseries data (supports standardized variable names and multiple data sources)\n\n        Read data from batch-saved cache files\n\n        Args:\n            gage_id_lst: List of station IDs\n            t_range: Time range [start, end]\n            var_lst: List of standard variable names\n            sources: Data source dictionary, format is {variable_name: data_source} or {variable_name: [data_source_list]}\n\n        Returns:\n            xr.Dataset: xarray dataset containing requested data\n        \"\"\"\n        if (\n            not hasattr(self, \"_dynamic_variable_mapping\")\n            or not self._dynamic_variable_mapping\n        ):\n            raise NotImplementedError(\n                \"This dataset does not support the standardized variable mapping.\"\n            )\n\n        if var_lst is None:\n            var_lst = list(self._dynamic_variable_mapping.keys())\n\n        if t_range is None:\n            t_range = self.default_t_range\n\n        target_vars_to_fetch = []\n        rename_map = {}\n\n        # Process variable name mapping and data source selection\n        for std_name in var_lst:\n            if std_name not in self._dynamic_variable_mapping:\n                raise ValueError(\n                    f\"'{std_name}' is not a recognized standard variable for this dataset.\"\n                )\n\n            mapping_info = self._dynamic_variable_mapping[std_name]\n\n            # Determine which data source(s) to use\n            is_explicit_source = sources and std_name in sources\n            sources_to_use = []\n            if is_explicit_source:\n                provided_sources = sources[std_name]\n                if isinstance(provided_sources, list):\n                    sources_to_use.extend(provided_sources)\n                else:\n                    sources_to_use.append(provided_sources)\n            else:\n                sources_to_use.append(mapping_info[\"default_source\"])\n\n            # Only need suffix when user explicitly requests multiple data sources\n            needs_suffix = is_explicit_source and len(sources_to_use) &gt; 1\n            for source in sources_to_use:\n                if source not in mapping_info[\"sources\"]:\n                    raise ValueError(\n                        f\"Source '{source}' is not available for variable '{std_name}'.\"\n                    )\n\n                actual_var_name = mapping_info[\"sources\"][source][\"specific_name\"]\n                target_vars_to_fetch.append(actual_var_name)\n                output_name = f\"{std_name}_{source}\" if needs_suffix else std_name\n                rename_map[actual_var_name] = output_name\n\n        # Find all batch files\n        import glob\n\n        batch_pattern = str(self.cache_dir / \"batch*_camelsh_timeseries.nc\")\n        batch_files = sorted(glob.glob(batch_pattern))\n\n        if not batch_files:\n            print(\"No batch cache files found, starting cache creation...\")\n            self.cache_timeseries_xrdataset()\n            batch_files = sorted(glob.glob(batch_pattern))\n\n            if not batch_files:\n                raise FileNotFoundError(\"Cache creation failed, no batch files found\")\n\n        print(f\"Found {len(batch_files)} batch files\")\n\n        # If no stations specified, read all stations\n        if gage_id_lst is None:\n            print(\"No station list specified, will read all stations...\")\n            gage_id_lst = self.read_object_ids().tolist()\n\n        # Convert station IDs to strings (ensure consistency)\n        gage_id_lst = [str(gid) for gid in gage_id_lst]\n\n        # Iterate through batch files to find batches containing required stations\n        relevant_datasets = []\n        for batch_file in batch_files:\n            try:\n                # First open only coordinates, don't load data\n                ds_batch = xr.open_dataset(batch_file)\n                batch_basins = [str(b) for b in ds_batch.basin.values]\n\n                # Check if this batch contains required stations\n                common_basins = list(set(gage_id_lst) &amp; set(batch_basins))\n\n                if common_basins:\n                    print(\n                        f\"Batch {os.path.basename(batch_file)}: contains {len(common_basins)} required stations\"\n                    )\n\n                    # Check if variables exist\n                    missing_vars = [\n                        v for v in target_vars_to_fetch if v not in ds_batch.data_vars\n                    ]\n                    if missing_vars:\n                        ds_batch.close()\n                        raise ValueError(\n                            f\"Batch {os.path.basename(batch_file)} missing variables: {missing_vars}\"\n                        )\n\n                    # Select variables and stations\n                    ds_subset = ds_batch[target_vars_to_fetch]\n                    ds_selected = ds_subset.sel(\n                        basin=common_basins, time=slice(t_range[0], t_range[1])\n                    )\n\n                    relevant_datasets.append(ds_selected)\n                    ds_batch.close()\n                else:\n                    ds_batch.close()\n\n            except Exception as e:\n                print(f\"Failed to read batch file {batch_file}: {e}\")\n                continue\n\n        if not relevant_datasets:\n            raise ValueError(\n                f\"Specified stations not found in any batch files: {gage_id_lst}\"\n            )\n\n        print(f\"Reading data from {len(relevant_datasets)} batches...\")\n\n        # Merge data from all relevant batches\n        if len(relevant_datasets) == 1:\n            final_ds = relevant_datasets[0]\n        else:\n            final_ds = xr.concat(relevant_datasets, dim=\"basin\")\n\n        # Rename to standard variable names\n        final_ds = final_ds.rename(rename_map)\n\n        # Ensure stations are arranged in input order\n        if len(gage_id_lst) &gt; 0:\n            # Only select actually existing stations\n            existing_basins = [b for b in gage_id_lst if b in final_ds.basin.values]\n            if existing_basins:\n                final_ds = final_ds.sel(basin=existing_basins)\n\n        return final_ds\n</code></pre>"},{"location":"api/camelsh/#hydrodataset.camelsh.Camelsh.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camelsh/#hydrodataset.camelsh.Camelsh.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELSH dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELSH data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/camelsh.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELSH dataset.\n\n    Args:\n        data_path: Path to the CAMELSH data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n\n    # Use the fixed CAMELSH class with corrected directory paths\n    self.aqua_fetch = CAMELSH(data_path)\n</code></pre>"},{"location":"api/camelsh/#hydrodataset.camelsh.Camelsh.read_ts_xrdataset","title":"<code>read_ts_xrdataset(gage_id_lst=None, t_range=None, var_lst=None, sources=None, **kwargs)</code>","text":"<p>Read timeseries data (supports standardized variable names and multiple data sources)</p> <p>Read data from batch-saved cache files</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list</code> <p>List of station IDs</p> <code>None</code> <code>t_range</code> <code>list</code> <p>Time range [start, end]</p> <code>None</code> <code>var_lst</code> <code>list</code> <p>List of standard variable names</p> <code>None</code> <code>sources</code> <code>dict</code> <p>Data source dictionary, format is {variable_name: data_source} or {variable_name: [data_source_list]}</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: xarray dataset containing requested data</p> Source code in <code>hydrodataset/camelsh.py</code> <pre><code>def read_ts_xrdataset(\n    self,\n    gage_id_lst: list = None,\n    t_range: list = None,\n    var_lst: list = None,\n    sources: dict = None,\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read timeseries data (supports standardized variable names and multiple data sources)\n\n    Read data from batch-saved cache files\n\n    Args:\n        gage_id_lst: List of station IDs\n        t_range: Time range [start, end]\n        var_lst: List of standard variable names\n        sources: Data source dictionary, format is {variable_name: data_source} or {variable_name: [data_source_list]}\n\n    Returns:\n        xr.Dataset: xarray dataset containing requested data\n    \"\"\"\n    if (\n        not hasattr(self, \"_dynamic_variable_mapping\")\n        or not self._dynamic_variable_mapping\n    ):\n        raise NotImplementedError(\n            \"This dataset does not support the standardized variable mapping.\"\n        )\n\n    if var_lst is None:\n        var_lst = list(self._dynamic_variable_mapping.keys())\n\n    if t_range is None:\n        t_range = self.default_t_range\n\n    target_vars_to_fetch = []\n    rename_map = {}\n\n    # Process variable name mapping and data source selection\n    for std_name in var_lst:\n        if std_name not in self._dynamic_variable_mapping:\n            raise ValueError(\n                f\"'{std_name}' is not a recognized standard variable for this dataset.\"\n            )\n\n        mapping_info = self._dynamic_variable_mapping[std_name]\n\n        # Determine which data source(s) to use\n        is_explicit_source = sources and std_name in sources\n        sources_to_use = []\n        if is_explicit_source:\n            provided_sources = sources[std_name]\n            if isinstance(provided_sources, list):\n                sources_to_use.extend(provided_sources)\n            else:\n                sources_to_use.append(provided_sources)\n        else:\n            sources_to_use.append(mapping_info[\"default_source\"])\n\n        # Only need suffix when user explicitly requests multiple data sources\n        needs_suffix = is_explicit_source and len(sources_to_use) &gt; 1\n        for source in sources_to_use:\n            if source not in mapping_info[\"sources\"]:\n                raise ValueError(\n                    f\"Source '{source}' is not available for variable '{std_name}'.\"\n                )\n\n            actual_var_name = mapping_info[\"sources\"][source][\"specific_name\"]\n            target_vars_to_fetch.append(actual_var_name)\n            output_name = f\"{std_name}_{source}\" if needs_suffix else std_name\n            rename_map[actual_var_name] = output_name\n\n    # Find all batch files\n    import glob\n\n    batch_pattern = str(self.cache_dir / \"batch*_camelsh_timeseries.nc\")\n    batch_files = sorted(glob.glob(batch_pattern))\n\n    if not batch_files:\n        print(\"No batch cache files found, starting cache creation...\")\n        self.cache_timeseries_xrdataset()\n        batch_files = sorted(glob.glob(batch_pattern))\n\n        if not batch_files:\n            raise FileNotFoundError(\"Cache creation failed, no batch files found\")\n\n    print(f\"Found {len(batch_files)} batch files\")\n\n    # If no stations specified, read all stations\n    if gage_id_lst is None:\n        print(\"No station list specified, will read all stations...\")\n        gage_id_lst = self.read_object_ids().tolist()\n\n    # Convert station IDs to strings (ensure consistency)\n    gage_id_lst = [str(gid) for gid in gage_id_lst]\n\n    # Iterate through batch files to find batches containing required stations\n    relevant_datasets = []\n    for batch_file in batch_files:\n        try:\n            # First open only coordinates, don't load data\n            ds_batch = xr.open_dataset(batch_file)\n            batch_basins = [str(b) for b in ds_batch.basin.values]\n\n            # Check if this batch contains required stations\n            common_basins = list(set(gage_id_lst) &amp; set(batch_basins))\n\n            if common_basins:\n                print(\n                    f\"Batch {os.path.basename(batch_file)}: contains {len(common_basins)} required stations\"\n                )\n\n                # Check if variables exist\n                missing_vars = [\n                    v for v in target_vars_to_fetch if v not in ds_batch.data_vars\n                ]\n                if missing_vars:\n                    ds_batch.close()\n                    raise ValueError(\n                        f\"Batch {os.path.basename(batch_file)} missing variables: {missing_vars}\"\n                    )\n\n                # Select variables and stations\n                ds_subset = ds_batch[target_vars_to_fetch]\n                ds_selected = ds_subset.sel(\n                    basin=common_basins, time=slice(t_range[0], t_range[1])\n                )\n\n                relevant_datasets.append(ds_selected)\n                ds_batch.close()\n            else:\n                ds_batch.close()\n\n        except Exception as e:\n            print(f\"Failed to read batch file {batch_file}: {e}\")\n            continue\n\n    if not relevant_datasets:\n        raise ValueError(\n            f\"Specified stations not found in any batch files: {gage_id_lst}\"\n        )\n\n    print(f\"Reading data from {len(relevant_datasets)} batches...\")\n\n    # Merge data from all relevant batches\n    if len(relevant_datasets) == 1:\n        final_ds = relevant_datasets[0]\n    else:\n        final_ds = xr.concat(relevant_datasets, dim=\"basin\")\n\n    # Rename to standard variable names\n    final_ds = final_ds.rename(rename_map)\n\n    # Ensure stations are arranged in input order\n    if len(gage_id_lst) &gt; 0:\n        # Only select actually existing stations\n        existing_basins = [b for b in gage_id_lst if b in final_ds.basin.values]\n        if existing_basins:\n            final_ds = final_ds.sel(basin=existing_basins)\n\n    return final_ds\n</code></pre>"},{"location":"api/camelsh_kr/","title":"CAMELSH-KR","text":""},{"location":"api/camelsh_kr/#overview","title":"Overview","text":"<p>CAMELSH-KR is the South Korea hourly hydrological dataset. Hourly resolution hydrological dataset for South Korean catchments with monsoon climate characteristics.</p>"},{"location":"api/camelsh_kr/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: South Korea</li> <li>Temporal Resolution: Hourly</li> <li>Module: <code>hydrodataset.camelsh_kr</code></li> <li>Class: <code>CamelshKr</code></li> </ul>"},{"location":"api/camelsh_kr/#key-features","title":"Key Features","text":""},{"location":"api/camelsh_kr/#hourly-resolution","title":"Hourly Resolution","text":"<p>Unlike daily CAMELS datasets, CAMELSH provides hourly timeseries data, enabling: - Sub-daily hydrological process analysis - Flash flood and storm event studies - High-frequency streamflow dynamics - Detailed precipitation event analysis</p>"},{"location":"api/camelsh_kr/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/camelsh_kr/#dynamic-variables","title":"Dynamic Variables","text":"<p>Hourly timeseries variables available: - Streamflow (hourly) - Precipitation (hourly) - Temperature - Potential evapotranspiration - Solar radiation - And more...</p>"},{"location":"api/camelsh_kr/#usage","title":"Usage","text":""},{"location":"api/camelsh_kr/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.camelsh_kr import CamelshKr\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelshKr(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read hourly timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"2015-01-01\", \"2015-01-31\"],  # One month of hourly data\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/camelsh_kr/#analyzing-storm-events","title":"Analyzing Storm Events","text":"<pre><code># Read hourly data for a specific storm event\nstorm_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:3],\n    t_range=[\"2015-06-15 00:00:00\", \"2015-06-20 23:00:00\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Analyze sub-daily patterns\nimport xarray as xr\nhourly_precip = storm_data[\"precipitation\"]\ndaily_total = hourly_precip.resample(time=\"1D\").sum()\nprint(\"Daily precipitation totals:\", daily_total)\n</code></pre>"},{"location":"api/camelsh_kr/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range (note hourly timestamps)\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"2015-01-01 00:00:00\", \"2015-12-31 23:00:00\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/camelsh_kr/#data-considerations","title":"Data Considerations","text":""},{"location":"api/camelsh_kr/#large-data-volumes","title":"Large Data Volumes","text":"<p>Hourly data results in significantly larger datasets compared to daily data: - 24x more data points per day - Larger cache files - Longer initial cache generation time</p>"},{"location":"api/camelsh_kr/#time-range-selection","title":"Time Range Selection","text":"<p>When working with hourly data: <pre><code># Specify full datetime for hourly data\nt_range = [\"2015-01-01 00:00:00\", \"2015-01-31 23:00:00\"]\n\n# Or use date strings (defaults to 00:00:00)\nt_range = [\"2015-01-01\", \"2015-01-31\"]\n</code></pre></p>"},{"location":"api/camelsh_kr/#api-reference","title":"API Reference","text":""},{"location":"api/camelsh_kr/#hydrodataset.camelsh_kr.CamelshKr","title":"<code>hydrodataset.camelsh_kr.CamelshKr</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELSH_KR dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELSH_KR dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camelsh_kr.py</code> <pre><code>class CamelshKr(HydroDataset):\n    \"\"\"CAMELSH_KR dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELSH_KR dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        region: Optional[str] = None,\n        download: bool = False,\n        cache_path: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize CAMELSH_KR dataset.\n\n        Args:\n            data_path: Path to the CAMELSH_KR data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n            cache_path: Path to the cache directory\n        \"\"\"\n        super().__init__(data_path, cache_path=cache_path)\n        self.region = region\n        self.download = download\n        # In aqua_fetch, CAMELS_SK is the alias of CAMELSH_KR\n        self.aqua_fetch = CAMELS_SK(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_sk_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_sk_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"2000-01-01\", \"2019-12-31\"]\n\n    # not find information of features\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"obs\",\n            \"sources\": {\n                \"obs\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n            },\n        },\n        StandardVariable.WATER_LEVEL: {\n            \"default_source\": \"obs\",\n            \"sources\": {\n                \"obs\": {\"specific_name\": \"water_level\", \"unit\": \"m\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"total_precipitation\", \"unit\": \"mm/day\"},\n                \"obs\": {\"specific_name\": \"precip_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"temperature_2m\", \"unit\": \"\u00b0C\"},\n                \"obs\": {\"specific_name\": \"air_temp_obs\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\"specific_name\": \"dewpoint_temperature_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.VAPOR_PRESSURE: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"dewpoint_temperature_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.SNOW_DEPTH: {\n            \"default_source\": \"era5_depth\",\n            \"sources\": {\n                \"era5_depth\": {\"specific_name\": \"snow_depth\", \"unit\": \"m\"},\n            },\n        },\n        StandardVariable.SNOW_COVER: {\n            \"default_source\": \"era5_cover\",\n            \"sources\": {\n                \"era5_cover\": {\"specific_name\": \"snow_cover\", \"unit\": \"fraction\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"potential_evaporation\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"u_component_of_wind_10m\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"v_component_of_wind_10m\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.WIND_SPEED: {\n            \"default_source\": \"obs_speed\",\n            \"sources\": {\n                \"obs_speed\": {\"specific_name\": \"wind_sp_obs\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.WIND_DIR: {\n            \"default_source\": \"obs_dir\",\n            \"sources\": {\n                \"obs_dir\": {\"specific_name\": \"wind_dir_obs\", \"unit\": \"degree\"},\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"surface_pressure\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.THERMAL_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_thermal_radiation\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_solar_radiation\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n    }\n</code></pre>"},{"location":"api/camelsh_kr/#hydrodataset.camelsh_kr.CamelshKr.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/camelsh_kr/#hydrodataset.camelsh_kr.CamelshKr.__init__","title":"<code>__init__(data_path, region=None, download=False, cache_path=None)</code>","text":"<p>Initialize CAMELSH_KR dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELSH_KR data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> <code>cache_path</code> <code>Optional[str]</code> <p>Path to the cache directory</p> <code>None</code> Source code in <code>hydrodataset/camelsh_kr.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    region: Optional[str] = None,\n    download: bool = False,\n    cache_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize CAMELSH_KR dataset.\n\n    Args:\n        data_path: Path to the CAMELSH_KR data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n        cache_path: Path to the cache directory\n    \"\"\"\n    super().__init__(data_path, cache_path=cache_path)\n    self.region = region\n    self.download = download\n    # In aqua_fetch, CAMELS_SK is the alias of CAMELSH_KR\n    self.aqua_fetch = CAMELS_SK(data_path)\n</code></pre>"},{"location":"api/caravan/","title":"Caravan","text":""},{"location":"api/caravan/#overview","title":"Overview","text":"<p>Caravan is a global, standardized dataset of catchment attributes and meteorological forcings for large-sample hydrology. It combines data from multiple regional datasets (CAMELS variants, HYSETS, LamaH) to create a unified, quality-controlled dataset suitable for hydrological modeling and machine learning applications.</p>"},{"location":"api/caravan/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Global (16,299 basins across 7 regions)</li> <li>Project: Caravan</li> <li>Module: <code>hydrodataset.caravan</code></li> <li>Class: <code>Caravan</code></li> <li>Paper: Kratzert et al. (2023)</li> <li>Code: GitHub Repository</li> <li>Data Source: Zenodo</li> </ul>"},{"location":"api/caravan/#supported-regions","title":"Supported Regions","text":"<p>The Caravan dataset includes data from the following regions:</p> Region Code Name Source Dataset Description <code>US</code> United States CAMELS-US Continental US catchments <code>AUS</code> Australia CAMELS-AUS Australian catchments <code>BR</code> Brazil CAMELS-BR Brazilian catchments <code>CL</code> Chile CAMELS-CL Chilean catchments <code>GB</code> Great Britain CAMELS-GB UK catchments <code>CE</code> Central Europe LamaH-CE Central European catchments <code>NA</code> North America HYSETS Canadian catchments (HYSETS) <code>Global</code> All regions Combined All 16,299 basins (default)"},{"location":"api/caravan/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Standardization: Unified variable naming and data format across all regions</li> <li>Quality Control: Rigorous quality checks and data cleaning</li> <li>Multi-source: Combines ERA5-Land reanalysis with regional observations</li> <li>Comprehensive: Includes streamflow, meteorological forcings, and catchment attributes</li> <li>Version: Currently supports Version 0.3 (Version 1.6 available but not yet integrated)</li> </ul>"},{"location":"api/caravan/#features","title":"Features","text":""},{"location":"api/caravan/#static-attributes","title":"Static Attributes","text":"<p>Caravan provides three types of catchment attributes:</p> <ol> <li>Caravan-specific attributes (e.g., gauge metadata, climate indices)</li> <li>HydroATLAS attributes (e.g., topography, land cover, soil properties)</li> <li>Other attributes (e.g., region-specific characteristics)</li> </ol> <p>Key static variables include: - Basin area (<code>area</code>) - Mean precipitation (<code>p_mean</code>) - Gauge location (latitude, longitude) - Climate indices (aridity, moisture index, seasonality) - Topographic characteristics (elevation, slope) - Land cover fractions (forest, crop, urban, etc.) - Soil properties (clay, silt, sand content) - And 200+ HydroATLAS attributes</p>"},{"location":"api/caravan/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables standardized across all regions:</p> Standard Variable Caravan Name Unit Source Streamflow <code>streamflow</code> mm/day Regional observations Precipitation <code>total_precipitation_sum</code> mm/day ERA5-Land Temperature (max) <code>temperature_2m_max</code> \u00b0C ERA5-Land Temperature (min) <code>temperature_2m_min</code> \u00b0C ERA5-Land Solar radiation <code>surface_net_solar_radiation_mean</code> W/m\u00b2 ERA5-Land Snow water equivalent <code>snow_depth_water_equivalent_mean</code> mm/day ERA5-Land Potential ET <code>potential_evaporation_sum</code> mm/day ERA5-Land"},{"location":"api/caravan/#usage","title":"Usage","text":""},{"location":"api/caravan/#basic-usage-global-dataset","title":"Basic Usage - Global Dataset","text":"<pre><code>from hydrodataset.caravan import Caravan\nfrom hydrodataset import SETTING\n\n# Initialize with all regions (Global mode)\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\ncaravan = Caravan(data_path)\n\n# Get all basin IDs\nbasin_ids = caravan.read_object_ids()\nprint(f\"Total basins: {len(basin_ids)}\")  # 16,299 basins\n\n# Check available features\nprint(\"Static features:\", caravan.available_static_features)\nprint(\"Dynamic features:\", caravan.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = caravan.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"2000-01-01\", \"2010-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_max\", \"temperature_min\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = caravan.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    var_lst=[\"area\", \"p_mean\", \"gauge_lat\", \"gauge_lon\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/caravan/#region-specific-usage","title":"Region-Specific Usage","text":"<pre><code># Initialize for a specific region\ncaravan_us = Caravan(data_path, region=\"US\")\ncaravan_aus = Caravan(data_path, region=\"AUS\")\ncaravan_ce = Caravan(data_path, region=\"CE\")\n\n# Get basins for specific region\nus_basins = caravan_us.read_object_ids()\nprint(f\"US basins: {len(us_basins)}\")\n\n# Read region-specific data\nus_data = caravan_us.read_ts_xrdataset(\n    gage_id_lst=us_basins[:5],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\n</code></pre>"},{"location":"api/caravan/#reading-multiple-variables","title":"Reading Multiple Variables","text":"<pre><code># Read comprehensive timeseries\nts_data = caravan.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:20],\n    t_range=[\"2000-01-01\", \"2015-12-31\"],\n    var_lst=[\n        \"streamflow\",\n        \"precipitation\",\n        \"temperature_max\",\n        \"temperature_min\",\n        \"potential_evapotranspiration\",\n        \"solar_radiation\",\n        \"snow_water_equivalent\"\n    ]\n)\n\n# Read basin properties\nbasin_attrs = caravan.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:20],\n    var_lst=[\n        \"area\",\n        \"p_mean\",\n        \"gauge_lat\",\n        \"gauge_lon\",\n        \"aridity\",\n        \"frac_snow\",\n        \"high_prec_freq\",\n        \"seasonality\"\n    ]\n)\n</code></pre>"},{"location":"api/caravan/#convenience-methods","title":"Convenience Methods","text":"<pre><code># Read basin area\nareas = caravan.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = caravan.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/caravan/#data-format-and-caching","title":"Data Format and Caching","text":""},{"location":"api/caravan/#netcdf-cache-structure","title":"NetCDF Cache Structure","text":"<p>Due to the large dataset size (&gt;30GB), Caravan uses a batched caching approach:</p> <p>Attributes Cache: Single file - File: <code>caravan_attributes.nc</code> - Contains: All static attributes for all basins - Size: ~100 MB</p> <p>Timeseries Cache: Multiple batch files per region - Files: <code>caravan_{region}_timeseries_batch_{start_id}_{end_id}.nc</code> - Contains: Timeseries data for batches of ~100 basins - Total size: ~30 GB (all regions)</p>"},{"location":"api/caravan/#first-time-setup","title":"First-Time Setup","text":"<pre><code># If data not downloaded, Caravan will automatically download from Zenodo\n# This may take several hours for the full dataset (~30GB)\ncaravan = Caravan(data_path)  # Downloads if needed\n\n# To manually cache the data:\ncaravan.cache_xrdataset(checkregion=\"all\")  # Cache all regions\n# or\ncaravan.cache_xrdataset(checkregion=\"hysets\")  # Cache specific region\n</code></pre>"},{"location":"api/caravan/#working-with-raw-attributes","title":"Working with Raw Attributes","text":"<pre><code># Read raw attributes (not standardized variable names)\nraw_attrs = caravan.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\n        \"ele_mt_sav\",  # HydroATLAS: mean elevation\n        \"slp_dg_sav\",  # HydroATLAS: mean slope\n        \"for_pc_sse\",  # HydroATLAS: forest cover\n        \"crp_pc_sse\",  # HydroATLAS: crop cover\n    ]\n)\n</code></pre>"},{"location":"api/caravan/#data-quality","title":"Data Quality","text":""},{"location":"api/caravan/#quality-control-features","title":"Quality Control Features","text":"<ul> <li>Streamflow: Quality flags, outlier detection, gap documentation</li> <li>Meteorological forcings: ERA5-Land reanalysis (consistent global coverage)</li> <li>Attributes: Cross-validation with HydroATLAS and regional datasets</li> <li>Standardization: Unified units and coordinate systems</li> </ul>"},{"location":"api/caravan/#known-limitations","title":"Known Limitations","text":"<ol> <li>Version: Currently supports Version 0.3; Version 1.6 available but not yet integrated</li> <li>Damaged Files: Some HYSETS files may be corrupted (automatically checked and repaired from CSV)</li> <li>Download Time: Full dataset download can take 3-6 hours depending on connection</li> </ol>"},{"location":"api/caravan/#integration-with-deep-learning","title":"Integration with Deep Learning","text":"<p>Caravan is designed for machine learning applications:</p> <pre><code>import torch\nfrom torch.utils.data import Dataset\n\nclass CaravanDataset(Dataset):\n    def __init__(self, caravan, basin_ids, t_range):\n        # Read timeseries\n        self.data = caravan.read_ts_xrdataset(\n            gage_id_lst=basin_ids,\n            t_range=t_range,\n            var_lst=[\"streamflow\", \"precipitation\", \"temperature_max\", \"temperature_min\"]\n        )\n        # Read static attributes\n        self.attrs = caravan.read_attr_xrdataset(\n            gage_id_lst=basin_ids,\n            var_lst=[\"area\", \"p_mean\", \"aridity\"]\n        )\n\n    def __len__(self):\n        return len(self.data.basin)\n\n    def __getitem__(self, idx):\n        basin_id = self.data.basin.values[idx]\n        timeseries = self.data.sel(basin=basin_id).to_array().values\n        attributes = self.attrs.sel(basin=basin_id).to_array().values\n        return torch.tensor(timeseries), torch.tensor(attributes)\n</code></pre>"},{"location":"api/caravan/#performance-tips","title":"Performance Tips","text":""},{"location":"api/caravan/#memory-management","title":"Memory Management","text":"<pre><code># For large queries, use parallel reading\nts_data = caravan.read_ts_xrdataset(\n    gage_id_lst=basin_ids,\n    t_range=[\"2000-01-01\", \"2010-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\"],\n    parallel=True  # Enable parallel reading with dask\n)\n\n# Load data lazily (returns dask arrays)\nts_data_lazy = caravan.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:100],\n    t_range=[\"1990-01-01\", \"2020-12-31\"],\n    var_lst=[\"streamflow\"]\n)\n# Compute when needed\nts_data_computed = ts_data_lazy.compute()\n</code></pre>"},{"location":"api/caravan/#region-specific-processing","title":"Region-Specific Processing","text":"<pre><code># Process one region at a time to reduce memory usage\nregions = [\"US\", \"AUS\", \"BR\", \"CL\", \"GB\", \"CE\", \"NA\"]\nfor region in regions:\n    caravan_region = Caravan(data_path, region=region)\n    basin_ids = caravan_region.read_object_ids()\n\n    # Process this region's data\n    data = caravan_region.read_ts_xrdataset(\n        gage_id_lst=basin_ids,\n        t_range=[\"2000-01-01\", \"2010-12-31\"],\n        var_lst=[\"streamflow\", \"precipitation\"]\n    )\n\n    # Your processing code here\n    # ...\n</code></pre>"},{"location":"api/caravan/#references","title":"References","text":"<p>If you use Caravan in your research, please cite:</p> <pre><code>@article{kratzert2023caravan,\n  title={Caravan--a global community dataset for large-sample hydrology},\n  author={Kratzert, Frederik and Nearing, Grey and Addor, Nans and Erickson, Tyler and Gauch, Martin and Gilon, Oren and Gudmundsson, Lukas and Hassidim, Avinatan and Klotz, Daniel and Nevo, Sella and others},\n  journal={Scientific Data},\n  volume={10},\n  number={1},\n  pages={61},\n  year={2023},\n  publisher={Nature Publishing Group UK London}\n}\n</code></pre>"},{"location":"api/caravan/#additional-resources","title":"Additional Resources","text":"<ul> <li>Caravan Website: https://github.com/kratzert/Caravan</li> <li>Paper: Nature Scientific Data</li> <li>HydroATLAS: Technical Documentation</li> <li>ERA5-Land: ECMWF Documentation</li> </ul>"},{"location":"api/caravan/#api-reference","title":"API Reference","text":""},{"location":"api/caravan/#hydrodataset.caravan.Caravan","title":"<code>hydrodataset.caravan.Caravan</code>","text":"<p>               Bases: <code>HydroDataset</code></p> Source code in <code>hydrodataset/caravan.py</code> <pre><code>class Caravan(HydroDataset):\n    def __init__(self, data_path: str, region: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Initialization for Caravan dataset\n\n        TODO: The newest version is Version 1.6, but now we only support Version 0.3\n\n        Parameters\n        ----------\n        data_path\n            where we put the dataset\n        region\n            the region can be US, AUS, BR, CL, GB, CE, NA (North America, meaning HYSETS)\n        \"\"\"\n        self.data_path = os.path.join(data_path, \"CARAVAN\")\n        super().__init__(self.data_path)\n        self.region = \"Global\" if region is None else region\n        region_name_dict = self.region_name_dict\n        if self.region == \"Global\":\n            self.region_data_name = list(region_name_dict.values())\n        else:\n            self.region_data_name = region_name_dict[self.region]\n        self.data_source_description = self.set_data_source_describe()\n        try:\n            self.is_data_ready()\n        except FileNotFoundError as e:\n            warnings.warn(e)\n            print(\"Downloading dataset...\")\n            self.download_data_source()\n            print(\"Dataset downloaded successfully.\")\n            print(\"Checking if dataset is ready...\")\n            self.is_data_ready()\n            print(\"Dataset is ready.\")\n        self.sites = self.read_site_info()\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"caravan_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        \"\"\"\n        Get glob pattern to match batch cache files.\n\n        Returns\n        -------\n        str\n            Glob pattern to match all batch cache files for the region(s)\n        \"\"\"\n        region_name = self.region_data_name\n        if isinstance(region_name, list):\n            # Match all regions: caravan_*_timeseries_batch_*.nc\n            return str(self.cache_dir.joinpath(\"caravan_*_timeseries_batch_*.nc\"))\n        else:\n            # Match specific region: caravan_{region}_timeseries_batch_*.nc\n            return str(\n                self.cache_dir.joinpath(f\"caravan_{region_name}_timeseries_batch_*.nc\")\n            )\n\n    @property\n    def default_t_range(self):\n        return [\"1981-01-01\", \"2020-12-31\"]\n\n    @property\n    def region_name_dict(self):\n        return {\n            \"US\": \"camels\",\n            \"AUS\": \"camelsaus\",\n            \"BR\": \"camelsbr\",\n            \"CL\": \"camelscl\",\n            \"GB\": \"camelsgb\",\n            \"NA\": \"hysets\",\n            \"CE\": \"lamah\",\n        }\n\n    _subclass_static_definitions = {\n        \"area\": {\"specific_name\": \"area\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n    }\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"each_region\",\n            \"sources\": {\n                \"each_region\": {\"specific_name\": \"streamflow\", \"unit\": \"mm/day\"}\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"each_region\",\n            \"sources\": {\n                \"each_region\": {\n                    \"specific_name\": \"total_precipitation_sum\",\n                    \"unit\": \"mm/day\",\n                },\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"each_region\",\n            \"sources\": {\n                \"each_region\": {\"specific_name\": \"temperature_2m_max\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"each_region\",\n            \"sources\": {\n                \"each_region\": {\"specific_name\": \"temperature_2m_min\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"each_region\",\n            \"sources\": {\n                \"each_region\": {\n                    \"specific_name\": \"surface_net_solar_radiation_mean\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"each_region\",\n            \"sources\": {\n                \"each_region\": {\n                    \"specific_name\": \"snow_depth_water_equivalent_mean\",\n                    \"unit\": \"mm/day\",\n                },\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"each_region\",\n            \"sources\": {\n                \"each_region\": {\n                    \"specific_name\": \"potential_evaporation_sum\",\n                    \"unit\": \"mm/day\",\n                }\n            },\n        },\n    }\n\n    def get_name(self):\n        return \"Caravan_\" + self.region\n\n    def set_data_source_describe(self) -&gt; collections.OrderedDict:\n        \"\"\"\n        Introduce the files in the dataset and list their location in the file system\n\n        Returns\n        -------\n        collections.OrderedDict\n            the description for Caravan\n        \"\"\"\n        dataset_dir = self._base_dir()\n\n        # We use A_basins_total_upstrm\n        # shp file of basins\n        if self.region == \"Global\":\n            shp_dir = os.path.join(dataset_dir, \"shapefiles\")\n        else:\n            shp_dir = os.path.join(dataset_dir, \"shapefiles\", self.region_data_name)\n        # read the shp in this dir\n        shp_files = [f for f in os.listdir(shp_dir) if f.endswith(\".shp\")]\n\n        if len(shp_files) != 1:\n            raise ValueError(\n                f\"Expected one shapefile in {shp_dir}, found {len(shp_files)}\"\n            )\n        shp_file_path = os.path.join(shp_dir, shp_files[0])\n        # config of flow data\n        flow_dir = os.path.join(dataset_dir, \"timeseries\", \"netcdf\")\n        forcing_dir = flow_dir\n        attr_dir = os.path.join(dataset_dir, \"attributes\")\n        ts_csv_dir = os.path.join(dataset_dir, \"timeseries\", \"csv\")\n        # TODO: The newest version is Version 1.6, but the url is not updated yet. it's still Version 0.3\n        download_url = \"https://zenodo.org/record/7944025/files/Caravan.zip\"\n        return collections.OrderedDict(\n            DATASET_DIR=dataset_dir,\n            FLOW_DIR=flow_dir,\n            FORCING_DIR=forcing_dir,\n            TS_CSV_DIR=ts_csv_dir,\n            ATTR_DIR=attr_dir,\n            BASINS_SHP_FILE=shp_file_path,\n            DOWNLOAD_URL=download_url,\n        )\n\n    def _base_dir(self):\n        return os.path.join(self.data_source_dir, \"Caravan\", \"Caravan\")\n\n    def download_data_source(self) -&gt; None:\n        \"\"\"\n        Download dataset.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        dataset_config = self.data_source_description\n        self.data_source_dir.mkdir(exist_ok=True)\n        url = dataset_config[\"DOWNLOAD_URL\"]\n        fzip = Path(self.data_source_dir, url.rsplit(\"/\", 1)[1])\n        if fzip.exists():\n            with urlopen(url) as response:\n                if int(response.info()[\"Content-length\"]) != fzip.stat().st_size:\n                    fzip.unlink()\n        to_dl = []\n        if not Path(self.data_source_dir, url.rsplit(\"/\", 1)[1]).exists():\n            to_dl.append(url)\n        hydro_file.download_zip_files(to_dl, self.data_source_dir)\n        # It seems that there is sth. wrong with hysets_06444000.nc\n        try:\n            hydro_file.zip_extract(dataset_config[\"DATASET_DIR\"])\n        except tarfile.ReadError:\n            Warning(\"Please manually unzip the file.\")\n\n    def is_data_ready(self):\n        \"\"\"Check if the data is ready to be read\"\"\"\n        if not os.path.exists(self.data_source_description[\"DATASET_DIR\"]):\n            raise FileNotFoundError(\n                f\"Dataset is not found in {self.data_source_description['DATASET_DIR']}\"\n            )\n        if not os.path.exists(self.data_source_description[\"FLOW_DIR\"]):\n            raise FileNotFoundError(\n                f\"Flow data is not found in {self.data_source_description['FLOW_DIR']}\"\n            )\n        if not os.path.exists(self.data_source_description[\"FORCING_DIR\"]):\n            raise FileNotFoundError(\n                f\"Forcing data is not found in {self.data_source_description['FORCING_DIR']}\"\n            )\n        if not os.path.exists(self.data_source_description[\"ATTR_DIR\"]):\n            raise FileNotFoundError(\n                f\"Attributes data is not found in {self.data_source_description['ATTR_DIR']}\"\n            )\n\n    def read_site_info(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Read the basic information of gages in dataset\n\n        Returns\n        -------\n        pd.DataFrame\n            basic info of gages\n        \"\"\"\n        if self.region == \"Global\":\n            attr = []\n            for region in self.region_data_name:\n                site_file = os.path.join(\n                    self.data_source_description[\"ATTR_DIR\"],\n                    region,\n                    \"attributes_caravan_\" + region + \".csv\",\n                )\n                attr_region = pd.read_csv(site_file, sep=\",\")\n                attr.append(attr_region)\n            return pd.concat(attr)\n        site_file = os.path.join(\n            self.data_source_description[\"ATTR_DIR\"],\n            self.region_data_name,\n            \"attributes_caravan_\" + self.region_data_name + \".csv\",\n        )\n        return pd.read_csv(site_file, sep=\",\")\n\n    def get_constant_cols(self) -&gt; np.array:\n        \"\"\"\n        all readable attrs\n\n        Returns\n        -------\n        np.array\n            attribute types\n        \"\"\"\n        if self.region == \"Global\":\n            attr_types = []\n            for region in self.region_data_name:\n                attr_indices = self._attr_columns_region(region)\n                attr_types.append(attr_indices)\n            return np.unique(np.concatenate(attr_types))\n        return self._attr_columns_region(self.region_data_name)\n\n    def _attr_columns_region(self, region):\n        attr_file1 = os.path.join(\n            self.data_source_description[\"DATASET_DIR\"],\n            \"attributes\",\n            region,\n            \"attributes_caravan_\" + region + \".csv\",\n        )\n        attr_indices_data1 = pd.read_csv(attr_file1, sep=\",\")\n        attr_file2 = os.path.join(\n            self.data_source_description[\"DATASET_DIR\"],\n            \"attributes\",\n            region,\n            \"attributes_hydroatlas_\" + region + \".csv\",\n        )\n        attr_indices_data2 = pd.read_csv(attr_file2, sep=\",\")\n        attr_file3 = os.path.join(\n            self.data_source_description[\"DATASET_DIR\"],\n            \"attributes\",\n            region,\n            \"attributes_other_\" + region + \".csv\",\n        )\n        attr_indices_data3 = pd.read_csv(attr_file3, sep=\",\")\n        return np.array(\n            attr_indices_data1.columns.values[1:].tolist()\n            + attr_indices_data2.columns.values[1:].tolist()\n            + attr_indices_data3.columns.values[1:].tolist()\n        )\n\n    def get_relevant_cols(self) -&gt; np.array:\n        \"\"\"\n        all readable forcing types, also including streamflow\n\n        Returns\n        -------\n        np.array\n            forcing types\n        \"\"\"\n        if self.region == \"Global\":\n            forcing_types = []\n            for region in self.region_data_name:\n                forcing_dir = os.path.join(\n                    self.data_source_description[\"FORCING_DIR\"],\n                    region,\n                )\n                if not (files := os.listdir(forcing_dir)):\n                    raise FileNotFoundError(\"No files found in the directory.\")\n                first_file = files[0]\n                file_path = os.path.join(forcing_dir, first_file)\n                data = xr.open_dataset(file_path)\n                forcing_types.append(list(data.data_vars))\n            return np.unique(np.concatenate(forcing_types))\n        forcing_dir = os.path.join(\n            self.data_source_description[\"FORCING_DIR\"],\n            self.region_data_name,\n        )\n        if not (files := os.listdir(forcing_dir)):\n            raise FileNotFoundError(\"No files found in the directory.\")\n        first_file = files[0]\n        file_path = os.path.join(forcing_dir, first_file)\n\n        if files := os.listdir(forcing_dir):\n            first_file = files[0]\n            file_path = os.path.join(forcing_dir, first_file)\n            data = xr.open_dataset(file_path)\n        else:\n            raise FileNotFoundError(\"No files found in the directory.\")\n        return np.array(list(data.data_vars))\n\n    def get_target_cols(self) -&gt; np.array:\n        \"\"\"\n        the target vars are streamflows\n\n        Returns\n        -------\n        np.array\n            streamflow types\n        \"\"\"\n        return np.array([\"streamflow\"])\n\n    def read_object_ids(self, **kwargs: Any) -&gt; np.array:\n        \"\"\"\n        read station ids\n\n        Parameters\n        ----------\n        **kwargs\n            optional params if needed\n\n        Returns\n        -------\n        np.array\n            gage/station ids\n        \"\"\"\n        return self.sites[\"gauge_id\"].values\n\n    def read_target_cols(\n        self,\n        gage_id_lst: Union[list, np.array] = None,\n        t_range: list = None,\n        target_cols: Union[list, np.array] = None,\n        **kwargs: Any,\n    ) -&gt; np.array:\n        if self.region == \"Global\":\n            return self._read_timeseries_data_global(\n                \"FLOW_DIR\", gage_id_lst, t_range, target_cols\n            )\n        return self._read_timeseries_data(\"FLOW_DIR\", gage_id_lst, t_range, target_cols)\n\n    def _read_timeseries_data_global(self, dir_name, gage_id_lst, t_range, var_lst):\n        ts_dir = self.data_source_description[dir_name]\n        if gage_id_lst is None:\n            gage_id_lst = self.read_object_ids()\n        # Find matching file paths\n        file_paths = []\n        for region in self.region_data_name:\n            for file_name in gage_id_lst:\n                file_path = os.path.join(ts_dir, region, file_name) + \".nc\"\n                if os.path.isfile(file_path):\n                    file_paths.append(file_path)\n        datasets = [\n            xr.open_dataset(path).assign_coords(gauge_id=name)\n            for path, name in zip(file_paths, gage_id_lst)\n        ]\n        # Concatenate the datasets along the new dimension\n        data = xr.concat(datasets, dim=\"gauge_id\").sortby(\"gauge_id\")\n        if t_range is not None:\n            data = data.sel(date=slice(t_range[0], t_range[1]))\n        if var_lst is None:\n            if dir_name == \"FLOW_DIR\":\n                var_lst = self.get_target_cols()\n            else:\n                var_lst = self.get_relevant_cols()\n        return data[var_lst]\n\n    def read_relevant_cols(\n        self,\n        gage_id_lst: Optional[list] = None,\n        t_range: Optional[list] = None,\n        var_lst: Optional[list] = None,\n        forcing_type: str = \"era5land\",\n        **kwargs: Any,\n    ) -&gt; np.ndarray:\n        \"\"\"_summary_\n\n        Parameters\n        ----------\n        gage_id_lst : list, optional\n            _description_, by default None\n        t_range : list, optional\n            A special notice is that for xarray, the time range is [start, end] which is a closed interval.\n        var_lst : list, optional\n            _description_, by default None\n        forcing_type : str, optional\n            _description_, by default \"daymet\"\n\n        Returns\n        -------\n        np.ndarray\n            _description_\n        \"\"\"\n        if self.region == \"Global\":\n            return self._read_timeseries_data_global(\n                \"FORCING_DIR\", gage_id_lst, t_range, var_lst\n            )\n        return self._read_timeseries_data(\"FORCING_DIR\", gage_id_lst, t_range, var_lst)\n\n    def _read_timeseries_data(self, dir_name, gage_id_lst, t_range, var_lst):\n        ts_dir = self.data_source_description[dir_name]\n        if gage_id_lst is None:\n            gage_id_lst = self.read_object_ids()\n        # Find matching file paths\n        file_paths = []\n        for file_name in gage_id_lst:\n            file_path = os.path.join(ts_dir, self.region_data_name, file_name) + \".nc\"\n            if os.path.isfile(file_path):\n                file_paths.append(file_path)\n        datasets = [\n            xr.open_dataset(path).assign_coords(gauge_id=name)\n            for path, name in zip(file_paths, gage_id_lst)\n        ]\n        # Concatenate the datasets along the new dimension\n        data = xr.concat(datasets, dim=\"gauge_id\").sortby(\"gauge_id\")\n        if t_range is not None:\n            data = data.sel(date=slice(t_range[0], t_range[1]))\n        if var_lst is None:\n            if dir_name == \"FLOW_DIR\":\n                var_lst = self.get_target_cols()\n            else:\n                var_lst = self.get_relevant_cols()\n        return data[var_lst]\n\n    def read_constant_cols(\n        self,\n        gage_id_lst: Optional[list] = None,\n        var_lst: Optional[list] = None,\n        is_return_dict: bool = False,\n        **kwargs: Any,\n    ) -&gt; Union[dict, np.ndarray]:\n        \"\"\"\n        Read Attributes data\n\n        Parameters\n        ----------\n        gage_id_lst\n            station ids\n        var_lst\n            attribute variable types\n        is_return_dict\n            if true, return var_dict and f_dict for CAMELS_US\n        Returns\n        -------\n        Union[tuple, np.array]\n            if attr var type is str, return factorized data.\n            When we need to know what a factorized value represents,\n            we need return a tuple;\n            otherwise just return an array\n        \"\"\"\n        if self.region == \"Global\":\n            return self._read_constant_cols_global(var_lst, gage_id_lst, is_return_dict)\n        data = self._read_attr_files_1region(\n            self.region_data_name, gage_id_lst, var_lst\n        )\n        return data.to_dict(\"index\") if is_return_dict else data.values\n\n    def _read_attr_files_1region(self, region, gage_id_lst, var_lst):\n        \"\"\"When gage_id_lst is None, we read all gages in this region;\n        when var_lst is None, we read all attributes in this region\n\n        Parameters\n        ----------\n        region : str\n            region name\n        gage_id_lst : list\n            gage ids\n        var_lst : list\n            attribute variable types\n\n        Returns\n        -------\n        pd.DataFrame\n            attributes data\n        \"\"\"\n        attr_file1 = os.path.join(\n            self.data_source_description[\"DATASET_DIR\"],\n            \"attributes\",\n            region,\n            \"attributes_caravan_\" + region + \".csv\",\n        )\n        attr_file2 = os.path.join(\n            self.data_source_description[\"DATASET_DIR\"],\n            \"attributes\",\n            region,\n            \"attributes_hydroatlas_\" + region + \".csv\",\n        )\n        attr_file3 = os.path.join(\n            self.data_source_description[\"DATASET_DIR\"],\n            \"attributes\",\n            region,\n            \"attributes_other_\" + region + \".csv\",\n        )\n        data1 = pd.read_csv(attr_file1, sep=\",\", dtype={\"gauge_id\": str})\n        data1 = data1.set_index(\"gauge_id\")\n        data2 = pd.read_csv(attr_file2, sep=\",\", dtype={\"gauge_id\": str})\n        data2 = data2.set_index(\"gauge_id\")\n        data3 = pd.read_csv(attr_file3, sep=\",\", dtype={\"gauge_id\": str})\n        data3 = data3.set_index(\"gauge_id\")\n        data = pd.concat([data1, data2, data3], axis=1)\n        if gage_id_lst is not None:\n            data = data.loc[gage_id_lst]\n        if var_lst is not None:\n            data = data.loc[:, var_lst]\n        return data\n\n    def _read_constant_cols_global(self, var_lst, gage_id_lst, is_return_dict):\n        attr = []\n        if var_lst is None:\n            var_lst = self.get_constant_cols()\n        if gage_id_lst is None:\n            gage_id_lst = self.read_object_ids()\n        for region in self.region_data_name:\n            # as the gage_id may come from different regions, to avoid error, we set gage_id_lst=None\n            attr_indices_data = self._read_attr_files_1region(\n                region, gage_id_lst=None, var_lst=var_lst\n            )\n            gage_in_this_region = np.intersect1d(\n                attr_indices_data.index.values, gage_id_lst\n            )\n            if gage_in_this_region.size &gt; 0:\n                attr_indices_data = attr_indices_data.loc[gage_in_this_region]\n                attr_indices_data = attr_indices_data.loc[:, var_lst]\n                attr.append(attr_indices_data)\n        data = pd.concat(attr)\n        return data.to_dict(\"index\") if is_return_dict else data.values\n\n    def read_mean_prep(self, object_ids: Optional[list] = None) -&gt; np.array:\n        return self.read_constant_cols(object_ids, [\"p_mean\"], is_return_dict=False)\n\n    def cache_attributes_xrdataset(self) -&gt; None:\n        \"\"\"cache attributes in xr dataset\"\"\"\n        import pint_xarray  # noqa\n        import pint\n        from pint import UnitRegistry\n\n        data = self.read_constant_cols()\n        basin_ids = self.read_object_ids()\n        var_names = self.get_constant_cols()\n        assert all(x &lt;= y for x, y in zip(basin_ids, basin_ids[1:]))\n        if self.region == \"Global\":\n            # for all attrs reading in Global mode, all attrs are sorted\n            assert all(x &lt;= y for x, y in zip(var_names, var_names[1:]))\n        ds = xr.Dataset(\n            {var: ([\"basin\"], data[:, i]) for i, var in enumerate(var_names)},\n            coords={\"basin\": basin_ids},\n        )\n        units_dict = {attribute: \"\" for attribute in var_names}\n        # Define a dictionary for the provided attribute patterns and their units\n        # https://data.hydrosheds.org/file/technical-documentation/BasinATLAS_Catalog_v10.pdf\n        units_mapping = {\n            \"dis_m3_\": \"cubic meters/second\",\n            \"run_mm_\": \"millimeters\",\n            \"inu_pc_\": \"percent cover\",\n            \"lka_pc_\": \"percent cover (x10)\",\n            \"lkv_mc_\": \"million cubic meters\",\n            \"rev_mc_\": \"million cubic meters\",\n            \"dor_pc_\": \"percent (x10)\",\n            \"ria_ha_\": \"hectares\",\n            \"riv_tc_\": \"thousand cubic meters\",\n            \"gwt_cm_\": \"centimeters\",\n            \"ele_mt_\": \"meters a.s.l.\",\n            \"slp_dg_\": \"degrees (x10)\",\n            \"sgr_dk_\": \"decimeters per km\",\n            \"clz_cl_\": \"classes (18)\",\n            \"cls_cl_\": \"classes (125)\",\n            \"tmp_dc_\": \"degrees Celsius (x10)\",\n            \"pre_mm_\": \"millimeters\",\n            \"pet_mm_\": \"millimeters\",\n            \"aet_mm_\": \"millimeters\",\n            \"ari_ix_\": \"index value (x100)\",\n            \"cmi_ix_\": \"index value (x100)\",\n            \"snw_pc_\": \"percent cover\",\n            \"glc_cl_\": \"classes (22)\",\n            \"glc_pc_\": \"percent cover\",\n            \"pnv_cl_\": \"classes (15)\",\n            \"pnv_pc_\": \"percent cover\",\n            \"wet_cl_\": \"classes (12)\",\n            \"wet_pc_\": \"percent cover\",\n            \"for_pc_\": \"percent cover\",\n            \"crp_pc_\": \"percent cover\",\n            \"pst_pc_\": \"percent cover\",\n            \"ire_pc_\": \"percent cover\",\n            \"gla_pc_\": \"percent cover\",\n            \"prm_pc_\": \"percent cover\",\n            \"pac_pc_\": \"percent cover\",\n            \"tbi_cl_\": \"classes (14)\",\n            \"tec_cl_\": \"classes (846)\",\n            \"fmh_cl_\": \"classes (13)\",\n            \"fec_cl_\": \"classes (426)\",\n            \"cly_pc_\": \"percent\",\n            \"slt_pc_\": \"percent\",\n            \"snd_pc_\": \"percent\",\n            \"soc_th_\": \"tonnes/hectare\",\n            \"swc_pc_\": \"percent\",\n            \"lit_cl_\": \"classes (16)\",\n            \"kar_pc_\": \"percent cover\",\n            \"ero_kh_\": \"kg/hectare per year\",\n            \"pop_ct_\": \"count (thousands)\",\n            \"ppd_pk_\": \"people per km\u00b2\",\n            \"urb_pc_\": \"percent cover\",\n            \"nli_ix_\": \"index value (x100)\",\n            \"rdd_mk_\": \"meters per km\u00b2\",\n            \"hft_ix_\": \"index value (x10)\",\n            \"gad_id_\": \"ID number\",\n            \"gdp_ud_\": \"US dollars\",\n            \"hdi_ix_\": \"index value (x1000)\",\n        }\n\n        # Update the attributes_dict based on the units_mapping\n        for key_pattern, unit in units_mapping.items():\n            for key in units_dict:\n                if key.startswith(key_pattern):\n                    units_dict[key] = unit\n\n        # for attrs not from hydroatlas in caravan, we directly set pint unit\n        units_dict[\"area\"] = \"km^2\"\n        units_dict[\"area_fraction_used_for_aggregation\"] = (\n            \"dimensionless\"  # this one is from atlas but not specified in the document\n        )\n        units_dict[\"aridity\"] = \"dimensionless\"\n        units_dict[\"country\"] = \"dimensionless\"\n        units_dict[\"frac_snow\"] = \"dimensionless\"\n        units_dict[\"gauge_lat\"] = \"degree\"\n        units_dict[\"gauge_lon\"] = \"degree\"\n        units_dict[\"gauge_name\"] = \"dimensionless\"\n        units_dict[\"high_prec_dur\"] = \"day\"\n        units_dict[\"high_prec_freq\"] = \"day/year\"\n        units_dict[\"low_prec_dur\"] = \"day\"\n        units_dict[\"low_prec_freq\"] = \"day/year\"\n        units_dict[\"moisture_index\"] = \"dimensionless\"\n        units_dict[\"p_mean\"] = \"mm/year\"\n        units_dict[\"pet_mean\"] = \"mm/year\"\n        units_dict[\"seasonality\"] = \"dimensionless\"\n\n        # Reinitialize unit registry and unit mapping dictionary\n        ureg = UnitRegistry()\n\n        pint_unit_mapping = {\n            \"cubic meters/second\": \"m^3/s\",\n            \"millimeters\": \"millimeter\",\n            \"percent cover\": \"percent\",\n            \"percent cover (x10)\": \"1e-1 * percent\",\n            \"million cubic meters\": \"1e6 * m^3\",\n            \"thousand cubic meters\": \"1e3 * m^3\",\n            \"centimeters\": \"centimeter\",\n            \"meters a.s.l.\": \"meter\",\n            \"degrees (x10)\": \"1e-1 * degree\",\n            \"decimeters per km\": \"decimeter/km\",\n            \"classes (18)\": \"dimensionless\",\n            \"classes (125)\": \"dimensionless\",\n            \"degrees Celsius (x10)\": \"degree_Celsius\",\n            \"index value (x100)\": \"1e-2\",\n            \"classes (22)\": \"dimensionless\",\n            \"classes (15)\": \"dimensionless\",\n            \"classes (12)\": \"dimensionless\",\n            \"classes (14)\": \"dimensionless\",\n            \"classes (846)\": \"dimensionless\",\n            \"classes (13)\": \"dimensionless\",\n            \"classes (426)\": \"dimensionless\",\n            \"percent\": \"percent\",\n            \"tonnes/hectare\": \"tonne/hectare\",\n            \"kg/hectare per year\": \"kg/hectare/year\",\n            \"count (thousands)\": \"1e3\",\n            \"people per km\u00b2\": \"1/km^2\",\n            \"index value (x1000)\": \"1e-3\",\n            \"meters per km\u00b2\": \"meter/km^2\",\n            \"index value (x10)\": \"1e-1\",\n            \"US dollars\": \"dimensionless\",\n            \"ID number\": \"dimensionless\",\n        }\n\n        # Validate each unit in the unit_mapping dictionary\n        valid_units = {}\n        invalid_units = {}\n\n        for provided_unit, pint_unit in pint_unit_mapping.items():\n            try:\n                ureg.parse_expression(pint_unit)\n                valid_units[provided_unit] = pint_unit\n            except pint.errors.UndefinedUnitError:\n                invalid_units[provided_unit] = pint_unit\n\n        converted_units = {\n            var: pint_unit_mapping.get(unit, unit) for var, unit in units_dict.items()\n        }\n        assert list(units_dict.keys()) == list(\n            converted_units.keys()\n        ), \"The keys of the dictionaries don't match or are not in the same order!\"\n\n        # for tmp_dc_ variable, we can't convert its unit to 0.1 * degree_Celsius\n        # hence we turn its value to degree_Celsius\n        for var in ds.data_vars:\n            if var.startswith(\"tmp_dc_\"):\n                ds[var] = ds[var] * 0.1\n\n        # Assign units to the variables in the Dataset\n        for var_name in converted_units:\n            if var_name in ds.data_vars:\n                ds[var_name].attrs[\"units\"] = converted_units[var_name]\n        cache_attr_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n        ds.to_netcdf(cache_attr_file)\n\n    def cache_xrdataset(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Save all attr data in a netcdf file in the cache directory,\n        ts data are already nc format\n\n        Parameters\n        ----------\n        checkregion : str, optional\n            as my experience, the dameged file is in hysets, by default \"hysets\"\n        \"\"\"\n        checkregion = kwargs.get(\"checkregion\", \"hysets\")\n        warnings.warn(\"Check you units of all variables\")\n        self.cache_attributes_xrdataset()\n        self.cache_timeseries_xrdataset(checkregion)\n\n    def read_timeseries(\n        self,\n        region: str,\n        basin_ids: Optional[list] = None,\n        t_range_list: Optional[list] = None,\n        var_lst: Optional[list] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Read time-series data from csv files\n\n        Parameters\n        ----------\n        region\n            the region can be US, AUS, BR, CL, GB, CE, NA (North America, meaning HYSETS)\n        basin_ids\n            station ids\n        t_range\n            time range\n        var_lst\n            relevant columns\n\n        Returns\n        -------\n        np.array\n            time-series data\n        \"\"\"\n        if basin_ids is None:\n            basin_ids = self.read_object_ids()\n        if var_lst is None:\n            var_lst = self.get_relevant_cols()\n        if t_range_list is None:\n            t_range_list = self.default_t_range\n        ts_dir = self.data_source_description[\"TS_CSV_DIR\"]\n        t_range = pd.date_range(start=t_range_list[0], end=t_range_list[-1], freq=\"1D\")\n        nt = len(t_range)\n        x = np.full([len(basin_ids), nt, len(var_lst)], np.nan)\n\n        for k in tqdm(range(len(basin_ids)), desc=\"Reading timeseries data\"):\n            ts_file = os.path.join(\n                ts_dir,\n                region,\n                basin_ids[k] + \".csv\",\n            )\n            ts_data = pd.read_csv(ts_file, engine=\"c\")\n            date = pd.to_datetime(ts_data[\"date\"]).values\n            [_, ind1, ind2] = np.intersect1d(date, t_range, return_indices=True)\n            for j in range(len(var_lst)):\n                x[k, ind2, j] = ts_data[var_lst[j]][ind1].values\n        return x\n\n    def _get_unit_json(self, onebasinid: str, region_name: str) -&gt; dict:\n        ancfile4unit = os.path.join(\n            self.data_source_description[\"FLOW_DIR\"],\n            region_name,\n            f\"{onebasinid}.nc\",\n        )\n        anc4unit = xr.open_dataset(ancfile4unit)\n        data_string = anc4unit.Units\n\n        # Convert string to dictionary\n        result_dict = {}\n        lines = data_string.strip().split(\"\\n\")\n        for line in lines:\n            key, value = line.split(\": \")\n            unit = value.split(\"[\")[-1].strip(\"]\")\n\n            # Convert to mm/day if the unit is mm\n            if unit == \"mm\":\n                unit = \"mm/day\"\n\n            result_dict[key] = unit\n        dataset_variable_names = list(anc4unit.data_vars.keys())\n        default_unit = \"dimensionless\"\n        # Update result_dict for matching variables\n        for var in dataset_variable_names:\n            matched = False\n            for key, unit in result_dict.items():\n                if key in var:  # Check if the key is a substring of the variable name\n                    result_dict[var] = unit\n                    matched = True\n                    break\n            if not matched:  # If no match is found, assign the default unit\n                result_dict[var] = default_unit\n        # streamflow's unit is same as prcp, we directly set it to mm/day\n        result_dict[\"streamflow\"] = \"mm/day\"\n        return result_dict\n\n    def cache_timeseries_xrdataset(\n        self, checkregion: Optional[str] = None, **kwargs: Any\n    ) -&gt; None:\n        if checkregion is not None:\n            regions = self.region_data_name if checkregion == \"all\" else [checkregion]\n            self._check_data(regions)\n\n        if isinstance(self.region_data_name, str):\n            region_data_name = [self.region_data_name]\n        else:\n            region_data_name = self.region_data_name\n        for region in region_data_name:\n            # all files are too large to read in memory, hence we read them region by region\n            site_file = os.path.join(\n                self.data_source_description[\"ATTR_DIR\"],\n                region,\n                \"attributes_caravan_\" + region + \".csv\",\n            )\n            sites_region = pd.read_csv(site_file, sep=\",\")\n            gage_id_lst = sites_region[\"gauge_id\"].values\n            batchsize = kwargs.get(\"batchsize\", 100)\n            t_range = kwargs.get(\"t_range\", None)\n            if t_range is None:\n                t_range = self.default_t_range\n            times = (\n                pd.date_range(start=t_range[0], end=t_range[-1], freq=\"1D\")\n                .strftime(\"%Y-%m-%d %H:%M:%S\")\n                .tolist()\n            )\n            variables = self.get_relevant_cols()\n            units_info = self._get_unit_json(gage_id_lst[0], region)\n\n            def data_generator(basins, batch_size):\n                for i in range(0, len(basins), batch_size):\n                    yield basins[i : i + batch_size]\n\n            for basin_batch in data_generator(gage_id_lst, batchsize):\n                # we make sure the basin ids are sorted\n                assert all(x &lt;= y for x, y in zip(basin_batch, basin_batch[1:]))\n                data = self.read_timeseries(\n                    region=region,\n                    basin_ids=basin_batch,\n                    t_range_list=t_range,\n                    var_lst=variables,\n                )\n                dataset = xr.Dataset(\n                    data_vars={\n                        variables[i]: (\n                            [\"basin\", \"time\"],\n                            data[:, :, i],\n                            {\"units\": units_info[variables[i]]},\n                        )\n                        for i in range(len(variables))\n                    },\n                    coords={\n                        \"basin\": basin_batch,\n                        \"time\": pd.to_datetime(times),\n                    },\n                )\n\n                # Save the dataset to a NetCDF file for the current batch and time unit\n                prefix_ = \"\" if region is None else region + \"_\"\n                batch_file_path = self.cache_dir.joinpath(\n                    f\"caravan_{prefix_}timeseries_batch_{basin_batch[0]}_{basin_batch[-1]}.nc\",\n                )\n                dataset.to_netcdf(batch_file_path)\n\n                # Release memory by deleting the dataset\n                del dataset\n                del data\n\n    def _check_data(self, regions):\n        pbar = tqdm(regions, desc=\"Start Checking Data...\")\n        for region in pbar:\n            pbar.set_description(f\"Processing Region-{region}\")\n            # all files are too large to read in memory, hence we read them region by region\n            site_file = os.path.join(\n                self.data_source_description[\"ATTR_DIR\"],\n                region,\n                \"attributes_caravan_\" + region + \".csv\",\n            )\n            sites_region = pd.read_csv(site_file, sep=\",\")\n            gage_id_lst = sites_region[\"gauge_id\"].values\n            # forcing dir is same as flow dir\n            ts_dir = self.data_source_description[\"FORCING_DIR\"]\n            # Find matching file paths\n            for gage_id in tqdm(gage_id_lst, desc=\"Check data by Gage\"):\n                file_path = os.path.join(ts_dir, region, gage_id) + \".nc\"\n                # Check download data! If any error, save csv file to nc file\n                try:\n                    a_ncfile_data = xr.open_dataset(file_path).assign_coords(\n                        gauge_id=gage_id\n                    )\n                except Exception as e:\n                    # it seems there is sth. wrong with hysets_06444000.nc, hence we trans its csv to nc\n                    ts_csv_dir = self.data_source_description[\"TS_CSV_DIR\"]\n                    csv_file_path = os.path.join(ts_csv_dir, region, gage_id) + \".csv\"\n                    if not os.path.isfile(csv_file_path):\n                        raise FileNotFoundError(\n                            f\"No csv file found for {gage_id} in {region}\"\n                        ) from e\n                    _data = pd.read_csv(csv_file_path, sep=\",\", parse_dates=[\"date\"])\n                    non_datetime_columns = _data.select_dtypes(\n                        exclude=[\"datetime64[ns]\"]\n                    ).columns\n                    _data[non_datetime_columns] = _data[non_datetime_columns].astype(\n                        \"float32\"\n                    )\n                    # we assume the last nc file is ok\n                    attrs = a_ncfile_data.attrs\n                    the_ncfile_data = xr.Dataset.from_dataframe(\n                        _data.set_index([\"date\"])\n                    )\n                    the_ncfile_data.attrs = attrs\n                    # tf = TimezoneFinder()\n                    site_meta_file = os.path.join(\n                        self.data_source_description[\"ATTR_DIR\"],\n                        region,\n                        \"attributes_other_\" + region + \".csv\",\n                    )\n                    df_metadata = pd.read_csv(site_meta_file, sep=\",\").set_index(\n                        \"gauge_id\"\n                    )\n                    lat = df_metadata.loc[\n                        df_metadata.index == gage_id, \"gauge_lat\"\n                    ].values[0]\n                    lon = df_metadata.loc[\n                        df_metadata.index == gage_id, \"gauge_lon\"\n                    ].values[0]\n                    the_ncfile_data.attrs[\"Timezone\"] = get_tz(lat=lat, lng=lon)\n                    the_ncfile = os.path.join(\n                        os.path.dirname(file_path), f\"{gage_id}.nc\"\n                    )\n                    the_ncfile_data.to_netcdf(the_ncfile)\n\n    def read_attr_xrdataset(\n        self,\n        gage_id_lst: Optional[list] = None,\n        var_lst: Optional[list] = None,\n        to_numeric: bool = True,\n        **kwargs: Any,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read attribute data as xarray Dataset.\n\n        This method supports both standardized variable names and dataset-specific names.\n        If var_lst contains standardized names (e.g., 'area', 'p_mean'),\n        it will use the parent class implementation. Otherwise, it reads the raw attributes.\n\n        Args:\n            gage_id_lst (list, optional): List of gauge IDs to select.\n            var_lst (list, optional): List of variable names (standardized or raw).\n            to_numeric (bool, optional): Whether to convert non-numeric variables to codes.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            xr.Dataset: Dataset containing the requested attributes.\n        \"\"\"\n        # Check if var_lst contains only standardized variable names\n        if var_lst is not None:\n            all_standard = all(\n                var in self._static_variable_definitions for var in var_lst\n            )\n            if all_standard:\n                # Use parent class implementation for standardized variables\n                return super().read_attr_xrdataset(\n                    gage_id_lst=gage_id_lst,\n                    var_lst=var_lst,\n                    to_numeric=to_numeric,\n                    **kwargs,\n                )\n\n        # For raw attribute names or mixed names, use direct reading\n        file_path = self.cache_dir.joinpath(self._attributes_cache_filename)\n\n        if not os.path.isfile(file_path):\n            self.cache_attributes_xrdataset()\n\n        # Open the dataset\n        ds = xr.open_dataset(file_path)\n\n        # Select the basins\n        if gage_id_lst is not None:\n            gage_id_lst = [str(gid) for gid in gage_id_lst]\n            ds = ds.sel(basin=gage_id_lst)\n\n        # If relevant columns (attributes) are specified, select them\n        if var_lst:\n            ds = ds[var_lst]\n\n        return ds\n\n    def _load_ts_dataset(self, **kwargs: Any) -&gt; xr.Dataset:\n        \"\"\"Load the time series dataset from cache files.\n\n        This method overrides the parent method to handle multiple batch files.\n\n        Args:\n            **kwargs: Additional keyword arguments.\n                parallel (bool, optional): Whether to use parallel reading. Defaults to False.\n\n        Returns:\n            xr.Dataset: The loaded time series dataset.\n        \"\"\"\n        file_paths = sorted(glob.glob(self._timeseries_cache_filename))\n        if len(file_paths) == 0:\n            self.cache_timeseries_xrdataset(checkregion=\"all\")\n            file_paths = sorted(glob.glob(self._timeseries_cache_filename))\n\n        # Open the dataset in a lazy manner using dask\n        parallel = kwargs.get(\"parallel\", False)\n        combined_ds = xr.open_mfdataset(\n            file_paths,\n            combine=\"nested\",\n            concat_dim=\"basin\",\n            parallel=parallel,\n        )\n        return combined_ds\n\n    def read_ts_xrdataset(\n        self,\n        gage_id_lst: Optional[list] = None,\n        t_range: Optional[list] = None,\n        var_lst: Optional[list] = None,\n        sources: Optional[dict] = None,\n        **kwargs: Any,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Reads time series data as an xarray Dataset with standardized variable names.\n\n        Args:\n            gage_id_lst (list, optional): List of gauge IDs to select.\n            t_range (list, optional): Time range as [start, end].\n            var_lst (list, optional): List of standard variable names (e.g., StandardVariable.STREAMFLOW).\n            sources (dict, optional): Dictionary mapping standard variable names to their sources.\n            **kwargs: Additional keyword arguments. Notably:\n                parallel (bool): Whether to use parallel reading (default: False).\n\n        Returns:\n            xr.Dataset: Dataset with standardized variable names.\n        \"\"\"\n        # Use parent class implementation for variable mapping logic\n        final_ds = super().read_ts_xrdataset(\n            gage_id_lst=gage_id_lst,\n            t_range=t_range,\n            var_lst=var_lst,\n            sources=sources,\n            **kwargs,\n        )\n\n        # Fix units that are not recognized by pint_xarray\n        unit_mapping = {\n            \"W/m2\": \"watt / meter ** 2\",\n            \"m3/m3\": \"meter^3/meter^3\",\n            \"W/m^2\": \"watt / meter ** 2\",\n        }\n\n        for var in final_ds.data_vars:\n            if \"units\" in final_ds[var].attrs:\n                unit = final_ds[var].attrs[\"units\"]\n                # If the unit is in the mapping dictionary, replace it\n                final_ds[var].attrs[\"units\"] = unit_mapping.get(unit, unit)\n\n        return final_ds\n\n    @property\n    def streamflow_unit(self):\n        return \"mm/d\"\n\n    def read_area(self, gage_id_lst: Optional[list] = None) -&gt; xr.Dataset:\n        return self.read_attr_xrdataset(gage_id_lst, [\"area\"])\n\n    def read_mean_prcp(\n        self, gage_id_lst: Optional[list] = None, unit: str = \"mm/d\"\n    ) -&gt; xr.Dataset:\n        return self.read_attr_xrdataset(gage_id_lst, [\"p_mean\"])\n</code></pre>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/caravan/#hydrodataset.caravan.Caravan.__init__","title":"<code>__init__(data_path, region=None)</code>","text":"<p>Initialization for Caravan dataset</p> <p>TODO: The newest version is Version 1.6, but now we only support Version 0.3</p>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.__init__--parameters","title":"Parameters","text":"<p>data_path     where we put the dataset region     the region can be US, AUS, BR, CL, GB, CE, NA (North America, meaning HYSETS)</p> Source code in <code>hydrodataset/caravan.py</code> <pre><code>def __init__(self, data_path: str, region: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Initialization for Caravan dataset\n\n    TODO: The newest version is Version 1.6, but now we only support Version 0.3\n\n    Parameters\n    ----------\n    data_path\n        where we put the dataset\n    region\n        the region can be US, AUS, BR, CL, GB, CE, NA (North America, meaning HYSETS)\n    \"\"\"\n    self.data_path = os.path.join(data_path, \"CARAVAN\")\n    super().__init__(self.data_path)\n    self.region = \"Global\" if region is None else region\n    region_name_dict = self.region_name_dict\n    if self.region == \"Global\":\n        self.region_data_name = list(region_name_dict.values())\n    else:\n        self.region_data_name = region_name_dict[self.region]\n    self.data_source_description = self.set_data_source_describe()\n    try:\n        self.is_data_ready()\n    except FileNotFoundError as e:\n        warnings.warn(e)\n        print(\"Downloading dataset...\")\n        self.download_data_source()\n        print(\"Dataset downloaded successfully.\")\n        print(\"Checking if dataset is ready...\")\n        self.is_data_ready()\n        print(\"Dataset is ready.\")\n    self.sites = self.read_site_info()\n</code></pre>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.read_object_ids","title":"<code>read_object_ids(**kwargs)</code>","text":"<p>read station ids</p>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.read_object_ids--parameters","title":"Parameters","text":"<p>**kwargs     optional params if needed</p>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.read_object_ids--returns","title":"Returns","text":"<p>np.array     gage/station ids</p> Source code in <code>hydrodataset/caravan.py</code> <pre><code>def read_object_ids(self, **kwargs: Any) -&gt; np.array:\n    \"\"\"\n    read station ids\n\n    Parameters\n    ----------\n    **kwargs\n        optional params if needed\n\n    Returns\n    -------\n    np.array\n        gage/station ids\n    \"\"\"\n    return self.sites[\"gauge_id\"].values\n</code></pre>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.read_ts_xrdataset","title":"<code>read_ts_xrdataset(gage_id_lst=None, t_range=None, var_lst=None, sources=None, **kwargs)</code>","text":"<p>Reads time series data as an xarray Dataset with standardized variable names.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list</code> <p>List of gauge IDs to select.</p> <code>None</code> <code>t_range</code> <code>list</code> <p>Time range as [start, end].</p> <code>None</code> <code>var_lst</code> <code>list</code> <p>List of standard variable names (e.g., StandardVariable.STREAMFLOW).</p> <code>None</code> <code>sources</code> <code>dict</code> <p>Dictionary mapping standard variable names to their sources.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Notably: parallel (bool): Whether to use parallel reading (default: False).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: Dataset with standardized variable names.</p> Source code in <code>hydrodataset/caravan.py</code> <pre><code>def read_ts_xrdataset(\n    self,\n    gage_id_lst: Optional[list] = None,\n    t_range: Optional[list] = None,\n    var_lst: Optional[list] = None,\n    sources: Optional[dict] = None,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Reads time series data as an xarray Dataset with standardized variable names.\n\n    Args:\n        gage_id_lst (list, optional): List of gauge IDs to select.\n        t_range (list, optional): Time range as [start, end].\n        var_lst (list, optional): List of standard variable names (e.g., StandardVariable.STREAMFLOW).\n        sources (dict, optional): Dictionary mapping standard variable names to their sources.\n        **kwargs: Additional keyword arguments. Notably:\n            parallel (bool): Whether to use parallel reading (default: False).\n\n    Returns:\n        xr.Dataset: Dataset with standardized variable names.\n    \"\"\"\n    # Use parent class implementation for variable mapping logic\n    final_ds = super().read_ts_xrdataset(\n        gage_id_lst=gage_id_lst,\n        t_range=t_range,\n        var_lst=var_lst,\n        sources=sources,\n        **kwargs,\n    )\n\n    # Fix units that are not recognized by pint_xarray\n    unit_mapping = {\n        \"W/m2\": \"watt / meter ** 2\",\n        \"m3/m3\": \"meter^3/meter^3\",\n        \"W/m^2\": \"watt / meter ** 2\",\n    }\n\n    for var in final_ds.data_vars:\n        if \"units\" in final_ds[var].attrs:\n            unit = final_ds[var].attrs[\"units\"]\n            # If the unit is in the mapping dictionary, replace it\n            final_ds[var].attrs[\"units\"] = unit_mapping.get(unit, unit)\n\n    return final_ds\n</code></pre>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.read_attr_xrdataset","title":"<code>read_attr_xrdataset(gage_id_lst=None, var_lst=None, to_numeric=True, **kwargs)</code>","text":"<p>Read attribute data as xarray Dataset.</p> <p>This method supports both standardized variable names and dataset-specific names. If var_lst contains standardized names (e.g., 'area', 'p_mean'), it will use the parent class implementation. Otherwise, it reads the raw attributes.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list</code> <p>List of gauge IDs to select.</p> <code>None</code> <code>var_lst</code> <code>list</code> <p>List of variable names (standardized or raw).</p> <code>None</code> <code>to_numeric</code> <code>bool</code> <p>Whether to convert non-numeric variables to codes.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: Dataset containing the requested attributes.</p> Source code in <code>hydrodataset/caravan.py</code> <pre><code>def read_attr_xrdataset(\n    self,\n    gage_id_lst: Optional[list] = None,\n    var_lst: Optional[list] = None,\n    to_numeric: bool = True,\n    **kwargs: Any,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read attribute data as xarray Dataset.\n\n    This method supports both standardized variable names and dataset-specific names.\n    If var_lst contains standardized names (e.g., 'area', 'p_mean'),\n    it will use the parent class implementation. Otherwise, it reads the raw attributes.\n\n    Args:\n        gage_id_lst (list, optional): List of gauge IDs to select.\n        var_lst (list, optional): List of variable names (standardized or raw).\n        to_numeric (bool, optional): Whether to convert non-numeric variables to codes.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        xr.Dataset: Dataset containing the requested attributes.\n    \"\"\"\n    # Check if var_lst contains only standardized variable names\n    if var_lst is not None:\n        all_standard = all(\n            var in self._static_variable_definitions for var in var_lst\n        )\n        if all_standard:\n            # Use parent class implementation for standardized variables\n            return super().read_attr_xrdataset(\n                gage_id_lst=gage_id_lst,\n                var_lst=var_lst,\n                to_numeric=to_numeric,\n                **kwargs,\n            )\n\n    # For raw attribute names or mixed names, use direct reading\n    file_path = self.cache_dir.joinpath(self._attributes_cache_filename)\n\n    if not os.path.isfile(file_path):\n        self.cache_attributes_xrdataset()\n\n    # Open the dataset\n    ds = xr.open_dataset(file_path)\n\n    # Select the basins\n    if gage_id_lst is not None:\n        gage_id_lst = [str(gid) for gid in gage_id_lst]\n        ds = ds.sel(basin=gage_id_lst)\n\n    # If relevant columns (attributes) are specified, select them\n    if var_lst:\n        ds = ds[var_lst]\n\n    return ds\n</code></pre>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.read_area","title":"<code>read_area(gage_id_lst=None)</code>","text":"Source code in <code>hydrodataset/caravan.py</code> <pre><code>def read_area(self, gage_id_lst: Optional[list] = None) -&gt; xr.Dataset:\n    return self.read_attr_xrdataset(gage_id_lst, [\"area\"])\n</code></pre>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.read_mean_prcp","title":"<code>read_mean_prcp(gage_id_lst=None, unit='mm/d')</code>","text":"Source code in <code>hydrodataset/caravan.py</code> <pre><code>def read_mean_prcp(\n    self, gage_id_lst: Optional[list] = None, unit: str = \"mm/d\"\n) -&gt; xr.Dataset:\n    return self.read_attr_xrdataset(gage_id_lst, [\"p_mean\"])\n</code></pre>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.cache_xrdataset","title":"<code>cache_xrdataset(**kwargs)</code>","text":"<p>Save all attr data in a netcdf file in the cache directory, ts data are already nc format</p>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.cache_xrdataset--parameters","title":"Parameters","text":"<p>checkregion : str, optional     as my experience, the dameged file is in hysets, by default \"hysets\"</p> Source code in <code>hydrodataset/caravan.py</code> <pre><code>def cache_xrdataset(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Save all attr data in a netcdf file in the cache directory,\n    ts data are already nc format\n\n    Parameters\n    ----------\n    checkregion : str, optional\n        as my experience, the dameged file is in hysets, by default \"hysets\"\n    \"\"\"\n    checkregion = kwargs.get(\"checkregion\", \"hysets\")\n    warnings.warn(\"Check you units of all variables\")\n    self.cache_attributes_xrdataset()\n    self.cache_timeseries_xrdataset(checkregion)\n</code></pre>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.cache_attributes_xrdataset","title":"<code>cache_attributes_xrdataset()</code>","text":"<p>cache attributes in xr dataset</p> Source code in <code>hydrodataset/caravan.py</code> <pre><code>def cache_attributes_xrdataset(self) -&gt; None:\n    \"\"\"cache attributes in xr dataset\"\"\"\n    import pint_xarray  # noqa\n    import pint\n    from pint import UnitRegistry\n\n    data = self.read_constant_cols()\n    basin_ids = self.read_object_ids()\n    var_names = self.get_constant_cols()\n    assert all(x &lt;= y for x, y in zip(basin_ids, basin_ids[1:]))\n    if self.region == \"Global\":\n        # for all attrs reading in Global mode, all attrs are sorted\n        assert all(x &lt;= y for x, y in zip(var_names, var_names[1:]))\n    ds = xr.Dataset(\n        {var: ([\"basin\"], data[:, i]) for i, var in enumerate(var_names)},\n        coords={\"basin\": basin_ids},\n    )\n    units_dict = {attribute: \"\" for attribute in var_names}\n    # Define a dictionary for the provided attribute patterns and their units\n    # https://data.hydrosheds.org/file/technical-documentation/BasinATLAS_Catalog_v10.pdf\n    units_mapping = {\n        \"dis_m3_\": \"cubic meters/second\",\n        \"run_mm_\": \"millimeters\",\n        \"inu_pc_\": \"percent cover\",\n        \"lka_pc_\": \"percent cover (x10)\",\n        \"lkv_mc_\": \"million cubic meters\",\n        \"rev_mc_\": \"million cubic meters\",\n        \"dor_pc_\": \"percent (x10)\",\n        \"ria_ha_\": \"hectares\",\n        \"riv_tc_\": \"thousand cubic meters\",\n        \"gwt_cm_\": \"centimeters\",\n        \"ele_mt_\": \"meters a.s.l.\",\n        \"slp_dg_\": \"degrees (x10)\",\n        \"sgr_dk_\": \"decimeters per km\",\n        \"clz_cl_\": \"classes (18)\",\n        \"cls_cl_\": \"classes (125)\",\n        \"tmp_dc_\": \"degrees Celsius (x10)\",\n        \"pre_mm_\": \"millimeters\",\n        \"pet_mm_\": \"millimeters\",\n        \"aet_mm_\": \"millimeters\",\n        \"ari_ix_\": \"index value (x100)\",\n        \"cmi_ix_\": \"index value (x100)\",\n        \"snw_pc_\": \"percent cover\",\n        \"glc_cl_\": \"classes (22)\",\n        \"glc_pc_\": \"percent cover\",\n        \"pnv_cl_\": \"classes (15)\",\n        \"pnv_pc_\": \"percent cover\",\n        \"wet_cl_\": \"classes (12)\",\n        \"wet_pc_\": \"percent cover\",\n        \"for_pc_\": \"percent cover\",\n        \"crp_pc_\": \"percent cover\",\n        \"pst_pc_\": \"percent cover\",\n        \"ire_pc_\": \"percent cover\",\n        \"gla_pc_\": \"percent cover\",\n        \"prm_pc_\": \"percent cover\",\n        \"pac_pc_\": \"percent cover\",\n        \"tbi_cl_\": \"classes (14)\",\n        \"tec_cl_\": \"classes (846)\",\n        \"fmh_cl_\": \"classes (13)\",\n        \"fec_cl_\": \"classes (426)\",\n        \"cly_pc_\": \"percent\",\n        \"slt_pc_\": \"percent\",\n        \"snd_pc_\": \"percent\",\n        \"soc_th_\": \"tonnes/hectare\",\n        \"swc_pc_\": \"percent\",\n        \"lit_cl_\": \"classes (16)\",\n        \"kar_pc_\": \"percent cover\",\n        \"ero_kh_\": \"kg/hectare per year\",\n        \"pop_ct_\": \"count (thousands)\",\n        \"ppd_pk_\": \"people per km\u00b2\",\n        \"urb_pc_\": \"percent cover\",\n        \"nli_ix_\": \"index value (x100)\",\n        \"rdd_mk_\": \"meters per km\u00b2\",\n        \"hft_ix_\": \"index value (x10)\",\n        \"gad_id_\": \"ID number\",\n        \"gdp_ud_\": \"US dollars\",\n        \"hdi_ix_\": \"index value (x1000)\",\n    }\n\n    # Update the attributes_dict based on the units_mapping\n    for key_pattern, unit in units_mapping.items():\n        for key in units_dict:\n            if key.startswith(key_pattern):\n                units_dict[key] = unit\n\n    # for attrs not from hydroatlas in caravan, we directly set pint unit\n    units_dict[\"area\"] = \"km^2\"\n    units_dict[\"area_fraction_used_for_aggregation\"] = (\n        \"dimensionless\"  # this one is from atlas but not specified in the document\n    )\n    units_dict[\"aridity\"] = \"dimensionless\"\n    units_dict[\"country\"] = \"dimensionless\"\n    units_dict[\"frac_snow\"] = \"dimensionless\"\n    units_dict[\"gauge_lat\"] = \"degree\"\n    units_dict[\"gauge_lon\"] = \"degree\"\n    units_dict[\"gauge_name\"] = \"dimensionless\"\n    units_dict[\"high_prec_dur\"] = \"day\"\n    units_dict[\"high_prec_freq\"] = \"day/year\"\n    units_dict[\"low_prec_dur\"] = \"day\"\n    units_dict[\"low_prec_freq\"] = \"day/year\"\n    units_dict[\"moisture_index\"] = \"dimensionless\"\n    units_dict[\"p_mean\"] = \"mm/year\"\n    units_dict[\"pet_mean\"] = \"mm/year\"\n    units_dict[\"seasonality\"] = \"dimensionless\"\n\n    # Reinitialize unit registry and unit mapping dictionary\n    ureg = UnitRegistry()\n\n    pint_unit_mapping = {\n        \"cubic meters/second\": \"m^3/s\",\n        \"millimeters\": \"millimeter\",\n        \"percent cover\": \"percent\",\n        \"percent cover (x10)\": \"1e-1 * percent\",\n        \"million cubic meters\": \"1e6 * m^3\",\n        \"thousand cubic meters\": \"1e3 * m^3\",\n        \"centimeters\": \"centimeter\",\n        \"meters a.s.l.\": \"meter\",\n        \"degrees (x10)\": \"1e-1 * degree\",\n        \"decimeters per km\": \"decimeter/km\",\n        \"classes (18)\": \"dimensionless\",\n        \"classes (125)\": \"dimensionless\",\n        \"degrees Celsius (x10)\": \"degree_Celsius\",\n        \"index value (x100)\": \"1e-2\",\n        \"classes (22)\": \"dimensionless\",\n        \"classes (15)\": \"dimensionless\",\n        \"classes (12)\": \"dimensionless\",\n        \"classes (14)\": \"dimensionless\",\n        \"classes (846)\": \"dimensionless\",\n        \"classes (13)\": \"dimensionless\",\n        \"classes (426)\": \"dimensionless\",\n        \"percent\": \"percent\",\n        \"tonnes/hectare\": \"tonne/hectare\",\n        \"kg/hectare per year\": \"kg/hectare/year\",\n        \"count (thousands)\": \"1e3\",\n        \"people per km\u00b2\": \"1/km^2\",\n        \"index value (x1000)\": \"1e-3\",\n        \"meters per km\u00b2\": \"meter/km^2\",\n        \"index value (x10)\": \"1e-1\",\n        \"US dollars\": \"dimensionless\",\n        \"ID number\": \"dimensionless\",\n    }\n\n    # Validate each unit in the unit_mapping dictionary\n    valid_units = {}\n    invalid_units = {}\n\n    for provided_unit, pint_unit in pint_unit_mapping.items():\n        try:\n            ureg.parse_expression(pint_unit)\n            valid_units[provided_unit] = pint_unit\n        except pint.errors.UndefinedUnitError:\n            invalid_units[provided_unit] = pint_unit\n\n    converted_units = {\n        var: pint_unit_mapping.get(unit, unit) for var, unit in units_dict.items()\n    }\n    assert list(units_dict.keys()) == list(\n        converted_units.keys()\n    ), \"The keys of the dictionaries don't match or are not in the same order!\"\n\n    # for tmp_dc_ variable, we can't convert its unit to 0.1 * degree_Celsius\n    # hence we turn its value to degree_Celsius\n    for var in ds.data_vars:\n        if var.startswith(\"tmp_dc_\"):\n            ds[var] = ds[var] * 0.1\n\n    # Assign units to the variables in the Dataset\n    for var_name in converted_units:\n        if var_name in ds.data_vars:\n            ds[var_name].attrs[\"units\"] = converted_units[var_name]\n    cache_attr_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n    ds.to_netcdf(cache_attr_file)\n</code></pre>"},{"location":"api/caravan/#hydrodataset.caravan.Caravan.cache_timeseries_xrdataset","title":"<code>cache_timeseries_xrdataset(checkregion=None, **kwargs)</code>","text":"Source code in <code>hydrodataset/caravan.py</code> <pre><code>def cache_timeseries_xrdataset(\n    self, checkregion: Optional[str] = None, **kwargs: Any\n) -&gt; None:\n    if checkregion is not None:\n        regions = self.region_data_name if checkregion == \"all\" else [checkregion]\n        self._check_data(regions)\n\n    if isinstance(self.region_data_name, str):\n        region_data_name = [self.region_data_name]\n    else:\n        region_data_name = self.region_data_name\n    for region in region_data_name:\n        # all files are too large to read in memory, hence we read them region by region\n        site_file = os.path.join(\n            self.data_source_description[\"ATTR_DIR\"],\n            region,\n            \"attributes_caravan_\" + region + \".csv\",\n        )\n        sites_region = pd.read_csv(site_file, sep=\",\")\n        gage_id_lst = sites_region[\"gauge_id\"].values\n        batchsize = kwargs.get(\"batchsize\", 100)\n        t_range = kwargs.get(\"t_range\", None)\n        if t_range is None:\n            t_range = self.default_t_range\n        times = (\n            pd.date_range(start=t_range[0], end=t_range[-1], freq=\"1D\")\n            .strftime(\"%Y-%m-%d %H:%M:%S\")\n            .tolist()\n        )\n        variables = self.get_relevant_cols()\n        units_info = self._get_unit_json(gage_id_lst[0], region)\n\n        def data_generator(basins, batch_size):\n            for i in range(0, len(basins), batch_size):\n                yield basins[i : i + batch_size]\n\n        for basin_batch in data_generator(gage_id_lst, batchsize):\n            # we make sure the basin ids are sorted\n            assert all(x &lt;= y for x, y in zip(basin_batch, basin_batch[1:]))\n            data = self.read_timeseries(\n                region=region,\n                basin_ids=basin_batch,\n                t_range_list=t_range,\n                var_lst=variables,\n            )\n            dataset = xr.Dataset(\n                data_vars={\n                    variables[i]: (\n                        [\"basin\", \"time\"],\n                        data[:, :, i],\n                        {\"units\": units_info[variables[i]]},\n                    )\n                    for i in range(len(variables))\n                },\n                coords={\n                    \"basin\": basin_batch,\n                    \"time\": pd.to_datetime(times),\n                },\n            )\n\n            # Save the dataset to a NetCDF file for the current batch and time unit\n            prefix_ = \"\" if region is None else region + \"_\"\n            batch_file_path = self.cache_dir.joinpath(\n                f\"caravan_{prefix_}timeseries_batch_{basin_batch[0]}_{basin_batch[-1]}.nc\",\n            )\n            dataset.to_netcdf(batch_file_path)\n\n            # Release memory by deleting the dataset\n            del dataset\n            del data\n</code></pre>"},{"location":"api/caravan_dk/","title":"Caravan-DK","text":""},{"location":"api/caravan_dk/#overview","title":"Overview","text":"<p>Caravan-DK is the Denmark dataset from the Caravan project. Danish subset of the global Caravan dataset, providing standardized hydrological data for Danish catchments.</p>"},{"location":"api/caravan_dk/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Denmark</li> <li>Project: Caravan</li> <li>Module: <code>hydrodataset.caravan_dk</code></li> <li>Class: <code>CaravanDK</code></li> </ul>"},{"location":"api/caravan_dk/#about-caravan","title":"About Caravan","text":"<p>The Caravan project provides a global, standardized dataset of catchment attributes and meteorological forcings for large-sample hydrology. It combines data from multiple sources to create a unified dataset for hydrological modeling and analysis.</p>"},{"location":"api/caravan_dk/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Standardized variable naming across regions</li> <li>Quality-controlled data</li> <li>Comprehensive catchment attributes</li> <li>Multiple meteorological data sources</li> <li>Suitable for machine learning applications</li> </ul>"},{"location":"api/caravan_dk/#features","title":"Features","text":""},{"location":"api/caravan_dk/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area and geometry - Topographic characteristics - Land cover information - Soil properties - Climate indices - Human impact indicators</p>"},{"location":"api/caravan_dk/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available: - Streamflow (observed) - Precipitation (multiple sources) - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - Snow water equivalent - And more...</p>"},{"location":"api/caravan_dk/#usage","title":"Usage","text":""},{"location":"api/caravan_dk/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.caravan_dk import CaravanDK\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CaravanDK(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/caravan_dk/#working-with-multiple-data-sources","title":"Working with Multiple Data Sources","text":"<p>Caravan datasets often provide multiple precipitation and temperature sources:</p> <pre><code># Compare different precipitation products\nprecip_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:3],\n    t_range=[\"2000-01-01\", \"2005-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5\"),\n        (\"precipitation\", \"mswep\"),\n        (\"precipitation\", \"chirps\")\n    ]\n)\n\n# Use specific meteorological forcing\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"2000-01-01\", \"2010-12-31\"],\n    var_lst=[\n        \"streamflow\",\n        (\"precipitation\", \"era5land\"),\n        (\"temperature_mean\", \"era5land\")\n    ]\n)\n</code></pre>"},{"location":"api/caravan_dk/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"2020-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\", \"pet\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/caravan_dk/#data-quality","title":"Data Quality","text":"<p>Caravan datasets undergo quality control: - Removal of unrealistic values - Gap filling documentation - Metadata completeness checks - Cross-validation with regional datasets</p>"},{"location":"api/caravan_dk/#api-reference","title":"API Reference","text":""},{"location":"api/caravan_dk/#hydrodataset.caravan_dk.CaravanDK","title":"<code>hydrodataset.caravan_dk.CaravanDK</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>Caravan_DK dataset class extending HydroDataset.</p> <p>This class uses a custom data reading implementation to support a newer dataset version than the one supported by the underlying aquafetch library. It overrides the download URLs and provides its own parsing and caching logic.</p> Source code in <code>hydrodataset/caravan_dk.py</code> <pre><code>class CaravanDK(HydroDataset):\n    \"\"\"Caravan_DK dataset class extending HydroDataset.\n\n    This class uses a custom data reading implementation to support a newer\n    dataset version than the one supported by the underlying aquafetch library.\n    It overrides the download URLs and provides its own parsing and caching logic.\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize Caravan_DK dataset.\n\n        Args:\n            data_path: Path to the Caravan_DK data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n\n        # Define the new URLs for the latest dataset version\n        new_url = \"https://zenodo.org/records/15200118\"\n\n        def do_nothing(self, *args, **kwargs):\n            pass\n\n        def custom_boundary_file(self) -&gt; os.PathLike:\n            return os.path.join(\n                self.path, \"shapefiles\", \"camelsdk\", \"camelsdk_basin_shapes.shp\"\n            )\n\n        def custom_csv_path(self):\n            return os.path.join(self.path, \"timeseries\", \"csv\", \"camelsdk\")\n\n        def custom_nc_path(self):\n            return os.path.join(self.path, \"timeseries\", \"netcdf\", \"camelsdk\")\n\n        def custom_other_attr_fpath(self):\n            \"\"\"returns path to attributes_other_camelsdk.csv file\"\"\"\n            return os.path.join(\n                self.path, \"attributes\", \"camelsdk\", \"attributes_other_camelsdk.csv\"\n            )\n\n        def custom_caravan_attr_fpath(self):\n            \"\"\"returns path to attributes_caravan_camelsdk.csv file\"\"\"\n            return os.path.join(\n                self.path, \"attributes\", \"camelsdk\", \"attributes_caravan_camelsdk.csv\"\n            )\n\n        def custom_hyd_atlas_fpath(self):\n            return os.path.join(\n                self.path,\n                \"attributes\",\n                \"camelsdk\",\n                \"attributes_hydroatlas_camelsdk.csv\",\n            )\n\n        # Create class attributes dictionary for dynamic class creation\n        class_attrs = {\n            \"url\": new_url,\n            \"boundary_file\": property(custom_boundary_file),\n            \"csv_path\": property(custom_csv_path),\n            \"nc_path\": property(custom_nc_path),\n            \"other_attr_fpath\": property(custom_other_attr_fpath),\n            \"caravan_attr_fpath\": property(custom_caravan_attr_fpath),\n            \"hyd_atlas_fpath\": property(custom_hyd_atlas_fpath),\n            \"_maybe_to_netcdf\": do_nothing,\n        }\n\n        # Create a custom Caravan_DK class using type() to preserve the class name\n        CustomCaravanDK = type(\"Caravan_DK\", (Caravan_DK,), class_attrs)\n\n        # Instantiate our custom class\n        self.aqua_fetch = CustomCaravanDK(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"caravan_dk_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"caravan_dk_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1981-01-02\", \"2020-12-31\"]\n\n    # Define standardized static variable mappings\n    # These variables are already present in the dataset, so we just map them\n    # get the information of features from \"https://essd.copernicus.org/articles/17/1551/2025/essd-17-1551-2025.pdf\"\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    # Define standardized dynamic variable mappings\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"}\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"era5_land\",\n            \"sources\": {\n                \"era5_land\": {\n                    \"specific_name\": \"total_precipitation_sum\",\n                    \"unit\": \"mm/day\",\n                }\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"era5_land\",\n            \"sources\": {\n                \"era5_land\": {\"specific_name\": \"temperature_2m_max\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\n                    \"specific_name\": \"dewpoint_temperature_2m_max\",\n                    \"unit\": \"\u00b0C\",\n                },\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"era5_land\",\n            \"sources\": {\n                \"era5_land\": {\"specific_name\": \"temperature_2m_min\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\n                    \"specific_name\": \"dewpoint_temperature_2m_min\",\n                    \"unit\": \"\u00b0C\",\n                },\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"era5_land\",\n            \"sources\": {\n                \"era5_land\": {\"specific_name\": \"temperature_2m_mean\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\n                    \"specific_name\": \"dewpoint_temperature_2m_mean\",\n                    \"unit\": \"\u00b0C\",\n                },\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"era5_land\",\n            \"sources\": {\n                \"era5_land\": {\n                    \"specific_name\": \"potential_evaporation_sum_era5_land\",\n                    \"unit\": \"mm/day\",\n                },\n                \"fao_penman_monteith\": {\n                    \"specific_name\": \"potential_evaporation_sum_fao_penman_monteith\",\n                    \"unit\": \"mm/day\",\n                },\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\n                    \"specific_name\": \"snow_depth_water_equivalent_mean\",\n                    \"unit\": \"mm\",\n                },\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\n                    \"specific_name\": \"snow_depth_water_equivalent_min\",\n                    \"unit\": \"mm\",\n                },\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\n                    \"specific_name\": \"snow_depth_water_equivalent_max\",\n                    \"unit\": \"mm\",\n                },\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\n                    \"specific_name\": \"surface_net_solar_radiation_mean\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\n                    \"specific_name\": \"surface_net_solar_radiation_min\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\n                    \"specific_name\": \"surface_net_solar_radiation_max\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.THERMAL_RADIATION_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\n                    \"specific_name\": \"surface_net_thermal_radiation_min\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.THERMAL_RADIATION_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\n                    \"specific_name\": \"surface_net_thermal_radiation_max\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.THERMAL_RADIATION: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\n                    \"specific_name\": \"surface_net_thermal_radiation_mean\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\"specific_name\": \"surface_pressure_min\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\"specific_name\": \"surface_pressure_max\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\"specific_name\": \"surface_pressure_mean\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\"specific_name\": \"u_component_of_wind_10m_min\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\"specific_name\": \"u_component_of_wind_10m_max\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\n                    \"specific_name\": \"u_component_of_wind_10m_mean\",\n                    \"unit\": \"m/s\",\n                },\n            },\n        },\n        StandardVariable.V_WIND_SPEED_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\"specific_name\": \"v_component_of_wind_10m_min\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.V_WIND_SPEED_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\"specific_name\": \"v_component_of_wind_10m_max\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\n                    \"specific_name\": \"v_component_of_wind_10m_mean\",\n                    \"unit\": \"m/s\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_1_min\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_1_max\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_1_mean\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_2_min\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_2_max\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_2_mean\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_3_min\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_3_max\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_3_mean\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4_MIN: {\n            \"default_source\": \"min\",\n            \"sources\": {\n                \"min\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_4_min\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4_MAX: {\n            \"default_source\": \"max\",\n            \"sources\": {\n                \"max\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_4_max\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4: {\n            \"default_source\": \"mean\",\n            \"sources\": {\n                \"mean\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_4_mean\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n    }\n</code></pre>"},{"location":"api/caravan_dk/#hydrodataset.caravan_dk.CaravanDK.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/caravan_dk/#hydrodataset.caravan_dk.CaravanDK.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize Caravan_DK dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the Caravan_DK data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/caravan_dk.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize Caravan_DK dataset.\n\n    Args:\n        data_path: Path to the Caravan_DK data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n\n    # Define the new URLs for the latest dataset version\n    new_url = \"https://zenodo.org/records/15200118\"\n\n    def do_nothing(self, *args, **kwargs):\n        pass\n\n    def custom_boundary_file(self) -&gt; os.PathLike:\n        return os.path.join(\n            self.path, \"shapefiles\", \"camelsdk\", \"camelsdk_basin_shapes.shp\"\n        )\n\n    def custom_csv_path(self):\n        return os.path.join(self.path, \"timeseries\", \"csv\", \"camelsdk\")\n\n    def custom_nc_path(self):\n        return os.path.join(self.path, \"timeseries\", \"netcdf\", \"camelsdk\")\n\n    def custom_other_attr_fpath(self):\n        \"\"\"returns path to attributes_other_camelsdk.csv file\"\"\"\n        return os.path.join(\n            self.path, \"attributes\", \"camelsdk\", \"attributes_other_camelsdk.csv\"\n        )\n\n    def custom_caravan_attr_fpath(self):\n        \"\"\"returns path to attributes_caravan_camelsdk.csv file\"\"\"\n        return os.path.join(\n            self.path, \"attributes\", \"camelsdk\", \"attributes_caravan_camelsdk.csv\"\n        )\n\n    def custom_hyd_atlas_fpath(self):\n        return os.path.join(\n            self.path,\n            \"attributes\",\n            \"camelsdk\",\n            \"attributes_hydroatlas_camelsdk.csv\",\n        )\n\n    # Create class attributes dictionary for dynamic class creation\n    class_attrs = {\n        \"url\": new_url,\n        \"boundary_file\": property(custom_boundary_file),\n        \"csv_path\": property(custom_csv_path),\n        \"nc_path\": property(custom_nc_path),\n        \"other_attr_fpath\": property(custom_other_attr_fpath),\n        \"caravan_attr_fpath\": property(custom_caravan_attr_fpath),\n        \"hyd_atlas_fpath\": property(custom_hyd_atlas_fpath),\n        \"_maybe_to_netcdf\": do_nothing,\n    }\n\n    # Create a custom Caravan_DK class using type() to preserve the class name\n    CustomCaravanDK = type(\"Caravan_DK\", (Caravan_DK,), class_attrs)\n\n    # Instantiate our custom class\n    self.aqua_fetch = CustomCaravanDK(data_path)\n</code></pre>"},{"location":"api/estreams/","title":"E-STREAMS","text":""},{"location":"api/estreams/#overview","title":"Overview","text":"<p>E-STREAMS is a hydrological dataset for Europe. European dataset providing streamflow data across multiple European countries with standardized processing.</p>"},{"location":"api/estreams/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Europe</li> <li>Module: <code>hydrodataset.estreams</code></li> <li>Class: <code>Estreams</code></li> </ul>"},{"location":"api/estreams/#features","title":"Features","text":""},{"location":"api/estreams/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/estreams/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available: - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - And additional variables depending on the dataset</p>"},{"location":"api/estreams/#usage","title":"Usage","text":""},{"location":"api/estreams/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.estreams import Estreams\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = Estreams(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Check default time range\nprint(f\"Default time range: {ds.default_t_range}\")\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/estreams/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"2020-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/estreams/#working-with-multiple-data-sources","title":"Working with Multiple Data Sources","text":"<p>If the dataset provides multiple sources for variables:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/estreams/#api-reference","title":"API Reference","text":""},{"location":"api/estreams/#hydrodataset.estreams.Estreams","title":"<code>hydrodataset.estreams.Estreams</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>EStreams dataset class extending HydroDataset.</p> <p>This class uses a custom data reading implementation to support a newer dataset version than the one supported by the underlying aquafetch library. It overrides the download URLs and provides updated methods.</p> Source code in <code>hydrodataset/estreams.py</code> <pre><code>class Estreams(HydroDataset):\n    \"\"\"EStreams dataset class extending HydroDataset.\n\n    This class uses a custom data reading implementation to support a newer\n    dataset version than the one supported by the underlying aquafetch library.\n    It overrides the download URLs and provides updated methods.\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize EStreams dataset.\n\n        Args:\n            data_path: Path to the EStreams data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n\n        # Instantiate EStreams from aqua_fetch\n        # The _read_stn_dyn method and path2 fix have been added directly to aqua_fetch\n        self.aqua_fetch = EStreams(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"estreams_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"estreams_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1950-01-01\", \"2023-06-30\"]\n\n    # get the information of features from \"https://www.nature.com/articles/s41597-024-03706-1/tables/6\"\n    # Define standardized static variable mappings\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    # Define standardized dynamic variable mappings\n    _dynamic_variable_mapping = {\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"estreams\",\n            \"sources\": {\"estreams\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"estreams\",\n            \"sources\": {\"estreams\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"estreams\",\n            \"sources\": {\"estreams\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"estreams\",\n            \"sources\": {\"estreams\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"estreams\",\n            \"sources\": {\"estreams\": {\"specific_name\": \"sp_mean\", \"unit\": \"hPa\"}},\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"estreams\",\n            \"sources\": {\"estreams\": {\"specific_name\": \"rh_\", \"unit\": \"%\"}},\n        },\n        StandardVariable.WIND_SPEED: {\n            \"default_source\": \"estreams\",\n            \"sources\": {\"estreams\": {\"specific_name\": \"windspeed_mps\", \"unit\": \"m/s\"}},\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"estreams\",\n            \"sources\": {\"estreams\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"}},\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"estreams\",\n            \"sources\": {\"estreams\": {\"specific_name\": \"pet_mm\", \"unit\": \"mm/day\"}},\n        },\n    }\n\n    def cache_timeseries_xrdataset(self, batch_size=100):\n        \"\"\"\n        Cache timeseries data to NetCDF files in batches\n\n        Args:\n            batch_size: Number of stations to process per batch, default is 100 stations\n        \"\"\"\n        if not hasattr(self, \"aqua_fetch\"):\n            raise NotImplementedError(\"aqua_fetch attribute is required\")\n\n        # Build mapping from variable names to units\n        unit_lookup = {}\n        if hasattr(self, \"_dynamic_variable_mapping\"):\n            for std_name, mapping_info in self._dynamic_variable_mapping.items():\n                for source, source_info in mapping_info[\"sources\"].items():\n                    unit_lookup[source_info[\"specific_name\"]] = source_info[\"unit\"]\n\n        # Get all station IDs\n        gage_id_lst = self.read_object_ids().tolist()\n        total_stations = len(gage_id_lst)\n\n        # Get original variable list and clean\n        original_var_lst = self.aqua_fetch.dynamic_features\n        cleaned_var_lst = self._clean_feature_names(original_var_lst)\n        var_name_mapping = dict(zip(original_var_lst, cleaned_var_lst))\n\n        print(\n            f\"Start batch processing {total_stations} stations, {batch_size} stations per batch\"\n        )\n        print(\n            f\"Total number of batches: {(total_stations + batch_size - 1)//batch_size}\"\n        )\n\n        # Ensure cache directory exists\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n        # Process stations in batches and save independently\n        batch_num = 1\n        for batch_idx in range(0, total_stations, batch_size):\n            batch_end = min(batch_idx + batch_size, total_stations)\n            batch_stations = gage_id_lst[batch_idx:batch_end]\n\n            print(\n                f\"\\nProcessing batch {batch_num}/{(total_stations + batch_size - 1)//batch_size}\"\n            )\n            print(\n                f\"Station range: {batch_idx} - {batch_end-1} (total {len(batch_stations)} stations)\"\n            )\n\n            try:\n                # Get data for this batch\n                batch_data = self.aqua_fetch.fetch_stations_features(\n                    stations=batch_stations,\n                    dynamic_features=original_var_lst,\n                    static_features=None,\n                    st=self.default_t_range[0],\n                    en=self.default_t_range[1],\n                    as_dataframe=False,\n                )\n\n                dynamic_data = (\n                    batch_data[1] if isinstance(batch_data, tuple) else batch_data\n                )\n\n                # Process variables\n                new_data_vars = {}\n                time_coord = dynamic_data.coords[\"time\"]\n\n                for original_var in tqdm(\n                    original_var_lst,\n                    desc=f\"Processing variables (batch {batch_num})\",\n                    total=len(original_var_lst),\n                ):\n                    cleaned_var = var_name_mapping[original_var]\n                    var_data = []\n                    for station in batch_stations:\n                        if station in dynamic_data.data_vars:\n                            station_data = dynamic_data[station].sel(\n                                dynamic_features=original_var\n                            )\n                            if \"dynamic_features\" in station_data.coords:\n                                station_data = station_data.drop(\"dynamic_features\")\n                            var_data.append(station_data)\n\n                    if var_data:\n                        combined = xr.concat(var_data, dim=\"basin\")\n                        combined[\"basin\"] = batch_stations\n                        combined.attrs[\"units\"] = unit_lookup.get(\n                            cleaned_var, \"unknown\"\n                        )\n                        new_data_vars[cleaned_var] = combined\n\n                # Create Dataset for this batch\n                batch_ds = xr.Dataset(\n                    data_vars=new_data_vars,\n                    coords={\n                        \"basin\": batch_stations,\n                        \"time\": time_coord,\n                    },\n                )\n\n                # Save this batch to independent file\n                batch_filename = f\"batch{batch_num:03d}_estreams_timeseries.nc\"\n                batch_filepath = self.cache_dir.joinpath(batch_filename)\n\n                print(f\"Saving batch {batch_num} to: {batch_filepath}\")\n                batch_ds.to_netcdf(batch_filepath)\n                print(f\"Batch {batch_num} saved successfully\")\n\n            except Exception as e:\n                print(f\"Batch {batch_num} processing failed: {e}\")\n                import traceback\n\n                traceback.print_exc()\n                continue\n\n            batch_num += 1\n\n        print(f\"\\nAll batches processed! Total {batch_num - 1} batch files saved\")\n\n    def read_ts_xrdataset(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        sources: dict = None,\n        **kwargs,\n    ) -&gt; xr.Dataset:\n        \"\"\"\n        Read timeseries data from batch-saved cache files\n\n        Args:\n            gage_id_lst: List of station IDs\n            t_range: Time range [start, end]\n            var_lst: List of standard variable names\n            sources: Data source dictionary\n\n        Returns:\n            xr.Dataset: xarray dataset containing requested data\n        \"\"\"\n        if (\n            not hasattr(self, \"_dynamic_variable_mapping\")\n            or not self._dynamic_variable_mapping\n        ):\n            raise NotImplementedError(\n                \"This dataset does not support the standardized variable mapping.\"\n            )\n\n        if var_lst is None:\n            var_lst = list(self._dynamic_variable_mapping.keys())\n\n        if t_range is None:\n            t_range = self.default_t_range\n\n        target_vars_to_fetch = []\n        rename_map = {}\n\n        # Process variable name mapping and data source selection\n        for std_name in var_lst:\n            if std_name not in self._dynamic_variable_mapping:\n                raise ValueError(\n                    f\"'{std_name}' is not a recognized standard variable for this dataset.\"\n                )\n\n            mapping_info = self._dynamic_variable_mapping[std_name]\n\n            # Determine which data source(s) to use\n            is_explicit_source = sources and std_name in sources\n            sources_to_use = []\n            if is_explicit_source:\n                provided_sources = sources[std_name]\n                if isinstance(provided_sources, list):\n                    sources_to_use.extend(provided_sources)\n                else:\n                    sources_to_use.append(provided_sources)\n            else:\n                sources_to_use.append(mapping_info[\"default_source\"])\n\n            # Only need suffix when user explicitly requests multiple data sources\n            needs_suffix = is_explicit_source and len(sources_to_use) &gt; 1\n            for source in sources_to_use:\n                if source not in mapping_info[\"sources\"]:\n                    raise ValueError(\n                        f\"Source '{source}' is not available for variable '{std_name}'.\"\n                    )\n\n                actual_var_name = mapping_info[\"sources\"][source][\"specific_name\"]\n                target_vars_to_fetch.append(actual_var_name)\n                output_name = f\"{std_name}_{source}\" if needs_suffix else std_name\n                rename_map[actual_var_name] = output_name\n\n        # Find all batch files\n        batch_pattern = str(self.cache_dir / \"batch*_estreams_timeseries.nc\")\n        batch_files = sorted(glob.glob(batch_pattern))\n\n        if not batch_files:\n            print(\"No batch cache files found, starting cache creation...\")\n            self.cache_timeseries_xrdataset()\n            batch_files = sorted(glob.glob(batch_pattern))\n\n            if not batch_files:\n                raise FileNotFoundError(\"Cache creation failed, no batch files found\")\n\n        print(f\"Found {len(batch_files)} batch files\")\n\n        # If no stations specified, read all stations\n        if gage_id_lst is None:\n            print(\"No station list specified, will read all stations...\")\n            gage_id_lst = self.read_object_ids().tolist()\n\n        # Convert station IDs to strings (ensure consistency)\n        gage_id_lst = [str(gid) for gid in gage_id_lst]\n\n        # Iterate through batch files to find batches containing required stations\n        relevant_datasets = []\n        for batch_file in batch_files:\n            try:\n                # First open only coordinates, don't load data\n                ds_batch = xr.open_dataset(batch_file)\n                batch_basins = [str(b) for b in ds_batch.basin.values]\n\n                # Check if this batch contains required stations\n                common_basins = list(set(gage_id_lst) &amp; set(batch_basins))\n\n                if common_basins:\n                    print(\n                        f\"Batch {os.path.basename(batch_file)}: contains {len(common_basins)} required stations\"\n                    )\n\n                    # Check if variables exist\n                    missing_vars = [\n                        v for v in target_vars_to_fetch if v not in ds_batch.data_vars\n                    ]\n                    if missing_vars:\n                        ds_batch.close()\n                        raise ValueError(\n                            f\"Batch {os.path.basename(batch_file)} missing variables: {missing_vars}\"\n                        )\n\n                    # Select variables and stations\n                    ds_subset = ds_batch[target_vars_to_fetch]\n                    ds_selected = ds_subset.sel(\n                        basin=common_basins, time=slice(t_range[0], t_range[1])\n                    )\n\n                    relevant_datasets.append(ds_selected)\n                    ds_batch.close()\n                else:\n                    ds_batch.close()\n\n            except Exception as e:\n                print(f\"Failed to read batch file {batch_file}: {e}\")\n                continue\n\n        if not relevant_datasets:\n            raise ValueError(\n                f\"Specified stations not found in any batch files: {gage_id_lst}\"\n            )\n\n        print(f\"Reading data from {len(relevant_datasets)} batches...\")\n\n        # Merge data from all relevant batches\n        if len(relevant_datasets) == 1:\n            final_ds = relevant_datasets[0]\n        else:\n            final_ds = xr.concat(relevant_datasets, dim=\"basin\")\n\n        # Rename to standard variable names\n        final_ds = final_ds.rename(rename_map)\n\n        # Ensure stations are arranged in input order\n        if len(gage_id_lst) &gt; 0:\n            # Only select actually existing stations\n            existing_basins = [b for b in gage_id_lst if b in final_ds.basin.values]\n            if existing_basins:\n                final_ds = final_ds.sel(basin=existing_basins)\n\n        return final_ds\n</code></pre>"},{"location":"api/estreams/#hydrodataset.estreams.Estreams.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/estreams/#hydrodataset.estreams.Estreams.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize EStreams dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the EStreams data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/estreams.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize EStreams dataset.\n\n    Args:\n        data_path: Path to the EStreams data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n\n    # Instantiate EStreams from aqua_fetch\n    # The _read_stn_dyn method and path2 fix have been added directly to aqua_fetch\n    self.aqua_fetch = EStreams(data_path)\n</code></pre>"},{"location":"api/estreams/#hydrodataset.estreams.Estreams.read_ts_xrdataset","title":"<code>read_ts_xrdataset(gage_id_lst=None, t_range=None, var_lst=None, sources=None, **kwargs)</code>","text":"<p>Read timeseries data from batch-saved cache files</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list</code> <p>List of station IDs</p> <code>None</code> <code>t_range</code> <code>list</code> <p>Time range [start, end]</p> <code>None</code> <code>var_lst</code> <code>list</code> <p>List of standard variable names</p> <code>None</code> <code>sources</code> <code>dict</code> <p>Data source dictionary</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: xarray dataset containing requested data</p> Source code in <code>hydrodataset/estreams.py</code> <pre><code>def read_ts_xrdataset(\n    self,\n    gage_id_lst: list = None,\n    t_range: list = None,\n    var_lst: list = None,\n    sources: dict = None,\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Read timeseries data from batch-saved cache files\n\n    Args:\n        gage_id_lst: List of station IDs\n        t_range: Time range [start, end]\n        var_lst: List of standard variable names\n        sources: Data source dictionary\n\n    Returns:\n        xr.Dataset: xarray dataset containing requested data\n    \"\"\"\n    if (\n        not hasattr(self, \"_dynamic_variable_mapping\")\n        or not self._dynamic_variable_mapping\n    ):\n        raise NotImplementedError(\n            \"This dataset does not support the standardized variable mapping.\"\n        )\n\n    if var_lst is None:\n        var_lst = list(self._dynamic_variable_mapping.keys())\n\n    if t_range is None:\n        t_range = self.default_t_range\n\n    target_vars_to_fetch = []\n    rename_map = {}\n\n    # Process variable name mapping and data source selection\n    for std_name in var_lst:\n        if std_name not in self._dynamic_variable_mapping:\n            raise ValueError(\n                f\"'{std_name}' is not a recognized standard variable for this dataset.\"\n            )\n\n        mapping_info = self._dynamic_variable_mapping[std_name]\n\n        # Determine which data source(s) to use\n        is_explicit_source = sources and std_name in sources\n        sources_to_use = []\n        if is_explicit_source:\n            provided_sources = sources[std_name]\n            if isinstance(provided_sources, list):\n                sources_to_use.extend(provided_sources)\n            else:\n                sources_to_use.append(provided_sources)\n        else:\n            sources_to_use.append(mapping_info[\"default_source\"])\n\n        # Only need suffix when user explicitly requests multiple data sources\n        needs_suffix = is_explicit_source and len(sources_to_use) &gt; 1\n        for source in sources_to_use:\n            if source not in mapping_info[\"sources\"]:\n                raise ValueError(\n                    f\"Source '{source}' is not available for variable '{std_name}'.\"\n                )\n\n            actual_var_name = mapping_info[\"sources\"][source][\"specific_name\"]\n            target_vars_to_fetch.append(actual_var_name)\n            output_name = f\"{std_name}_{source}\" if needs_suffix else std_name\n            rename_map[actual_var_name] = output_name\n\n    # Find all batch files\n    batch_pattern = str(self.cache_dir / \"batch*_estreams_timeseries.nc\")\n    batch_files = sorted(glob.glob(batch_pattern))\n\n    if not batch_files:\n        print(\"No batch cache files found, starting cache creation...\")\n        self.cache_timeseries_xrdataset()\n        batch_files = sorted(glob.glob(batch_pattern))\n\n        if not batch_files:\n            raise FileNotFoundError(\"Cache creation failed, no batch files found\")\n\n    print(f\"Found {len(batch_files)} batch files\")\n\n    # If no stations specified, read all stations\n    if gage_id_lst is None:\n        print(\"No station list specified, will read all stations...\")\n        gage_id_lst = self.read_object_ids().tolist()\n\n    # Convert station IDs to strings (ensure consistency)\n    gage_id_lst = [str(gid) for gid in gage_id_lst]\n\n    # Iterate through batch files to find batches containing required stations\n    relevant_datasets = []\n    for batch_file in batch_files:\n        try:\n            # First open only coordinates, don't load data\n            ds_batch = xr.open_dataset(batch_file)\n            batch_basins = [str(b) for b in ds_batch.basin.values]\n\n            # Check if this batch contains required stations\n            common_basins = list(set(gage_id_lst) &amp; set(batch_basins))\n\n            if common_basins:\n                print(\n                    f\"Batch {os.path.basename(batch_file)}: contains {len(common_basins)} required stations\"\n                )\n\n                # Check if variables exist\n                missing_vars = [\n                    v for v in target_vars_to_fetch if v not in ds_batch.data_vars\n                ]\n                if missing_vars:\n                    ds_batch.close()\n                    raise ValueError(\n                        f\"Batch {os.path.basename(batch_file)} missing variables: {missing_vars}\"\n                    )\n\n                # Select variables and stations\n                ds_subset = ds_batch[target_vars_to_fetch]\n                ds_selected = ds_subset.sel(\n                    basin=common_basins, time=slice(t_range[0], t_range[1])\n                )\n\n                relevant_datasets.append(ds_selected)\n                ds_batch.close()\n            else:\n                ds_batch.close()\n\n        except Exception as e:\n            print(f\"Failed to read batch file {batch_file}: {e}\")\n            continue\n\n    if not relevant_datasets:\n        raise ValueError(\n            f\"Specified stations not found in any batch files: {gage_id_lst}\"\n        )\n\n    print(f\"Reading data from {len(relevant_datasets)} batches...\")\n\n    # Merge data from all relevant batches\n    if len(relevant_datasets) == 1:\n        final_ds = relevant_datasets[0]\n    else:\n        final_ds = xr.concat(relevant_datasets, dim=\"basin\")\n\n    # Rename to standard variable names\n    final_ds = final_ds.rename(rename_map)\n\n    # Ensure stations are arranged in input order\n    if len(gage_id_lst) &gt; 0:\n        # Only select actually existing stations\n        existing_basins = [b for b in gage_id_lst if b in final_ds.basin.values]\n        if existing_basins:\n            final_ds = final_ds.sel(basin=existing_basins)\n\n    return final_ds\n</code></pre>"},{"location":"api/grdc_caravan/","title":"GRDC-Caravan","text":""},{"location":"api/grdc_caravan/#overview","title":"Overview","text":"<p>GRDC-Caravan is the Global dataset from the Caravan project. Global Runoff Data Centre (GRDC) dataset integrated with Caravan, providing worldwide streamflow data from major rivers.</p>"},{"location":"api/grdc_caravan/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Global</li> <li>Project: Caravan</li> <li>Module: <code>hydrodataset.grdc_caravan</code></li> <li>Class: <code>GrdcCaravan</code></li> </ul>"},{"location":"api/grdc_caravan/#about-caravan","title":"About Caravan","text":"<p>The Caravan project provides a global, standardized dataset of catchment attributes and meteorological forcings for large-sample hydrology. It combines data from multiple sources to create a unified dataset for hydrological modeling and analysis.</p>"},{"location":"api/grdc_caravan/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Standardized variable naming across regions</li> <li>Quality-controlled data</li> <li>Comprehensive catchment attributes</li> <li>Multiple meteorological data sources</li> <li>Suitable for machine learning applications</li> </ul>"},{"location":"api/grdc_caravan/#features","title":"Features","text":""},{"location":"api/grdc_caravan/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area and geometry - Topographic characteristics - Land cover information - Soil properties - Climate indices - Human impact indicators</p>"},{"location":"api/grdc_caravan/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available: - Streamflow (observed) - Precipitation (multiple sources) - Temperature (min, max, mean) - Potential evapotranspiration - Solar radiation - Snow water equivalent - And more...</p>"},{"location":"api/grdc_caravan/#usage","title":"Usage","text":""},{"location":"api/grdc_caravan/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.grdc_caravan import GrdcCaravan\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = GrdcCaravan(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/grdc_caravan/#working-with-multiple-data-sources","title":"Working with Multiple Data Sources","text":"<p>Caravan datasets often provide multiple precipitation and temperature sources:</p> <pre><code># Compare different precipitation products\nprecip_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:3],\n    t_range=[\"2000-01-01\", \"2005-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5\"),\n        (\"precipitation\", \"mswep\"),\n        (\"precipitation\", \"chirps\")\n    ]\n)\n\n# Use specific meteorological forcing\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"2000-01-01\", \"2010-12-31\"],\n    var_lst=[\n        \"streamflow\",\n        (\"precipitation\", \"era5land\"),\n        (\"temperature_mean\", \"era5land\")\n    ]\n)\n</code></pre>"},{"location":"api/grdc_caravan/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"2020-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\", \"pet\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/grdc_caravan/#data-quality","title":"Data Quality","text":"<p>Caravan datasets undergo quality control: - Removal of unrealistic values - Gap filling documentation - Metadata completeness checks - Cross-validation with regional datasets</p>"},{"location":"api/grdc_caravan/#api-reference","title":"API Reference","text":""},{"location":"api/grdc_caravan/#hydrodataset.grdc_caravan.GrdcCaravan","title":"<code>hydrodataset.grdc_caravan.GrdcCaravan</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>GRDC-Caravan dataset class extending HydroDataset.</p> <p>This class provides access to the GRDC-Caravan dataset, which contains hydrological and meteorological data for watersheds globally.</p> <p>This class uses a custom data reading implementation to support a newer dataset version than the one supported by the underlying aquafetch library. It overrides the download URLs and provides its own parsing and caching logic.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> Source code in <code>hydrodataset/grdc_caravan.py</code> <pre><code>class GrdcCaravan(HydroDataset):\n    \"\"\"GRDC-Caravan dataset class extending HydroDataset.\n\n    This class provides access to the GRDC-Caravan dataset, which contains\n    hydrological and meteorological data for watersheds globally.\n\n    This class uses a custom data reading implementation to support a newer\n    dataset version than the one supported by the underlying aquafetch library.\n    It overrides the download URLs and provides its own parsing and caching logic.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        region: Optional[str] = None,\n        download: bool = False,\n        cache_path: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize GRDC-Caravan dataset.\n\n        Args:\n            data_path: Path to the GRDC-Caravan data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n            cache_path: Path to the cache directory\n        \"\"\"\n        super().__init__(data_path, cache_path=cache_path)\n        self.region = region\n        self.download = download\n\n        # Instantiate the custom class defined at module level\n        self.aqua_fetch = GRDCCaravan(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"grdc_caravan_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"grdc_caravan_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1950-01-02\", \"2023-05-18\"]\n\n    # get the information of features from grdc-caravan_data_description.pdf\n    # Static variable definitions based on inspected data\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    # Dynamic variable mapping based on inspected data\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"mm\",\n            \"sources\": {\n                \"mm\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/day\"},\n                \"cms\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airtemp_c_2m_max\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\n                    \"specific_name\": \"dewpoint_temperature_2m_max\",\n                    \"unit\": \"\u00b0C\",\n                },\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airtemp_c_2m_min\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\n                    \"specific_name\": \"dewpoint_temperature_2m_min\",\n                    \"unit\": \"\u00b0C\",\n                },\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airtemp_c_mean_2m\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\n                    \"specific_name\": \"dewpoint_temperature_2m_mean\",\n                    \"unit\": \"\u00b0C\",\n                },\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"era5_land\",\n            \"sources\": {\n                \"era5_land\": {\n                    \"specific_name\": \"potential_evaporation_sum_era5_land\",\n                    \"unit\": \"mm/day\",\n                },\n                \"fao_pm\": {\n                    \"specific_name\": \"potential_evaporation_sum_fao_penman_monteith\",\n                    \"unit\": \"mm/day\",\n                },\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"snow_depth_water_equivalent_max\",\n                    \"unit\": \"m\",\n                },\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"snow_depth_water_equivalent_min\",\n                    \"unit\": \"m\",\n                },\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"snow_depth_water_equivalent_mean\",\n                    \"unit\": \"m\",\n                },\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_solar_radiation_max\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_solar_radiation_min\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_solar_radiation_mean\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.THERMAL_RADIATION_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_thermal_radiation_max\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.THERMAL_RADIATION_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_thermal_radiation_min\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.THERMAL_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_thermal_radiation_mean\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"surface_pressure_max\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"surface_pressure_min\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"surface_pressure_mean\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"u_component_of_wind_10m_max\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"u_component_of_wind_10m_min\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"u_component_of_wind_10m_mean\",\n                    \"unit\": \"m/s\",\n                },\n            },\n        },\n        StandardVariable.V_WIND_SPEED_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"v_component_of_wind_10m_max\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.V_WIND_SPEED_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"v_component_of_wind_10m_min\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"v_component_of_wind_10m_mean\",\n                    \"unit\": \"m/s\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_1_max\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_1_min\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_1_mean\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_2_max\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_2_min\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER2: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_2_mean\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_3_max\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_3_min\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER3: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_3_mean\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_4_max\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_4_min\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"volumetric_soil_water_layer_4_mean\",\n                    \"unit\": \"m^3/m^3\",\n                },\n            },\n        },\n    }\n</code></pre>"},{"location":"api/grdc_caravan/#hydrodataset.grdc_caravan.GrdcCaravan.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/grdc_caravan/#hydrodataset.grdc_caravan.GrdcCaravan.__init__","title":"<code>__init__(data_path, region=None, download=False, cache_path=None)</code>","text":"<p>Initialize GRDC-Caravan dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the GRDC-Caravan data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> <code>cache_path</code> <code>Optional[str]</code> <p>Path to the cache directory</p> <code>None</code> Source code in <code>hydrodataset/grdc_caravan.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    region: Optional[str] = None,\n    download: bool = False,\n    cache_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize GRDC-Caravan dataset.\n\n    Args:\n        data_path: Path to the GRDC-Caravan data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n        cache_path: Path to the cache directory\n    \"\"\"\n    super().__init__(data_path, cache_path=cache_path)\n    self.region = region\n    self.download = download\n\n    # Instantiate the custom class defined at module level\n    self.aqua_fetch = GRDCCaravan(data_path)\n</code></pre>"},{"location":"api/hydro_dataset/","title":"HydroDataset Base Class","text":""},{"location":"api/hydro_dataset/#overview","title":"Overview","text":"<p><code>HydroDataset</code> is the abstract base class that defines the unified interface for all hydrological dataset implementations in hydrodataset. It provides common functionality for data reading, caching, variable standardization, and unit management.</p>"},{"location":"api/hydro_dataset/#key-responsibilities","title":"Key Responsibilities","text":""},{"location":"api/hydro_dataset/#data-reading","title":"Data Reading","text":"<ul> <li>Read basin/station IDs</li> <li>Read timeseries data as xarray.Dataset</li> <li>Read attribute/static data as xarray.Dataset</li> <li>Support for selecting specific variables and time ranges</li> </ul>"},{"location":"api/hydro_dataset/#netcdf-caching","title":"NetCDF Caching","text":"<ul> <li>Generate and cache timeseries data in <code>.nc</code> format</li> <li>Generate and cache attribute data in <code>.nc</code> format</li> <li>Fast subsequent reads from cached files</li> <li>Cache location configured in <code>hydro_setting.yml</code></li> </ul>"},{"location":"api/hydro_dataset/#variable-standardization","title":"Variable Standardization","text":"<ul> <li>Map dataset-specific variable names to standard names</li> <li>Support multiple data sources for same variable</li> <li>Automatic unit conversion and tracking</li> </ul>"},{"location":"api/hydro_dataset/#feature-management","title":"Feature Management","text":"<ul> <li>List available static features (attributes)</li> <li>List available dynamic features (timeseries)</li> <li>Clean feature names for consistency</li> </ul>"},{"location":"api/hydro_dataset/#architecture","title":"Architecture","text":""},{"location":"api/hydro_dataset/#required-properties-subclass-implementation","title":"Required Properties (Subclass Implementation)","text":"<p>Subclasses must implement these properties:</p> <ul> <li><code>_attributes_cache_filename</code>: NetCDF filename for cached attributes (e.g., \"camels_us_attributes.nc\")</li> <li><code>_timeseries_cache_filename</code>: NetCDF filename for cached timeseries (e.g., \"camels_us_timeseries.nc\")</li> <li><code>default_t_range</code>: Default time range as <code>[\"YYYY-MM-DD\", \"YYYY-MM-DD\"]</code></li> </ul>"},{"location":"api/hydro_dataset/#optional-properties-subclass-override","title":"Optional Properties (Subclass Override)","text":"<ul> <li><code>_subclass_static_definitions</code>: Dictionary mapping standard static variable names to dataset-specific names and units</li> <li><code>_dynamic_variable_mapping</code>: Dictionary mapping StandardVariable constants to dataset-specific timeseries variables</li> </ul>"},{"location":"api/hydro_dataset/#usage-pattern","title":"Usage Pattern","text":""},{"location":"api/hydro_dataset/#creating-a-new-dataset-class","title":"Creating a New Dataset Class","text":"<pre><code>from hydrodataset import HydroDataset, StandardVariable\nfrom aqua_fetch import DatasetClass\n\nclass MyDataset(HydroDataset):\n    def __init__(self, data_path, region=None, download=False):\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = DatasetClass(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"mydataset_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"mydataset_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1980-01-01\", \"2020-12-31\"]\n\n    # Define static variable mappings\n    _subclass_static_definitions = {\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n    }\n\n    # Define dynamic variable mappings\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"q_cms\", \"unit\": \"m^3/s\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"gauge\",\n            \"sources\": {\n                \"gauge\": {\"specific_name\": \"precip_mm\", \"unit\": \"mm/day\"},\n                \"era5\": {\"specific_name\": \"tp\", \"unit\": \"mm/day\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/hydro_dataset/#using-a-dataset-instance","title":"Using a Dataset Instance","text":"<pre><code>from hydrodataset.camels_us import CamelsUs\nfrom hydrodataset import SETTING\n\n# Initialize\nds = CamelsUs(SETTING[\"local_data_path\"][\"datasets-origin\"])\n\n# Get available features\nprint(ds.available_static_features)\nprint(ds.available_dynamic_features)\n\n# Read data\nbasin_ids = ds.read_object_ids()\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\n\n# Access convenience methods\nareas = ds.read_area(gage_id_lst=basin_ids[:5])\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:5])\n</code></pre>"},{"location":"api/hydro_dataset/#api-reference","title":"API Reference","text":""},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset","title":"<code>hydrodataset.hydro_dataset.HydroDataset</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An interface for Hydrological Dataset</p> <p>For unit, we use Pint package's unit system -- unit registry</p>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset--parameters","title":"Parameters","text":"<p>ABC : type description</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>class HydroDataset(ABC):\n    \"\"\"An interface for Hydrological Dataset\n\n    For unit, we use Pint package's unit system -- unit registry\n\n    Parameters\n    ----------\n    ABC : _type_\n        _description_\n    \"\"\"\n\n    # A unified definition for static variables, including name mapping and units\n    _base_static_definitions = {\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n    }\n    # variable name map for timeseries\n    _dynamic_variable_mapping = {}\n\n    def __init__(self, data_path, cache_path=None):\n        self.data_source_dir = Path(ROOT_DIR, data_path)\n        if not self.data_source_dir.is_dir():\n            self.data_source_dir.mkdir(parents=True)\n        if cache_path is None:\n            self.cache_dir = Path(CACHE_DIR)\n        else:\n            self.cache_dir = Path(cache_path)\n        if not self.cache_dir.is_dir():\n            self.cache_dir.mkdir(parents=True)\n\n        # Merge static variable definitions\n        self._static_variable_definitions = self._base_static_definitions.copy()\n        if hasattr(self.__class__, \"_subclass_static_definitions\"):\n            self._static_variable_definitions.update(self._subclass_static_definitions)\n\n    def get_name(self):\n        raise NotImplementedError\n\n    def set_data_source_describe(self):\n        raise NotImplementedError\n\n    def download_data_source(self):\n        raise NotImplementedError\n\n    def is_data_ready(self):\n        raise NotImplementedError\n\n    def read_object_ids(self) -&gt; np.ndarray:\n        \"\"\"Read watershed station ID list.\"\"\"\n        if hasattr(self, \"aqua_fetch\"):\n            stations_list = self.aqua_fetch.stations()\n            return np.sort(np.array(stations_list))\n        raise NotImplementedError\n\n    def read_target_cols(\n        self, gage_id_lst=None, t_range=None, target_cols=None, **kwargs\n    ) -&gt; np.ndarray:\n        raise NotImplementedError\n\n    def read_relevant_cols(\n        self, gage_id_lst=None, t_range=None, var_lst=None, forcing_type=None, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"3d data (site_num * time_length * var_num), time-series data\"\"\"\n        raise NotImplementedError\n\n    def read_constant_cols(\n        self, gage_id_lst=None, var_lst=None, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"2d data (site_num * var_num), non-time-series data\"\"\"\n        raise NotImplementedError\n\n    def read_other_cols(\n        self, object_ids=None, other_cols: dict = None, **kwargs\n    ) -&gt; dict:\n        \"\"\"some data which cannot be easily treated as constant vars or time-series with same length as relevant vars\n        CONVENTION: other_cols is a dict, where each item is also a dict with all params in it\n        \"\"\"\n        raise NotImplementedError\n\n    def get_constant_cols(self) -&gt; np.ndarray:\n        \"\"\"the constant cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def get_relevant_cols(self) -&gt; np.ndarray:\n        \"\"\"the relevant cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def get_target_cols(self) -&gt; np.ndarray:\n        \"\"\"the target cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def get_other_cols(self) -&gt; dict:\n        \"\"\"the other cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def _dynamic_features(self) -&gt; list:\n        \"\"\"the dynamic features in this data_source\"\"\"\n        if hasattr(self, \"aqua_fetch\"):\n            original_features = self.aqua_fetch.dynamic_features\n            return self._clean_feature_names(original_features)\n        raise NotImplementedError\n\n    @staticmethod\n    def _clean_feature_names(feature_names):\n        \"\"\"Clean feature names to be compatible with NetCDF format and our internal standard.\n\n        The cleaning process follows these steps:\n        1. Remove units in parentheses (along with any preceding whitespace)\n           e.g., 'Prcp(mm/day)' -&gt; 'Prcp' or 'Temp (\u00b0C)' -&gt; 'Temp'\n        2. Convert all characters to lowercase\n           e.g., 'Prcp' -&gt; 'prcp'\n        3. Remove any remaining invalid characters (only keep a-z, 0-9, and _)\n           This ensures NetCDF variable naming compliance\n\n        Args:\n            feature_names (list or pd.Index): Original feature names that may contain\n                units and special characters\n\n        Returns:\n            list: Cleaned feature names with only lowercase letters, numbers, and underscores\n\n        Examples:\n            &gt;&gt;&gt; _clean_feature_names(['Prcp(mm/day)_daymet', 'Temp (\u00b0C)'])\n            ['prcp_daymet', 'temp']\n        \"\"\"\n        if not isinstance(feature_names, pd.Index):\n            feature_names = pd.Index(feature_names)\n\n        # Remove units in parentheses, then convert to lowercase\n        cleaned_names = feature_names.str.replace(\n            r\"\\s*\\([^)]*\\)\", \"\", regex=True\n        ).str.lower()\n        # Replace any remaining invalid characters\n        cleaned_names = cleaned_names.str.replace(r\"\"\"[^a-z0-9_]\"\"\", \"\", regex=True)\n        return cleaned_names.tolist()\n\n    def _static_features(self) -&gt; list:\n        \"\"\"the static features in this data_source\"\"\"\n        if hasattr(self, \"aqua_fetch\"):\n            original_features = self.aqua_fetch.static_features\n            return self._clean_feature_names(original_features)\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def _attributes_cache_filename(self):\n        pass\n\n    @property\n    @abstractmethod\n    def _timeseries_cache_filename(self):\n        pass\n\n    @property\n    @abstractmethod\n    def default_t_range(self):\n        pass\n\n    def cache_timeseries_xrdataset(self):\n        if hasattr(self, \"aqua_fetch\"):\n            # Build a lookup map from specific name to unit\n            unit_lookup = {}\n            if hasattr(self, \"_dynamic_variable_mapping\"):\n                for (\n                    std_name,\n                    mapping_info,\n                ) in self._dynamic_variable_mapping.items():\n                    for source, source_info in mapping_info[\"sources\"].items():\n                        unit_lookup[source_info[\"specific_name\"]] = source_info[\"unit\"]\n\n            gage_id_lst = self.read_object_ids().tolist()\n            original_var_lst = self.aqua_fetch.dynamic_features\n            cleaned_var_lst = self._clean_feature_names(original_var_lst)\n            # Create a mapping from original variable names to cleaned names\n            # to ensure correct correspondence even if list order changes\n            var_name_mapping = dict(zip(original_var_lst, cleaned_var_lst))\n\n            batch_data = self.aqua_fetch.fetch_stations_features(\n                stations=gage_id_lst,\n                dynamic_features=original_var_lst,\n                static_features=None,\n                st=self.default_t_range[0],\n                en=self.default_t_range[1],\n                as_dataframe=False,\n            )\n\n            dynamic_data = (\n                batch_data[1] if isinstance(batch_data, tuple) else batch_data\n            )\n\n            new_data_vars = {}\n            time_coord = dynamic_data.coords[\"time\"]\n\n            # Process only the variables that exist in the data source\n            # Subclasses can add additional variables in their override methods\n            for original_var in tqdm(\n                original_var_lst,\n                desc=\"Processing variables\",\n                total=len(original_var_lst),\n            ):\n                cleaned_var = var_name_mapping[original_var]\n                var_data = []\n                for station in gage_id_lst:\n                    if station in dynamic_data.data_vars:\n                        station_data = dynamic_data[station].sel(\n                            dynamic_features=original_var\n                        )\n                        if \"dynamic_features\" in station_data.coords:\n                            station_data = station_data.drop(\"dynamic_features\")\n                        var_data.append(station_data)\n\n                if var_data:\n                    combined = xr.concat(var_data, dim=\"basin\")\n                    combined[\"basin\"] = gage_id_lst\n                    combined.attrs[\"units\"] = unit_lookup.get(cleaned_var, \"unknown\")\n                    new_data_vars[cleaned_var] = combined\n\n            new_ds = xr.Dataset(\n                data_vars=new_data_vars,\n                coords={\n                    \"basin\": gage_id_lst,\n                    \"time\": time_coord,\n                },\n            )\n\n            batch_filepath = self.cache_dir.joinpath(self._timeseries_cache_filename)\n            batch_filepath.parent.mkdir(parents=True, exist_ok=True)\n            new_ds.to_netcdf(batch_filepath)\n            print(f\"\u6210\u529f\u4fdd\u5b58\u5230: {batch_filepath}\")\n        else:\n            raise NotImplementedError\n\n    def _assign_units_to_dataset(self, ds, units_map):\n        def get_unit_by_prefix(var_name):\n            for prefix, unit in units_map.items():\n                if var_name.startswith(prefix):\n                    return unit\n            return None\n\n        def get_unit(var_name):\n            prefix_unit = get_unit_by_prefix(var_name)\n            if prefix_unit:\n                return prefix_unit\n            return \"undefined\"\n\n        for var in ds.data_vars:\n            unit = get_unit(var)\n            ds[var].attrs[\"units\"] = unit\n            if unit == \"class\":\n                ds[var].attrs[\"description\"] = \"Classification code\"\n        return ds\n\n        return ds\n\n    def _get_attribute_units(self) -&gt; dict:\n        \"\"\"Builds a unit dictionary from the static variable definitions.\"\"\"\n        return {\n            info[\"specific_name\"]: info[\"unit\"]\n            for std_name, info in self._static_variable_definitions.items()\n        }\n\n    def cache_attributes_xrdataset(self):\n        if hasattr(self, \"aqua_fetch\"):\n            df_attr = self.aqua_fetch.fetch_static_features()\n            print(df_attr.columns)\n            # Clean column names using the unified method\n            df_attr.columns = self._clean_feature_names(df_attr.columns)\n            # Remove duplicate columns if any (keep first occurrence)\n            if df_attr.columns.duplicated().any():\n                df_attr = df_attr.loc[:, ~df_attr.columns.duplicated()]\n            # Ensure index is string type for basin IDs\n            df_attr.index = df_attr.index.astype(str)\n            ds_attr = df_attr.to_xarray()\n            # Check if the coordinate is named 'basin', if not rename it\n            coord_names = list(ds_attr.dims.keys())\n            if len(coord_names) &gt; 0 and coord_names[0] != \"basin\":\n                ds_attr = ds_attr.rename({coord_names[0]: \"basin\"})\n            units_map = self._get_attribute_units()\n            ds_attr = self._assign_units_to_dataset(ds_attr, units_map)\n            ds_attr.to_netcdf(self.cache_dir.joinpath(self._attributes_cache_filename))\n        else:\n            raise NotImplementedError\n\n    def read_attr_xrdataset(\n        self,\n        gage_id_lst: list = None,\n        var_lst: list = None,\n        to_numeric: bool = True,\n        **kwargs,\n    ) -&gt; xr.Dataset:\n        \"\"\"Reads attribute data for a list of basins using standardized variable names.\n\n        Args:\n            gage_id_lst: A list of basin identifiers.\n            var_lst: A list of **standard** attribute names to retrieve.\n                If None, nothing will be returned.\n            to_numeric: If True, converts all non-numeric variables to numeric codes\n                and stores the original labels in the variable's attributes.\n                Defaults to True.\n\n        Returns:\n            An xarray Dataset containing the attribute data for the requested basins,\n            with variables named using the standard names.\n        \"\"\"\n        if not var_lst:\n            return None\n\n        # 1. Translate standard names to dataset-specific names\n        target_vars_to_fetch = []\n        rename_map = {}\n        for std_name in var_lst:\n            if std_name not in self._static_variable_definitions:\n                raise ValueError(\n                    f\"'{std_name}' is not a recognized standard static variable.\"\n                )\n            actual_var_name = self._static_variable_definitions[std_name][\n                \"specific_name\"\n            ]\n            target_vars_to_fetch.append(actual_var_name)\n            rename_map[actual_var_name] = std_name\n\n        # 2. Read data from cache using actual variable names\n        attr_cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n        try:\n            attr_ds = xr.open_dataset(attr_cache_file)\n        except FileNotFoundError:\n            self.cache_attributes_xrdataset()\n            attr_ds = xr.open_dataset(attr_cache_file)\n\n        # 3. Select variables and basins\n        ds_subset = attr_ds[target_vars_to_fetch]\n        if gage_id_lst is not None:\n            gage_id_lst = [str(gid) for gid in gage_id_lst]\n            ds_selected = ds_subset.sel(basin=gage_id_lst)\n        else:\n            ds_selected = ds_subset\n\n        # 4. Rename to standard names\n        final_ds = ds_selected.rename(rename_map)\n\n        if not to_numeric:\n            return final_ds\n\n        # 5. If to_numeric is True, perform conversion\n        converted_ds = xr.Dataset(coords=final_ds.coords)\n        for var_name, da in final_ds.data_vars.items():\n            if np.issubdtype(da.dtype, np.number):\n                converted_ds[var_name] = da\n            else:\n                # Assumes string-like array that needs factorizing\n                numeric_vals, labels = pd.factorize(da.values, sort=True)\n                new_da = xr.DataArray(\n                    numeric_vals,\n                    coords=da.coords,\n                    dims=da.dims,\n                    name=da.name,\n                    attrs=da.attrs,  # Preserve original attributes\n                )\n                new_da.attrs[\"labels\"] = labels.tolist()\n                converted_ds[var_name] = new_da\n        return converted_ds\n\n    def _load_ts_dataset(self, **kwargs):\n        \"\"\"\n        Loads the time series dataset from cache.\n\n        This method can be overridden by subclasses to implement different loading\n        strategies (e.g., loading multiple files).\n\n        Args:\n            **kwargs: Additional keyword arguments for loading.\n\n        Returns:\n            xarray.Dataset: The loaded time series dataset.\n        \"\"\"\n        ts_cache_file = self.cache_dir.joinpath(self._timeseries_cache_filename)\n\n        if not os.path.isfile(ts_cache_file):\n            self.cache_timeseries_xrdataset()\n\n        return xr.open_dataset(ts_cache_file)\n\n    def read_ts_xrdataset(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        sources: dict = None,\n        **kwargs,\n    ):\n        if (\n            not hasattr(self, \"_dynamic_variable_mapping\")\n            or not self._dynamic_variable_mapping\n        ):\n            raise NotImplementedError(\n                \"This dataset does not support the standardized variable mapping.\"\n            )\n\n        if var_lst is None:\n            var_lst = list(self._dynamic_variable_mapping.keys())\n\n        if t_range is None:\n            t_range = self.default_t_range\n\n        target_vars_to_fetch = []\n        rename_map = {}\n\n        for std_name in var_lst:\n            if std_name not in self._dynamic_variable_mapping:\n                raise ValueError(\n                    f\"'{std_name}' is not a recognized standard variable for this dataset.\"\n                )\n\n            mapping_info = self._dynamic_variable_mapping[std_name]\n\n            # Determine which source(s) to use and if they were explicitly requested\n            is_explicit_source = sources and std_name in sources\n            sources_to_use = []\n            if is_explicit_source:\n                provided_sources = sources[std_name]\n                if isinstance(provided_sources, list):\n                    sources_to_use.extend(provided_sources)\n                else:\n                    sources_to_use.append(provided_sources)\n            else:\n                sources_to_use.append(mapping_info[\"default_source\"])\n\n            # A suffix is only needed if the user explicitly requested multiple sources\n            needs_suffix = is_explicit_source and len(sources_to_use) &gt; 1\n            for source in sources_to_use:\n                if source not in mapping_info[\"sources\"]:\n                    raise ValueError(\n                        f\"Source '{source}' is not available for variable '{std_name}'.\"\n                    )\n\n                actual_var_name = mapping_info[\"sources\"][source][\"specific_name\"]\n                target_vars_to_fetch.append(actual_var_name)\n                output_name = f\"{std_name}_{source}\" if needs_suffix else std_name\n                rename_map[actual_var_name] = output_name\n\n        # Read data from cache using actual variable names\n        ts = self._load_ts_dataset(**kwargs)\n        missing_vars = [v for v in target_vars_to_fetch if v not in ts.data_vars]\n        if missing_vars:\n            # To provide a better error message, map back to standard names\n            reverse_rename_map = {v: k for k, v in rename_map.items()}\n            missing_std_vars = [reverse_rename_map.get(v, v) for v in missing_vars]\n            raise ValueError(\n                f\"The following variables are missing from the cache file: {missing_std_vars}\"\n            )\n\n        ds_subset = ts[target_vars_to_fetch]\n        ds_selected = ds_subset.sel(\n            basin=gage_id_lst, time=slice(t_range[0], t_range[1])\n        )\n        final_ds = ds_selected.rename(rename_map)\n        return final_ds\n\n    def get_available_dynamic_features(self) -&gt; dict:\n        \"\"\"\n        Returns a dictionary of available standard dynamic feature names\n        and their possible sources.\n        \"\"\"\n        if (\n            not hasattr(self, \"_dynamic_variable_mapping\")\n            or not self._dynamic_variable_mapping\n        ):\n            return {}\n\n        feature_info = {}\n        for std_name, mapping_info in self._dynamic_variable_mapping.items():\n            feature_info[std_name] = {\n                \"default_source\": mapping_info.get(\"default_source\"),\n                \"available_sources\": list(mapping_info.get(\"sources\", {}).keys()),\n            }\n        return feature_info\n\n    def get_available_static_features(self) -&gt; list:\n        \"\"\"Returns a list of available standard static feature names.\"\"\"\n        return list(self._static_variable_definitions.keys())\n\n    @property\n    def available_static_features(self) -&gt; list:\n        \"\"\"Returns a list of available static attribute names.\"\"\"\n        return self.get_available_static_features()\n\n    @property\n    def available_dynamic_features(self) -&gt; dict:\n        \"\"\"Returns a dictionary of available dynamic feature names and their possible sources.\"\"\"\n        return self.get_available_dynamic_features()\n\n    def read_area(self, gage_id_lst: list[str]) -&gt; xr.Dataset:\n        \"\"\"Reads the catchment area for a list of basins.\n\n        Args:\n            gage_id_lst: A list of basin identifiers for which to retrieve the area.\n\n        Returns:\n            An xarray Dataset containing the area data for the requested basins.\n        \"\"\"\n        data_ds = self.read_attr_xrdataset(gage_id_lst=gage_id_lst, var_lst=[\"area\"])\n        return data_ds\n\n    def read_mean_prcp(self, gage_id_lst: list[str], unit: str = \"mm/d\") -&gt; xr.Dataset:\n        \"\"\"Reads the mean daily precipitation for a list of basins, with unit conversion.\n\n        Args:\n            gage_id_lst: A list of basin identifiers.\n            unit: The desired unit for the output precipitation. Defaults to \"mm/d\".\n                Supported units: ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h',\n                'mm/3hour', 'mm/8d', 'mm/8day'].\n\n        Returns:\n            An xarray Dataset containing the mean precipitation data in the specified units.\n\n        Raises:\n            ValueError: If an unsupported unit is provided.\n        \"\"\"\n        prcp_var_name = \"p_mean\"\n        data_ds = self.read_attr_xrdataset(\n            gage_id_lst=gage_id_lst, var_lst=[prcp_var_name]\n        )\n        # No conversion needed\n        if unit in [\"mm/d\", \"mm/day\"]:\n            return data_ds\n\n        # Conversion needed, create a new dataset\n        converted_ds = data_ds.copy()\n        # After renaming, the variable in the dataset is now the standard name\n        if unit in [\"mm/h\", \"mm/hour\"]:\n            converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 24\n        elif unit in [\"mm/3h\", \"mm/3hour\"]:\n            converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 8\n        elif unit in [\"mm/8d\", \"mm/8day\"]:\n            converted_ds[prcp_var_name] = data_ds[prcp_var_name] * 8\n        else:\n            raise ValueError(\n                \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n            )\n        return converted_ds\n</code></pre>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.available_static_features","title":"<code>available_static_features</code>  <code>property</code>","text":"<p>Returns a list of available static attribute names.</p>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.available_dynamic_features","title":"<code>available_dynamic_features</code>  <code>property</code>","text":"<p>Returns a dictionary of available dynamic feature names and their possible sources.</p>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.default_t_range","title":"<code>default_t_range</code>  <code>abstractmethod</code> <code>property</code>","text":""},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.__init__","title":"<code>__init__(data_path, cache_path=None)</code>","text":"Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def __init__(self, data_path, cache_path=None):\n    self.data_source_dir = Path(ROOT_DIR, data_path)\n    if not self.data_source_dir.is_dir():\n        self.data_source_dir.mkdir(parents=True)\n    if cache_path is None:\n        self.cache_dir = Path(CACHE_DIR)\n    else:\n        self.cache_dir = Path(cache_path)\n    if not self.cache_dir.is_dir():\n        self.cache_dir.mkdir(parents=True)\n\n    # Merge static variable definitions\n    self._static_variable_definitions = self._base_static_definitions.copy()\n    if hasattr(self.__class__, \"_subclass_static_definitions\"):\n        self._static_variable_definitions.update(self._subclass_static_definitions)\n</code></pre>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.read_object_ids","title":"<code>read_object_ids()</code>","text":"<p>Read watershed station ID list.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_object_ids(self) -&gt; np.ndarray:\n    \"\"\"Read watershed station ID list.\"\"\"\n    if hasattr(self, \"aqua_fetch\"):\n        stations_list = self.aqua_fetch.stations()\n        return np.sort(np.array(stations_list))\n    raise NotImplementedError\n</code></pre>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.read_ts_xrdataset","title":"<code>read_ts_xrdataset(gage_id_lst=None, t_range=None, var_lst=None, sources=None, **kwargs)</code>","text":"Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_ts_xrdataset(\n    self,\n    gage_id_lst: list = None,\n    t_range: list = None,\n    var_lst: list = None,\n    sources: dict = None,\n    **kwargs,\n):\n    if (\n        not hasattr(self, \"_dynamic_variable_mapping\")\n        or not self._dynamic_variable_mapping\n    ):\n        raise NotImplementedError(\n            \"This dataset does not support the standardized variable mapping.\"\n        )\n\n    if var_lst is None:\n        var_lst = list(self._dynamic_variable_mapping.keys())\n\n    if t_range is None:\n        t_range = self.default_t_range\n\n    target_vars_to_fetch = []\n    rename_map = {}\n\n    for std_name in var_lst:\n        if std_name not in self._dynamic_variable_mapping:\n            raise ValueError(\n                f\"'{std_name}' is not a recognized standard variable for this dataset.\"\n            )\n\n        mapping_info = self._dynamic_variable_mapping[std_name]\n\n        # Determine which source(s) to use and if they were explicitly requested\n        is_explicit_source = sources and std_name in sources\n        sources_to_use = []\n        if is_explicit_source:\n            provided_sources = sources[std_name]\n            if isinstance(provided_sources, list):\n                sources_to_use.extend(provided_sources)\n            else:\n                sources_to_use.append(provided_sources)\n        else:\n            sources_to_use.append(mapping_info[\"default_source\"])\n\n        # A suffix is only needed if the user explicitly requested multiple sources\n        needs_suffix = is_explicit_source and len(sources_to_use) &gt; 1\n        for source in sources_to_use:\n            if source not in mapping_info[\"sources\"]:\n                raise ValueError(\n                    f\"Source '{source}' is not available for variable '{std_name}'.\"\n                )\n\n            actual_var_name = mapping_info[\"sources\"][source][\"specific_name\"]\n            target_vars_to_fetch.append(actual_var_name)\n            output_name = f\"{std_name}_{source}\" if needs_suffix else std_name\n            rename_map[actual_var_name] = output_name\n\n    # Read data from cache using actual variable names\n    ts = self._load_ts_dataset(**kwargs)\n    missing_vars = [v for v in target_vars_to_fetch if v not in ts.data_vars]\n    if missing_vars:\n        # To provide a better error message, map back to standard names\n        reverse_rename_map = {v: k for k, v in rename_map.items()}\n        missing_std_vars = [reverse_rename_map.get(v, v) for v in missing_vars]\n        raise ValueError(\n            f\"The following variables are missing from the cache file: {missing_std_vars}\"\n        )\n\n    ds_subset = ts[target_vars_to_fetch]\n    ds_selected = ds_subset.sel(\n        basin=gage_id_lst, time=slice(t_range[0], t_range[1])\n    )\n    final_ds = ds_selected.rename(rename_map)\n    return final_ds\n</code></pre>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.read_attr_xrdataset","title":"<code>read_attr_xrdataset(gage_id_lst=None, var_lst=None, to_numeric=True, **kwargs)</code>","text":"<p>Reads attribute data for a list of basins using standardized variable names.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list</code> <p>A list of basin identifiers.</p> <code>None</code> <code>var_lst</code> <code>list</code> <p>A list of standard attribute names to retrieve. If None, nothing will be returned.</p> <code>None</code> <code>to_numeric</code> <code>bool</code> <p>If True, converts all non-numeric variables to numeric codes and stores the original labels in the variable's attributes. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the attribute data for the requested basins,</p> <code>Dataset</code> <p>with variables named using the standard names.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_attr_xrdataset(\n    self,\n    gage_id_lst: list = None,\n    var_lst: list = None,\n    to_numeric: bool = True,\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"Reads attribute data for a list of basins using standardized variable names.\n\n    Args:\n        gage_id_lst: A list of basin identifiers.\n        var_lst: A list of **standard** attribute names to retrieve.\n            If None, nothing will be returned.\n        to_numeric: If True, converts all non-numeric variables to numeric codes\n            and stores the original labels in the variable's attributes.\n            Defaults to True.\n\n    Returns:\n        An xarray Dataset containing the attribute data for the requested basins,\n        with variables named using the standard names.\n    \"\"\"\n    if not var_lst:\n        return None\n\n    # 1. Translate standard names to dataset-specific names\n    target_vars_to_fetch = []\n    rename_map = {}\n    for std_name in var_lst:\n        if std_name not in self._static_variable_definitions:\n            raise ValueError(\n                f\"'{std_name}' is not a recognized standard static variable.\"\n            )\n        actual_var_name = self._static_variable_definitions[std_name][\n            \"specific_name\"\n        ]\n        target_vars_to_fetch.append(actual_var_name)\n        rename_map[actual_var_name] = std_name\n\n    # 2. Read data from cache using actual variable names\n    attr_cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n    try:\n        attr_ds = xr.open_dataset(attr_cache_file)\n    except FileNotFoundError:\n        self.cache_attributes_xrdataset()\n        attr_ds = xr.open_dataset(attr_cache_file)\n\n    # 3. Select variables and basins\n    ds_subset = attr_ds[target_vars_to_fetch]\n    if gage_id_lst is not None:\n        gage_id_lst = [str(gid) for gid in gage_id_lst]\n        ds_selected = ds_subset.sel(basin=gage_id_lst)\n    else:\n        ds_selected = ds_subset\n\n    # 4. Rename to standard names\n    final_ds = ds_selected.rename(rename_map)\n\n    if not to_numeric:\n        return final_ds\n\n    # 5. If to_numeric is True, perform conversion\n    converted_ds = xr.Dataset(coords=final_ds.coords)\n    for var_name, da in final_ds.data_vars.items():\n        if np.issubdtype(da.dtype, np.number):\n            converted_ds[var_name] = da\n        else:\n            # Assumes string-like array that needs factorizing\n            numeric_vals, labels = pd.factorize(da.values, sort=True)\n            new_da = xr.DataArray(\n                numeric_vals,\n                coords=da.coords,\n                dims=da.dims,\n                name=da.name,\n                attrs=da.attrs,  # Preserve original attributes\n            )\n            new_da.attrs[\"labels\"] = labels.tolist()\n            converted_ds[var_name] = new_da\n    return converted_ds\n</code></pre>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.cache_timeseries_xrdataset","title":"<code>cache_timeseries_xrdataset()</code>","text":"Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def cache_timeseries_xrdataset(self):\n    if hasattr(self, \"aqua_fetch\"):\n        # Build a lookup map from specific name to unit\n        unit_lookup = {}\n        if hasattr(self, \"_dynamic_variable_mapping\"):\n            for (\n                std_name,\n                mapping_info,\n            ) in self._dynamic_variable_mapping.items():\n                for source, source_info in mapping_info[\"sources\"].items():\n                    unit_lookup[source_info[\"specific_name\"]] = source_info[\"unit\"]\n\n        gage_id_lst = self.read_object_ids().tolist()\n        original_var_lst = self.aqua_fetch.dynamic_features\n        cleaned_var_lst = self._clean_feature_names(original_var_lst)\n        # Create a mapping from original variable names to cleaned names\n        # to ensure correct correspondence even if list order changes\n        var_name_mapping = dict(zip(original_var_lst, cleaned_var_lst))\n\n        batch_data = self.aqua_fetch.fetch_stations_features(\n            stations=gage_id_lst,\n            dynamic_features=original_var_lst,\n            static_features=None,\n            st=self.default_t_range[0],\n            en=self.default_t_range[1],\n            as_dataframe=False,\n        )\n\n        dynamic_data = (\n            batch_data[1] if isinstance(batch_data, tuple) else batch_data\n        )\n\n        new_data_vars = {}\n        time_coord = dynamic_data.coords[\"time\"]\n\n        # Process only the variables that exist in the data source\n        # Subclasses can add additional variables in their override methods\n        for original_var in tqdm(\n            original_var_lst,\n            desc=\"Processing variables\",\n            total=len(original_var_lst),\n        ):\n            cleaned_var = var_name_mapping[original_var]\n            var_data = []\n            for station in gage_id_lst:\n                if station in dynamic_data.data_vars:\n                    station_data = dynamic_data[station].sel(\n                        dynamic_features=original_var\n                    )\n                    if \"dynamic_features\" in station_data.coords:\n                        station_data = station_data.drop(\"dynamic_features\")\n                    var_data.append(station_data)\n\n            if var_data:\n                combined = xr.concat(var_data, dim=\"basin\")\n                combined[\"basin\"] = gage_id_lst\n                combined.attrs[\"units\"] = unit_lookup.get(cleaned_var, \"unknown\")\n                new_data_vars[cleaned_var] = combined\n\n        new_ds = xr.Dataset(\n            data_vars=new_data_vars,\n            coords={\n                \"basin\": gage_id_lst,\n                \"time\": time_coord,\n            },\n        )\n\n        batch_filepath = self.cache_dir.joinpath(self._timeseries_cache_filename)\n        batch_filepath.parent.mkdir(parents=True, exist_ok=True)\n        new_ds.to_netcdf(batch_filepath)\n        print(f\"\u6210\u529f\u4fdd\u5b58\u5230: {batch_filepath}\")\n    else:\n        raise NotImplementedError\n</code></pre>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.cache_attributes_xrdataset","title":"<code>cache_attributes_xrdataset()</code>","text":"Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def cache_attributes_xrdataset(self):\n    if hasattr(self, \"aqua_fetch\"):\n        df_attr = self.aqua_fetch.fetch_static_features()\n        print(df_attr.columns)\n        # Clean column names using the unified method\n        df_attr.columns = self._clean_feature_names(df_attr.columns)\n        # Remove duplicate columns if any (keep first occurrence)\n        if df_attr.columns.duplicated().any():\n            df_attr = df_attr.loc[:, ~df_attr.columns.duplicated()]\n        # Ensure index is string type for basin IDs\n        df_attr.index = df_attr.index.astype(str)\n        ds_attr = df_attr.to_xarray()\n        # Check if the coordinate is named 'basin', if not rename it\n        coord_names = list(ds_attr.dims.keys())\n        if len(coord_names) &gt; 0 and coord_names[0] != \"basin\":\n            ds_attr = ds_attr.rename({coord_names[0]: \"basin\"})\n        units_map = self._get_attribute_units()\n        ds_attr = self._assign_units_to_dataset(ds_attr, units_map)\n        ds_attr.to_netcdf(self.cache_dir.joinpath(self._attributes_cache_filename))\n    else:\n        raise NotImplementedError\n</code></pre>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.read_area","title":"<code>read_area(gage_id_lst)</code>","text":"<p>Reads the catchment area for a list of basins.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list[str]</code> <p>A list of basin identifiers for which to retrieve the area.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the area data for the requested basins.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_area(self, gage_id_lst: list[str]) -&gt; xr.Dataset:\n    \"\"\"Reads the catchment area for a list of basins.\n\n    Args:\n        gage_id_lst: A list of basin identifiers for which to retrieve the area.\n\n    Returns:\n        An xarray Dataset containing the area data for the requested basins.\n    \"\"\"\n    data_ds = self.read_attr_xrdataset(gage_id_lst=gage_id_lst, var_lst=[\"area\"])\n    return data_ds\n</code></pre>"},{"location":"api/hydro_dataset/#hydrodataset.hydro_dataset.HydroDataset.read_mean_prcp","title":"<code>read_mean_prcp(gage_id_lst, unit='mm/d')</code>","text":"<p>Reads the mean daily precipitation for a list of basins, with unit conversion.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list[str]</code> <p>A list of basin identifiers.</p> required <code>unit</code> <code>str</code> <p>The desired unit for the output precipitation. Defaults to \"mm/d\". Supported units: ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day'].</p> <code>'mm/d'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the mean precipitation data in the specified units.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported unit is provided.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_mean_prcp(self, gage_id_lst: list[str], unit: str = \"mm/d\") -&gt; xr.Dataset:\n    \"\"\"Reads the mean daily precipitation for a list of basins, with unit conversion.\n\n    Args:\n        gage_id_lst: A list of basin identifiers.\n        unit: The desired unit for the output precipitation. Defaults to \"mm/d\".\n            Supported units: ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h',\n            'mm/3hour', 'mm/8d', 'mm/8day'].\n\n    Returns:\n        An xarray Dataset containing the mean precipitation data in the specified units.\n\n    Raises:\n        ValueError: If an unsupported unit is provided.\n    \"\"\"\n    prcp_var_name = \"p_mean\"\n    data_ds = self.read_attr_xrdataset(\n        gage_id_lst=gage_id_lst, var_lst=[prcp_var_name]\n    )\n    # No conversion needed\n    if unit in [\"mm/d\", \"mm/day\"]:\n        return data_ds\n\n    # Conversion needed, create a new dataset\n    converted_ds = data_ds.copy()\n    # After renaming, the variable in the dataset is now the standard name\n    if unit in [\"mm/h\", \"mm/hour\"]:\n        converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 24\n    elif unit in [\"mm/3h\", \"mm/3hour\"]:\n        converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 8\n    elif unit in [\"mm/8d\", \"mm/8day\"]:\n        converted_ds[prcp_var_name] = data_ds[prcp_var_name] * 8\n    else:\n        raise ValueError(\n            \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n        )\n    return converted_ds\n</code></pre>"},{"location":"api/hydrodataset/","title":"hydrodataset","text":""},{"location":"api/hydrodataset/#overview","title":"Overview","text":"<p>hydrodataset is a Python package for accessing hydrological datasets with a unified API. It provides a standardized interface for reading diverse hydrological datasets, serving as a data-adapting layer on top of AquaFetch.</p>"},{"location":"api/hydrodataset/#key-features","title":"Key Features","text":"<ul> <li>Unified API: Consistent interface across all datasets</li> <li>Standardized Variables: Common variable names across different datasets</li> <li>NetCDF Caching: Fast data access through cached <code>.nc</code> files</li> <li>Multiple Data Sources: Support for alternative data sources within datasets</li> <li>Unit Management: Automatic unit handling with pint integration</li> </ul>"},{"location":"api/hydrodataset/#core-components","title":"Core Components","text":""},{"location":"api/hydrodataset/#base-classes","title":"Base Classes","text":"<ul> <li>HydroDataset: Abstract base class for all dataset implementations</li> <li>StandardVariable: Standardized variable name constants</li> </ul>"},{"location":"api/hydrodataset/#supported-datasets","title":"Supported Datasets","text":""},{"location":"api/hydrodataset/#camels-series","title":"CAMELS Series","text":"<p>Continental-scale hydrological datasets from different regions: - CAMELS-AUS, CAMELS-BR, CAMELS-CH, CAMELS-CL, CAMELS-COL - CAMELS-DE, CAMELS-DK, CAMELS-FI, CAMELS-FR, CAMELS-GB - CAMELS-IND, CAMELS-LUX, CAMELS-NZ, CAMELS-SE, CAMELS-US</p>"},{"location":"api/hydrodataset/#camelsh-series","title":"CAMELSH Series","text":"<p>Hourly resolution CAMELS datasets: - CAMELSH (Hourly US data) - CAMELSH-KR (South Korea)</p>"},{"location":"api/hydrodataset/#caravan-series","title":"Caravan Series","text":"<p>Global datasets from Caravan project: - Caravan-DK (Denmark) - GRDC-Caravan (Global Runoff Data Centre)</p>"},{"location":"api/hydrodataset/#lamah-series","title":"LamaH Series","text":"<p>Large-sample hydrological datasets: - LamaH-CE (Central Europe) - LamaH-ICE (Iceland)</p>"},{"location":"api/hydrodataset/#other-datasets","title":"Other Datasets","text":"<ul> <li>BULL (Spain)</li> <li>E-STREAMS (Europe)</li> <li>HYSETS (North America)</li> <li>SIMBI (Multiple regions)</li> </ul>"},{"location":"api/hydrodataset/#quick-start","title":"Quick Start","text":"<pre><code>from hydrodataset.camels_us import CamelsUs\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = CamelsUs(data_path)\n\n# Read basin IDs\nbasin_ids = ds.read_object_ids()\n\n# Read timeseries data\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\n\n# Read attribute data\nattr_data = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\n</code></pre>"},{"location":"api/hydrodataset/#configuration","title":"Configuration","text":"<p>Create <code>~/hydro_setting.yml</code> in your home directory:</p> <pre><code>local_data_path:\n  root: 'D:\\data\\waterism'\n  datasets-origin: 'D:\\data\\waterism\\datasets-origin'  # Raw data\n  cache: 'D:\\data\\waterism\\cache'  # Cached NetCDF files\n</code></pre>"},{"location":"api/hydrodataset/#module-reference","title":"Module Reference","text":""},{"location":"api/hydrodataset/#hydrodataset","title":"<code>hydrodataset</code>","text":"<p>Author: Wenyu Ouyang Date: 2022-09-05 23:20:24 LastEditTime: 2025-11-06 15:37:45 LastEditors: Wenyu Ouyang Description: set file dir FilePath: \\hydrodataset\\hydrodataset__init__.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"api/hydrodataset/#hydrodataset.CAMELS_US","title":"<code>CAMELS_US</code>","text":"<p>               Bases: <code>CAMELS_US</code></p> <p>Custom CAMELS_US class with Zenodo mirror URLs and -999 missing value handling.</p> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>class CAMELS_US(_AquaFetchCAMELS_US):\n    \"\"\"Custom CAMELS_US class with Zenodo mirror URLs and -999 missing value handling.\"\"\"\n\n    # Override URLs to use Zenodo mirror\n    # Note: URLs should NOT include the filename as AquaFetch will append it\n    url = {\n        'camels_attributes_v2.0.pdf': 'https://zenodo.org/records/15529996/files/',\n        'camels_attributes_v2.0.xlsx': 'https://zenodo.org/records/15529996/files/',\n        'camels_clim.txt': 'https://zenodo.org/records/15529996/files/',\n        'camels_geol.txt': 'https://zenodo.org/records/15529996/files/',\n        'camels_hydro.txt': 'https://zenodo.org/records/15529996/files/',\n        'camels_name.txt': 'https://zenodo.org/records/15529996/files/',\n        'camels_soil.txt': 'https://zenodo.org/records/15529996/files/',\n        'camels_topo.txt': 'https://zenodo.org/records/15529996/files/',\n        'camels_vege.txt': 'https://zenodo.org/records/15529996/files/',\n        'readme.txt': 'https://zenodo.org/records/15529996/files/',\n        'basin_timeseries_v1p2_metForcing_obsFlow.zip': 'https://zenodo.org/records/15529996/files/',\n        'basin_set_full_res.zip': 'https://zenodo.org/records/15529996/files/',\n        'basin_timeseries_v1p2_modelOutput_daymet.zip': 'https://zenodo.org/records/15529996/files/',\n        'basin_timeseries_v1p2_modelOutput_maurer.zip': 'https://zenodo.org/records/15529996/files/',\n        'basin_timeseries_v1p2_modelOutput_nldas.zip': 'https://zenodo.org/records/15529996/files/',\n    }\n\n    def _read_stn_dyn(self, stn: str):\n        \"\"\"\n        Override parent method to handle -999 missing values in streamflow data.\n\n        According to readme_streamflow.txt:\n        \"Streamflow data that are missing are given the streamflow value -999.0\"\n\n        This method reads dynamic features (forcing + streamflow) and replaces -999\n        values with NaN for proper data handling.\n        \"\"\"\n        assert isinstance(stn, str)\n        df = None\n        dir_name = self.folders[self.data_source]\n\n        # Read forcing data\n        for cat in os.listdir(os.path.join(self.dataset_dir, dir_name)):\n            cat_dirs = os.listdir(os.path.join(self.dataset_dir, f'{dir_name}{SEP}{cat}'))\n            stn_file = f'{stn}_lump_cida_forcing_leap.txt'\n            if stn_file in cat_dirs:\n                df = pd.read_csv(\n                    os.path.join(self.dataset_dir, f'{dir_name}{SEP}{cat}{SEP}{stn_file}'),\n                    sep=r\"\\s+|;|:\",\n                    skiprows=4,\n                    engine='python',\n                    names=['Year', 'Mnth', 'Day', 'Hr', 'dayl(s)', 'prcp(mm/day)',\n                           'srad(W/m2)', 'swe(mm)', 'tmax(C)', 'tmin(C)', 'vp(Pa)'],\n                )\n                df.index = pd.to_datetime(\n                    df['Year'].map(str) + '-' + df['Mnth'].map(str) + '-' + df['Day'].map(str)\n                )\n\n        # Read streamflow data\n        flow_dir = os.path.join(self.dataset_dir, 'usgs_streamflow')\n        for cat in os.listdir(flow_dir):\n            cat_dirs = os.listdir(os.path.join(flow_dir, cat))\n            stn_file = f'{stn}_streamflow_qc.txt'\n            if stn_file in cat_dirs:\n                fpath = os.path.join(flow_dir, f'{cat}{SEP}{stn_file}')\n                q_df = pd.read_csv(\n                    fpath,\n                    sep=r\"\\s+\",\n                    names=['station', 'Year', 'Month', 'Day', 'Flow', 'Flag'],\n                    engine='python'\n                )\n                q_df.index = pd.to_datetime(\n                    q_df['Year'].map(str) + '-' + q_df['Month'].map(str) + '-' + q_df['Day'].map(str)\n                )\n\n                # Replace -999 missing values with NaN\n                q_df['Flow'] = q_df['Flow'].replace(-999.0, np.nan)\n\n        # Concatenate forcing and streamflow data\n        stn_df = pd.concat([\n            df[['dayl(s)', 'prcp(mm/day)', 'srad(W/m2)', 'swe(mm)', 'tmax(C)', 'tmin(C)', 'vp(Pa)']],\n            q_df['Flow']\n        ], axis=1)\n\n        # Rename columns according to standard names\n        stn_df.rename(columns=self.dyn_map, inplace=True)\n\n        # Apply unit conversion factors\n        for col, fact in self.dyn_factors.items():\n            if col in stn_df.columns:\n                stn_df[col] *= fact\n\n        return stn_df\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels","title":"<code>Camels</code>","text":"<p>               Bases: <code>HydroDataset</code></p> Source code in <code>hydrodataset/camels.py</code> <pre><code>class Camels(HydroDataset):\n    def __init__(\n        self,\n        data_path,\n        download=False,\n        region: str = \"US\",\n    ):\n        print(\"Initializing Camels class...\")\n        \"\"\"\n        Initialization for CAMELS series dataset\n\n        Parameters\n        ----------\n        data_path\n            where we put the dataset.\n            we already set the ROOT directory for hydrodataset,\n            so here just set it as a relative path,\n            by default \"camels/camels_us\"\n        download\n            if true, download, by defaulf False\n        region\n            the default is CAMELS(-US), since it's the first CAMELS dataset.\n            All are included in CAMELS_REGIONS\n        \"\"\"\n        self.data_path = os.path.join(data_path, \"CAMELS_US\")\n        super().__init__(self.data_path)\n        if region not in CAMELS_REGIONS:\n            raise NotImplementedError(\n                f\"Please chose one region in: {str(CAMELS_REGIONS)}\"\n            )\n        self.region = region\n        self.data_source_description = self.set_data_source_describe()\n        check_download = False\n        for url in self.data_source_description[\"CAMELS_DOWNLOAD_URL_LST\"]:\n            fpath = os.path.join(self.data_source_dir, url.rsplit(\"/\", 1)[1])\n            if not os.path.exists(fpath):\n                check_download = True\n                break\n        if check_download:\n            if download:\n                self.download_data_source()\n        check_zip_extract = False\n        # Check if zip files have been extracted\n        for url in self.data_source_description[\"CAMELS_DOWNLOAD_URL_LST\"]:\n            filename = url.rsplit(\"/\", 1)[1]\n            # Only check for zip files\n            if filename.endswith(\".zip\"):\n                # The extracted directory name (without .zip extension)\n                extracted_dir = self.data_source_dir / filename[:-4]\n                if not extracted_dir.exists():\n                    check_zip_extract = True\n                    break\n        if check_zip_extract:\n            hydro_file.zip_extract(self.data_source_description[\"CAMELS_DIR\"])\n        self.sites = self.read_site_info()\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camelsus_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camelsus_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1980-01-01\", \"2014-12-31\"]\n\n    def _get_attribute_units(self):\n        return {\n            \"gauge_lat\": \"degree\",\n            \"gauge_lon\": \"degree\",\n            \"elev_mean\": \"m\",\n            \"slope_mean\": \"m/km\",\n            \"area\": \"km^2\",\n            \"geol_1st_class\": \"dimensionless\",\n            \"glim_1st_class_frac\": \"dimensionless\",\n            \"geol_2nd_class\": \"dimensionless\",\n            \"glim_2nd_class_frac\": \"dimensionless\",\n            \"carbonate_rocks_frac\": \"dimensionless\",\n            \"geol_porostiy\": \"dimensionless\",\n            \"geol_permeability\": \"m^2\",\n            \"frac_forest\": \"dimensionless\",\n            \"lai_max\": \"dimensionless\",\n            \"lai_diff\": \"dimensionless\",\n            \"gvf_max\": \"dimensionless\",\n            \"gvf_diff\": \"dimensionless\",\n            \"dom_land_cover_frac\": \"dimensionless\",\n            \"dom_land_cover\": \"dimensionless\",\n            \"root_depth_50\": \"m\",\n            \"root_depth_99\": \"m\",\n            \"q_mean\": \"mm/day\",\n            \"runoff_ratio\": \"dimensionless\",\n            \"slope_fdc\": \"dimensionless\",\n            \"baseflow_index\": \"dimensionless\",\n            \"stream_elas\": \"dimensionless\",\n            \"q5\": \"mm/day\",\n            \"q95\": \"mm/day\",\n            \"high_q_freq\": \"day/year\",\n            \"high_q_dur\": \"day\",\n            \"low_q_freq\": \"day/year\",\n            \"low_q_dur\": \"day\",\n            \"zero_q_freq\": \"percent\",\n            \"hfd_mean\": \"dimensionless\",\n            \"soil_depth_pelletier\": \"m\",\n            \"soil_depth_statsgo\": \"m\",\n            \"soil_porosity\": \"dimensionless\",\n            \"soil_conductivity\": \"cm/hr\",\n            \"max_water_content\": \"m\",\n            \"sand_frac\": \"percent\",\n            \"silt_frac\": \"percent\",\n            \"clay_frac\": \"percent\",\n            \"water_frac\": \"percent\",\n            \"organic_frac\": \"percent\",\n            \"other_frac\": \"percent\",\n            \"p_mean\": \"mm/day\",\n            \"pet_mean\": \"mm/day\",\n            \"p_seasonality\": \"dimensionless\",\n            \"frac_snow\": \"dimensionless\",\n            \"aridity\": \"dimensionless\",\n            \"high_prec_freq\": \"days/year\",\n            \"high_prec_dur\": \"day\",\n            \"high_prec_timing\": \"dimensionless\",\n            \"low_prec_freq\": \"days/year\",\n            \"low_prec_dur\": \"day\",\n            \"low_prec_timing\": \"dimensionless\",\n            \"huc_02\": \"dimensionless\",\n            \"gauge_name\": \"dimensionless\",\n        }\n\n    def _get_timeseries_units(self):\n        # Order matches get_relevant_cols(): dayl, pcp_mm, solrad_wm2, swe_mm, airtemp_c_max, airtemp_c_min, vp_hpa, PET\n        return [\"s\", \"mm/day\", \"W/m^2\", \"mm/day\", \"\u00b0C\", \"\u00b0C\", \"hPa\", \"mm/day\"]\n\n    def get_name(self):\n        return \"CAMELS_\" + self.region\n\n    def _dynamic_features(self) -&gt; list:\n        return self.get_relevant_cols().tolist() + [\"streamflow\"]\n\n    def _static_features(self) -&gt; list:\n        return self.get_constant_cols().tolist()\n\n    def set_data_source_describe(self) -&gt; collections.OrderedDict:\n        \"\"\"\n        the files in the dataset and their location in file system\n\n        Returns\n        -------\n        collections.OrderedDict\n            the description for a CAMELS dataset\n        \"\"\"\n        camels_db = self.data_source_dir\n        return self._set_data_source_camelsus_describe(camels_db)\n\n    def _set_data_source_camelsus_describe(self, camels_db):\n        # shp file of basins\n        camels_shp_file = camels_db.joinpath(\n            \"basin_set_full_res\", \"HCDN_nhru_final_671.shp\"\n        )\n        # config of flow data\n        flow_dir = camels_db.joinpath(\n            \"basin_timeseries_v1p2_metForcing_obsFlow\",\n            \"basin_dataset_public_v1p2\",\n            \"usgs_streamflow\",\n        )\n        flow_after_2015_dir = camels_db.joinpath(\n            \"camels_streamflow\", \"camels_streamflow\"\n        )\n        # forcing\n        forcing_dir = camels_db.joinpath(\n            \"basin_timeseries_v1p2_metForcing_obsFlow\",\n            \"basin_dataset_public_v1p2\",\n            \"basin_mean_forcing\",\n        )\n        forcing_types = [\"daymet\", \"maurer\", \"nldas\"]\n        # attr\n        attr_dir = camels_db\n        gauge_id_file = attr_dir.joinpath(\"camels_name.txt\")\n        attr_key_lst = [\"topo\", \"clim\", \"hydro\", \"vege\", \"soil\", \"geol\"]\n        base_url = \"https://gdex.ucar.edu/dataset/camels\"\n        download_url_lst = [\n            f\"{base_url}/file/basin_set_full_res.zip\",\n            # f\"{base_url}/file/basin_timeseries_v1p2_metForcing_obsFlow.zip\",\n            f\"{base_url}/file/camels_attributes_v2.0.xlsx\",\n            f\"{base_url}/file/camels_clim.txt\",\n            f\"{base_url}/file/camels_geol.txt\",\n            f\"{base_url}/file/camels_hydro.txt\",\n            f\"{base_url}/file/camels_name.txt\",\n            f\"{base_url}/file/camels_soil.txt\",\n            f\"{base_url}/file/camels_topo.txt\",\n            f\"{base_url}/file/camels_vege.txt\",\n        ]\n\n        return collections.OrderedDict(\n            CAMELS_DIR=camels_db,\n            CAMELS_FLOW_DIR=flow_dir,\n            CAMELS_FLOW_AFTER2015_DIR=flow_after_2015_dir,\n            CAMELS_FORCING_DIR=forcing_dir,\n            CAMELS_FORCING_TYPE=forcing_types,\n            CAMELS_ATTR_DIR=attr_dir,\n            CAMELS_ATTR_KEY_LST=attr_key_lst,\n            CAMELS_GAUGE_FILE=gauge_id_file,\n            CAMELS_BASINS_SHP_FILE=camels_shp_file,\n            CAMELS_DOWNLOAD_URL_LST=download_url_lst,\n        )\n\n    def download_data_source(self) -&gt; None:\n        \"\"\"\n        Download the required zip files\n\n        Now we only support CAMELS-US's downloading.\n        For others, please download it manually,\n        and put all files of a dataset in one directory.\n        For example, all files of CAMELS_AUS should be put in \"camels_aus\"\n\n        Returns\n        -------\n        None\n        \"\"\"\n        camels_config = self.data_source_description\n        self.data_source_dir.mkdir(exist_ok=True)\n        links = camels_config[\"CAMELS_DOWNLOAD_URL_LST\"]\n        for url in links:\n            fzip = Path(self.data_source_dir, url.rsplit(\"/\", 1)[1])\n            if fzip.exists():\n                with urlopen(url) as response:\n                    if int(response.info()[\"Content-length\"]) != fzip.stat().st_size:\n                        fzip.unlink()\n        to_dl = [\n            url\n            for url in links\n            if not Path(self.data_source_dir, url.rsplit(\"/\", 1)[1]).exists()\n        ]\n        hydro_file.download_zip_files(to_dl, self.data_source_dir)\n\n    def read_site_info(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Read the basic information of gages in a CAMELS dataset\n\n        Returns\n        -------\n        pd.DataFrame\n            basic info of gages\n        \"\"\"\n        camels_file = self.data_source_description[\"CAMELS_GAUGE_FILE\"]\n        return pd.read_csv(camels_file, sep=\";\", dtype={\"gauge_id\": str, \"huc_02\": str})\n\n    def get_constant_cols(self) -&gt; np.ndarray:\n        \"\"\"\n        all readable attrs in CAMELS\n\n        Returns\n        -------\n        np.array\n            attribute types\n        \"\"\"\n        data_folder = self.data_source_description[\"CAMELS_ATTR_DIR\"]\n        return self._get_constant_cols_some(data_folder, \"camels_\", \".txt\", \";\")\n\n    def _get_constant_cols_some(self, data_folder, prefix, postfix, sep):\n        var_dict = {}\n        var_lst = []\n        key_lst = self.data_source_description[\"CAMELS_ATTR_KEY_LST\"]\n        for key in key_lst:\n            data_file = os.path.join(data_folder, prefix + key + postfix)\n            data_temp = pd.read_csv(data_file, sep=sep)\n            var_lst_temp = list(data_temp.columns[1:])\n            var_dict[key] = var_lst_temp\n            var_lst.extend(var_lst_temp)\n        return np.array(var_lst)\n\n    def get_relevant_cols(self) -&gt; np.ndarray:\n        \"\"\"\n        all readable forcing types\n\n        Returns\n        -------\n        np.array\n            forcing types\n        \"\"\"\n        # PET is from model_output file in CAMELS-US\n        return np.array([\"dayl\", \"pcp_mm\", \"solrad_wm2\", \"swe_mm\", \"airtemp_c_max\", \"airtemp_c_min\", \"vp_hpa\", \"PET\"])\n\n    def get_target_cols(self) -&gt; np.ndarray:\n        \"\"\"\n        For CAMELS, the target vars are streamflows\n\n        Returns\n        -------\n        np.array\n            streamflow types\n        \"\"\"\n        return np.array([\"q_cms_obs\", \"ET\"])\n\n    def read_object_ids(self, **kwargs) -&gt; np.ndarray:\n        \"\"\"\n        read station ids\n\n        Parameters\n        ----------\n        **kwargs\n            optional params if needed\n\n        Returns\n        -------\n        np.array\n            gage/station ids\n        \"\"\"\n        return np.sort(self.sites[\"gauge_id\"].values)\n\n    def read_usgs_gage(self, usgs_id, t_range):\n        \"\"\"\n        read streamflow data of a station for date before 2015-01-01 from CAMELS-US\n\n        Parameters\n        ----------\n        usgs_id\n            the station id\n        t_range\n            the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n\n        Returns\n        -------\n        np.array\n            streamflow data of one station for a given time range\n            Unit: m^3/s (converted from original foot^3/s)\n        \"\"\"\n        logging.debug(\"reading %s streamflow data before 2015\", usgs_id)\n        gage_id_df = self.sites\n        huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n        usgs_file = os.path.join(\n            self.data_source_description[\"CAMELS_FLOW_DIR\"],\n            huc,\n            usgs_id + \"_streamflow_qc.txt\",\n        )\n        data_temp = pd.read_csv(usgs_file, sep=r\"\\s+\", header=None)\n        obs = data_temp[4].values\n        obs[obs &lt; 0] = np.nan\n\n        # Convert from foot^3/s to m^3/s\n        # Conversion factor: 1 foot^3/s = 0.0283168 m^3/s\n        obs = obs * 0.0283168\n\n        t_lst = hydro_time.t_range_days(t_range)\n        nt = t_lst.shape[0]\n        return (\n            self._read_usgs_gage_for_some(nt, data_temp, t_lst, obs)\n            if len(obs) != nt\n            else obs\n        )\n\n    def _read_usgs_gage_for_some(self, nt, data_temp, t_lst, obs):\n        result = np.full([nt], np.nan)\n        df_date = data_temp[[1, 2, 3]]\n        df_date.columns = [\"year\", \"month\", \"day\"]\n        date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n        [C, ind1, ind2] = np.intersect1d(date, t_lst, return_indices=True)\n        result[ind2] = obs[ind1]\n        return result\n\n    def read_camels_streamflow(self, usgs_id, t_range):\n        \"\"\"\n        read streamflow data of a station for date after 2015 from CAMELS-US\n\n        The streamflow data is downloaded from USGS website by HyRivers tools\n\n        Parameters\n        ----------\n        usgs_id\n            the station id\n        t_range\n            the time range, for example, [\"2015-01-01\", \"2022-01-01\"]\n\n        Returns\n        -------\n        np.array\n            streamflow data of one station for a given time range\n            Unit: m^3/s (converted from original foot^3/s)\n        \"\"\"\n        logging.debug(\"reading %s streamflow data after 2015\", usgs_id)\n        gage_id_df = self.sites\n        huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n        usgs_file = os.path.join(\n            self.data_source_description[\"CAMELS_FLOW_AFTER2015_DIR\"],\n            huc,\n            usgs_id + \"_streamflow_qc.txt\",\n        )\n        data_temp = pd.read_csv(usgs_file, sep=\",\", header=None, skiprows=1)\n        obs = data_temp[4].values\n        obs[obs &lt; 0] = np.nan\n\n        # Convert from foot^3/s to m^3/s\n        # Conversion factor: 1 foot^3/s = 0.0283168 m^3/s\n        obs = obs * 0.0283168\n\n        t_lst = hydro_time.t_range_days(t_range)\n        nt = t_lst.shape[0]\n        return (\n            self._read_usgs_gage_for_some(nt, data_temp, t_lst, obs)\n            if len(obs) != nt\n            else obs\n        )\n\n    def read_target_cols(\n        self,\n        gage_id_lst: Union[list, np.array] = None,\n        t_range: list = None,\n        target_cols: Union[list, np.array] = None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        read target values; for CAMELS, they are streamflows\n\n        default target_cols is an one-value list\n        Notice: the unit of target outputs in different regions are not totally same\n\n        Parameters\n        ----------\n        gage_id_lst\n            station ids\n        t_range\n            the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n        target_cols\n            the default is None, but we neea at least one default target.\n            For CAMELS-US, it is [\"usgsFlow\"];\n        kwargs\n            some other params if needed\n\n        Returns\n        -------\n        np.array\n            streamflow data, 3-dim [station, time, streamflow]\n        \"\"\"\n        if target_cols is None:\n            return np.array([])\n        else:\n            nf = len(target_cols)\n        t_range_list = hydro_time.t_range_days(t_range)\n        nt = t_range_list.shape[0]\n        y = np.full([len(gage_id_lst), nt, nf], np.nan)\n        for k in tqdm(\n            range(len(gage_id_lst)), desc=\"Read streamflow data of CAMELS-US\"\n        ):\n            for j in range(len(target_cols)):\n                if target_cols[j] == \"ET\":\n                    data_et = self.read_camels_us_model_output_data(\n                        gage_id_lst[k : k + 1], t_range, [\"ET\"]\n                    )\n                    y[k, :, j : j + 1] = data_et\n                else:\n                    data_obs = self._read_augmented_camels_streamflow(\n                        gage_id_lst, t_range, t_range_list, k\n                    )\n                    y[k, :, j] = data_obs\n        return y\n\n    def _read_augmented_camels_streamflow(self, gage_id_lst, t_range, t_range_list, k):\n        dt150101 = hydro_time.t2str(\"2015-01-01\")\n        if t_range_list[-1] &gt; dt150101 and t_range_list[0] &lt; dt150101:\n            # latest streamflow data in CAMELS is 2014/12/31\n            data_obs_after_2015 = self.read_camels_streamflow(\n                gage_id_lst[k], [\"2015-01-01\", t_range[1]]\n            )\n            data_obs_before_2015 = self.read_usgs_gage(\n                gage_id_lst[k], [t_range[0], \"2015-01-01\"]\n            )\n            return np.concatenate((data_obs_before_2015, data_obs_after_2015))\n        elif t_range_list[-1] &lt;= dt150101:\n            return self.read_usgs_gage(gage_id_lst[k], t_range)\n        else:\n            return self.read_camels_streamflow(gage_id_lst[k], t_range)\n\n    def read_camels_us_model_output_data(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        forcing_type=\"daymet\",\n    ) -&gt; np.array:\n        \"\"\"\n        Read model output data of CAMELS-US, including SWE, PRCP, RAIM, TAIR, PET, ET, MOD_RUN, OBS_RUN\n        Date starts from 1980-10-01 to 2014-12-31\n\n        Parameters\n        ----------\n        gage_id_lst : list\n            the station id list\n        var_lst : list\n            the variable list\n        t_range : list\n            the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n        forcing_type : str, optional\n            by default \"daymet\"\n        \"\"\"\n        t_range_list = hydro_time.t_range_days(t_range)\n        model_out_put_var_lst = [\n            \"SWE\",\n            \"PRCP\",\n            \"RAIM\",\n            \"TAIR\",\n            \"PET\",\n            \"ET\",\n            \"MOD_RUN\",\n            \"OBS_RUN\",\n        ]\n        if not set(var_lst).issubset(set(model_out_put_var_lst)):\n            raise RuntimeError(\"not in this list\")\n        nt = t_range_list.shape[0]\n        chosen_camels_mods = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n        count = 0\n        for usgs_id in tqdm(gage_id_lst, desc=\"Read model output data of CAMELS-US\"):\n            gage_id_df = self.sites\n            huc02_ = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n            file_path_dir = os.path.join(\n                self.data_source_dir,\n                \"basin_timeseries_v1p2_modelOutput_\" + forcing_type,\n                \"model_output_\" + forcing_type,\n                \"model_output\",\n                \"flow_timeseries\",\n                forcing_type,\n                huc02_,\n            )\n            sac_random_seeds = [\n                \"05\",\n                \"11\",\n                \"27\",\n                \"33\",\n                \"48\",\n                \"59\",\n                \"66\",\n                \"72\",\n                \"80\",\n                \"94\",\n            ]\n            files = [\n                os.path.join(\n                    file_path_dir, usgs_id + \"_\" + random_seed + \"_model_output.txt\"\n                )\n                for random_seed in sac_random_seeds\n            ]\n            results = []\n            for file in files:\n                result = pd.read_csv(file, sep=\"\\s+\")\n                df_date = result[[\"YR\", \"MNTH\", \"DY\"]]\n                df_date.columns = [\"year\", \"month\", \"day\"]\n                date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n                [c, ind1, ind2] = np.intersect1d(\n                    date, t_range_list, return_indices=True\n                )\n                results.append(result[var_lst].values[ind1])\n            result_np = np.array(results)\n            chosen_camels_mods[count, ind2, :] = np.mean(result_np, axis=0)\n            count = count + 1\n        return chosen_camels_mods\n\n    def read_forcing_gage(self, usgs_id, var_lst, t_range_list, forcing_type=\"daymet\"):\n        # data_source = daymet or maurer or nldas\n        logging.debug(\"reading %s forcing data\", usgs_id)\n        gage_id_df = self.sites\n        huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n\n        data_folder = self.data_source_description[\"CAMELS_FORCING_DIR\"]\n        temp_s = \"cida\" if forcing_type == \"daymet\" else forcing_type\n        data_file = os.path.join(\n            data_folder,\n            forcing_type,\n            huc,\n            f\"{usgs_id}_lump_{temp_s}_forcing_leap.txt\",\n        )\n        data_temp = pd.read_csv(data_file, sep=r\"\\s+\", header=None, skiprows=4)\n\n        # File column names (actual names in the forcing file)\n        forcing_lst = [\n            \"Year\",\n            \"Mnth\",\n            \"Day\",\n            \"Hr\",\n            \"dayl\",\n            \"prcp\",\n            \"srad\",\n            \"swe\",\n            \"tmax\",\n            \"tmin\",\n            \"vp\",\n        ]\n\n        # Map new standardized variable names to old file column names\n        var_name_mapping = {\n            \"dayl\": \"dayl\",\n            \"pcp_mm\": \"prcp\",\n            \"solrad_wm2\": \"srad\",\n            \"swe_mm\": \"swe\",\n            \"airtemp_c_max\": \"tmax\",\n            \"airtemp_c_min\": \"tmin\",\n            \"vp_hpa\": \"vp\",\n        }\n\n        df_date = data_temp[[0, 1, 2]]\n        df_date.columns = [\"year\", \"month\", \"day\"]\n        date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n\n        nf = len(var_lst)\n        [c, ind1, ind2] = np.intersect1d(date, t_range_list, return_indices=True)\n        nt = c.shape[0]\n        out = np.full([nt, nf], np.nan)\n\n        for k in range(nf):\n            # Convert new variable name to old file column name\n            old_var_name = var_name_mapping.get(var_lst[k], var_lst[k])\n            ind = forcing_lst.index(old_var_name)\n            out[ind2, k] = data_temp[ind].values[ind1]\n        return out\n\n    def read_relevant_cols(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        forcing_type=\"daymet\",\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Read forcing data\n\n        Parameters\n        ----------\n        gage_id_lst\n            station ids\n        t_range\n            the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n        var_lst\n            forcing variable types\n        forcing_type\n            now only for CAMELS-US, there are three types: daymet, nldas, maurer\n        Returns\n        -------\n        np.array\n            forcing data\n        \"\"\"\n        t_range_list = hydro_time.t_range_days(t_range)\n        nt = t_range_list.shape[0]\n        x = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n        for k in tqdm(range(len(gage_id_lst)), desc=\"Read forcing data of CAMELS-US\"):\n            if \"PET\" in var_lst:\n                pet_idx = var_lst.index(\"PET\")\n                data_pet = self.read_camels_us_model_output_data(\n                    gage_id_lst[k : k + 1], t_range, [\"PET\"]\n                )\n                x[k, :, pet_idx : pet_idx + 1] = data_pet\n                no_pet_var_lst = [x for x in var_lst if x != \"PET\"]\n                data = self.read_forcing_gage(\n                    gage_id_lst[k],\n                    no_pet_var_lst,\n                    t_range_list,\n                    forcing_type=forcing_type,\n                )\n                var_indices = [var_lst.index(var) for var in no_pet_var_lst]\n                x[k : k + 1, :, var_indices] = data\n            else:\n                data = self.read_forcing_gage(\n                    gage_id_lst[k],\n                    var_lst,\n                    t_range_list,\n                    forcing_type=forcing_type,\n                )\n                x[k, :, :] = data\n        return x\n\n    def read_attr_all(self):\n        data_folder = self.data_source_description[\"CAMELS_ATTR_DIR\"]\n        key_lst = self.data_source_description[\"CAMELS_ATTR_KEY_LST\"]\n        f_dict = {}\n        var_dict = {}\n        var_lst = []\n        out_lst = []\n        gage_dict = self.sites\n        camels_str = \"camels_\"\n        sep_ = \";\"\n        for key in key_lst:\n            data_file = os.path.join(data_folder, camels_str + key + \".txt\")\n            if self.region == \"GB\":\n                data_file = os.path.join(\n                    data_folder, camels_str + key + \"_attributes.csv\"\n                )\n            elif self.region == \"CC\":\n                data_file = os.path.join(data_folder, key + \".csv\")\n            data_temp = pd.read_csv(data_file, sep=sep_)\n            var_lst_temp = list(data_temp.columns[1:])\n            var_dict[key] = var_lst_temp\n            var_lst.extend(var_lst_temp)\n            k = 0\n            gage_id_key = \"gauge_id\"\n            if self.region == \"CC\":\n                gage_id_key = \"gage_id\"\n            n_gage = len(gage_dict[gage_id_key].values)\n            out_temp = np.full([n_gage, len(var_lst_temp)], np.nan)\n            for field in var_lst_temp:\n                if is_string_dtype(data_temp[field]):\n                    value, ref = pd.factorize(data_temp[field], sort=True)\n                    out_temp[:, k] = value\n                    f_dict[field] = ref.tolist()\n                elif is_numeric_dtype(data_temp[field]):\n                    out_temp[:, k] = data_temp[field].values\n                k = k + 1\n            out_lst.append(out_temp)\n        out = np.concatenate(out_lst, 1)\n        return out, var_lst, var_dict, f_dict\n\n    def read_attr_all_yr(self):\n        var_lst = self.get_constant_cols().tolist()\n        gage_id_lst = self.read_object_ids()\n        # for factorized data, we need factorize all gages' data to keep the factorized number same all the time\n        n_gage = len(self.read_object_ids())\n        c = np.full([n_gage, len(var_lst)], np.nan, dtype=object)\n        for k in range(n_gage):\n            attr_file = os.path.join(\n                self.data_source_description[\"CAMELS_ATTR_DIR\"],\n                gage_id_lst[k],\n                \"attributes.json\",\n            )\n            attr_data = hydro_file.unserialize_json_ordered(attr_file)\n            for j in range(len(var_lst)):\n                c[k, j] = attr_data[var_lst[j]]\n        data_temp = pd.DataFrame(c, columns=var_lst)\n        out_temp = np.full([n_gage, len(var_lst)], np.nan)\n        f_dict = {}\n        k = 0\n        for field in var_lst:\n            if field in [\"high_prec_timing\", \"low_prec_timing\"]:\n                # string type\n                value, ref = pd.factorize(data_temp[field], sort=True)\n                out_temp[:, k] = value\n                f_dict[field] = ref.tolist()\n            else:\n                out_temp[:, k] = data_temp[field].values\n            k = k + 1\n        # keep same format with CAMELS_US\n        return out_temp, var_lst, None, f_dict\n\n    def read_constant_cols(\n        self, gage_id_lst=None, var_lst=None, is_return_dict=False, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Read Attributes data\n\n        Parameters\n        ----------\n        gage_id_lst\n            station ids\n        var_lst\n            attribute variable types\n        is_return_dict\n            if true, return var_dict and f_dict for CAMELS_US\n        Returns\n        -------\n        Union[tuple, np.array]\n            if attr var type is str, return factorized data.\n            When we need to know what a factorized value represents, we need return a tuple;\n            otherwise just return an array\n        \"\"\"\n        attr_all, var_lst_all, var_dict, f_dict = self.read_attr_all()\n        ind_var = [var_lst_all.index(var) for var in var_lst]\n        id_lst_all = self.read_object_ids()\n        # Notice the sequence of station ids ! Some id_lst_all are not sorted, so don't use np.intersect1d\n        ind_grid = [id_lst_all.tolist().index(tmp) for tmp in gage_id_lst]\n        temp = attr_all[ind_grid, :]\n        out = temp[:, ind_var]\n        return (out, var_dict, f_dict) if is_return_dict else out\n\n    def read_area(self, gage_id_lst) -&gt; np.ndarray:\n        return self.read_attr_xrdataset(gage_id_lst, [\"area\"])\n\n    def read_mean_prcp(self, gage_id_lst, unit=\"mm/d\") -&gt; xr.Dataset:\n        \"\"\"Read mean precipitation data\n\n        Parameters\n        ----------\n        gage_id_lst : list\n            station ids\n        unit : str, optional\n            the unit of mean_prcp, by default \"mm/d\"\n\n        Returns\n        -------\n        xr.Dataset\n            TODO: now only support CAMELS-US\n\n        Raises\n        ------\n        NotImplementedError\n            some regions are not supported\n        ValueError\n            unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\n        \"\"\"\n        data = self.read_attr_xrdataset(gage_id_lst, [\"p_mean\"])\n        if unit in [\"mm/d\", \"mm/day\"]:\n            converted_data = data\n        elif unit in [\"mm/h\", \"mm/hour\"]:\n            converted_data = data / 24\n        elif unit in [\"mm/3h\", \"mm/3hour\"]:\n            converted_data = data / 8\n        elif unit in [\"mm/8d\", \"mm/8day\"]:\n            converted_data = data * 8\n        else:\n            raise ValueError(\n                \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n            )\n        return converted_data\n\n    def cache_forcing_np_json(self):\n        \"\"\"\n        Save all daymet basin-forcing data in a numpy array file in the cache directory.\n\n        Because it takes much time to read data from txt files,\n        it is a good way to cache data as a numpy file to speed up the reading.\n        In addition, we need a document to explain the meaning of all dimensions.\n\n        TODO: now only support CAMELS-US\n        \"\"\"\n        cache_npy_file = self.cache_dir.joinpath(\"camels_daymet_forcing.npy\")\n        json_file = self.cache_dir.joinpath(\"camels_daymet_forcing.json\")\n        variables = self.get_relevant_cols()\n        basins = self.sites[\"gauge_id\"].values\n        daymet_t_range = [\"1980-01-01\", \"2015-01-01\"]\n        times = [\n            hydro_time.t2str(tmp)\n            for tmp in hydro_time.t_range_days(daymet_t_range).tolist()\n        ]\n        data_info = collections.OrderedDict(\n            {\n                \"dim\": [\"basin\", \"time\", \"variable\"],\n                \"basin\": basins.tolist(),\n                \"time\": times,\n                \"variable\": variables.tolist(),\n            }\n        )\n        with open(json_file, \"w\") as FP:\n            json.dump(data_info, FP, indent=4)\n        data = self.read_relevant_cols(\n            gage_id_lst=basins.tolist(),\n            t_range=daymet_t_range,\n            var_lst=variables.tolist(),\n        )\n        np.save(cache_npy_file, data)\n\n    def cache_streamflow_np_json(self):\n        \"\"\"\n        Save all basins' streamflow data in a numpy array file in the cache directory\n\n        TODO: now only support CAMELS-US\n        \"\"\"\n        cache_npy_file = self.cache_dir.joinpath(\"camels_streamflow.npy\")\n        json_file = self.cache_dir.joinpath(\"camels_streamflow.json\")\n        variables = self.get_target_cols()\n        basins = self.sites[\"gauge_id\"].values\n        t_range = [\"1980-01-01\", \"2015-01-01\"]\n        times = [\n            hydro_time.t2str(tmp) for tmp in hydro_time.t_range_days(t_range).tolist()\n        ]\n        data_info = collections.OrderedDict(\n            {\n                \"dim\": [\"basin\", \"time\", \"variable\"],\n                \"basin\": basins.tolist(),\n                \"time\": times,\n                \"variable\": variables.tolist(),\n            }\n        )\n        with open(json_file, \"w\") as FP:\n            json.dump(data_info, FP, indent=4)\n        data = self.read_target_cols(\n            gage_id_lst=basins,\n            t_range=t_range,\n            target_cols=variables,\n        )\n        np.save(cache_npy_file, data)\n\n    def cache_attributes_xrdataset(self):\n        \"\"\"Convert all the attributes to a single dataframe and save as a netcdf file\n        TODO: now only support CAMELS-US\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # NOTICE: although it seems that we don't use pint_xarray, we have to import this package\n        import pint_xarray\n\n        attr_files = self.data_source_dir.glob(\"camels_*.txt\")\n        attrs = {\n            f.stem.split(\"_\")[1]: pd.read_csv(\n                f, sep=\";\", index_col=0, dtype={\"huc_02\": str, \"gauge_id\": str}\n            )\n            for f in attr_files\n        }\n\n        attrs_df = pd.concat(attrs.values(), axis=1)\n\n        # fix station names\n        def fix_station_nm(station_nm):\n            name = station_nm.title().rsplit(\" \", 1)\n            name[0] = name[0] if name[0][-1] == \",\" else f\"{name[0]},\"\n            name[1] = name[1].replace(\".\", \"\")\n            return \" \".join(\n                (name[0], name[1].upper() if len(name[1]) == 2 else name[1].title())\n            )\n\n        attrs_df[\"gauge_name\"] = [fix_station_nm(n) for n in attrs_df[\"gauge_name\"]]\n        obj_cols = attrs_df.columns[attrs_df.dtypes == \"object\"]\n        for c in obj_cols:\n            attrs_df[c] = attrs_df[c].str.strip().astype(str)\n\n        # transform categorical variables to numeric\n        categorical_mappings = {}\n        for column in attrs_df.columns:\n            if attrs_df[column].dtype == \"object\":\n                attrs_df[column] = attrs_df[column].astype(\"category\")\n                categorical_mappings[column] = dict(\n                    enumerate(attrs_df[column].cat.categories)\n                )\n                attrs_df[column] = attrs_df[column].cat.codes\n\n        # unify id to basin\n        attrs_df.index.name = \"basin\"\n        # We use xarray dataset to cache all data\n        ds_from_df = attrs_df.to_xarray()\n\n        # Rename variables to match standardized names\n        var_name_mapping = {\n            \"area_gages2\": \"area\",\n            \"area_geospa_fabric\": \"area_geospa_fabric\",  # Keep this one as is\n        }\n        ds_from_df = ds_from_df.rename({\n            old_name: new_name\n            for old_name, new_name in var_name_mapping.items()\n            if old_name in ds_from_df.data_vars\n        })\n\n        units_dict = {\n            \"gauge_lat\": \"degree\",\n            \"gauge_lon\": \"degree\",\n            \"elev_mean\": \"m\",\n            \"slope_mean\": \"m/km\",\n            \"area\": \"km^2\",\n            \"geol_1st_class\": \"dimensionless\",\n            \"glim_1st_class_frac\": \"dimensionless\",\n            \"geol_2nd_class\": \"dimensionless\",\n            \"glim_2nd_class_frac\": \"dimensionless\",\n            \"carbonate_rocks_frac\": \"dimensionless\",\n            \"geol_porostiy\": \"dimensionless\",\n            \"geol_permeability\": \"m^2\",\n            \"frac_forest\": \"dimensionless\",\n            \"lai_max\": \"dimensionless\",\n            \"lai_diff\": \"dimensionless\",\n            \"gvf_max\": \"dimensionless\",\n            \"gvf_diff\": \"dimensionless\",\n            \"dom_land_cover_frac\": \"dimensionless\",\n            \"dom_land_cover\": \"dimensionless\",\n            \"root_depth_50\": \"m\",\n            \"root_depth_99\": \"m\",\n            \"q_mean\": \"mm/day\",\n            \"runoff_ratio\": \"dimensionless\",\n            \"slope_fdc\": \"dimensionless\",\n            \"baseflow_index\": \"dimensionless\",\n            \"stream_elas\": \"dimensionless\",\n            \"q5\": \"mm/day\",\n            \"q95\": \"mm/day\",\n            \"high_q_freq\": \"day/year\",\n            \"high_q_dur\": \"day\",\n            \"low_q_freq\": \"day/year\",\n            \"low_q_dur\": \"day\",\n            \"zero_q_freq\": \"percent\",\n            \"hfd_mean\": \"dimensionless\",\n            \"soil_depth_pelletier\": \"m\",\n            \"soil_depth_statsgo\": \"m\",\n            \"soil_porosity\": \"dimensionless\",\n            \"soil_conductivity\": \"cm/hr\",\n            \"max_water_content\": \"m\",\n            \"sand_frac\": \"percent\",\n            \"silt_frac\": \"percent\",\n            \"clay_frac\": \"percent\",\n            \"water_frac\": \"percent\",\n            \"organic_frac\": \"percent\",\n            \"other_frac\": \"percent\",\n            \"p_mean\": \"mm/day\",\n            \"pet_mean\": \"mm/day\",\n            \"p_seasonality\": \"dimensionless\",\n            \"frac_snow\": \"dimensionless\",\n            \"aridity\": \"dimensionless\",\n            \"high_prec_freq\": \"days/year\",\n            \"high_prec_dur\": \"day\",\n            \"high_prec_timing\": \"dimensionless\",\n            \"low_prec_freq\": \"days/year\",\n            \"low_prec_dur\": \"day\",\n            \"low_prec_timing\": \"dimensionless\",\n            \"huc_02\": \"dimensionless\",\n            \"gauge_name\": \"dimensionless\",\n        }\n\n        # Assign units to the variables in the Dataset\n        for var_name in units_dict:\n            if var_name in ds_from_df.data_vars:\n                ds_from_df[var_name].attrs[\"units\"] = units_dict[var_name]\n\n        # Assign categorical mappings to the variables in the Dataset\n        for column in ds_from_df.data_vars:\n            if column in categorical_mappings:\n                mapping_str = categorical_mappings[column]\n                ds_from_df[column].attrs[\"category_mapping\"] = str(mapping_str)\n        ds_from_df.to_netcdf(self.cache_dir.joinpath(self._attributes_cache_filename))\n\n    def cache_timeseries_xrdataset(self):\n        warnings.warn(\"Check you units of all variables\")\n        ds_streamflow = self._cache_streamflow_xrdataset()\n        ds_forcing = self._cache_forcing_xrdataset()\n        ds = xr.merge([ds_streamflow, ds_forcing])\n        ds.to_netcdf(self.cache_dir.joinpath(self._timeseries_cache_filename))\n\n    def _cache_streamflow_xrdataset(self):\n        \"\"\"Save all basins' streamflow data in a netcdf file in the cache directory\n\n        TODO: ONLY SUPPORT CAMELS-US now\n        \"\"\"\n        cache_npy_file = self.cache_dir.joinpath(\"camels_streamflow.npy\")\n        json_file = self.cache_dir.joinpath(\"camels_streamflow.json\")\n        if (not os.path.isfile(cache_npy_file)) or (not os.path.isfile(json_file)):\n            self.cache_streamflow_np_json()\n        streamflow = np.load(cache_npy_file)\n        with open(json_file, \"r\") as fp:\n            streamflow_dict = json.load(fp, object_pairs_hook=collections.OrderedDict)\n        import pint_xarray\n\n        basins = streamflow_dict[\"basin\"]\n        times = pd.date_range(\n            streamflow_dict[\"time\"][0], periods=len(streamflow_dict[\"time\"])\n        )\n        return xr.Dataset(\n            {\n                \"q_cms_obs\": (\n                    [\"basin\", \"time\"],\n                    streamflow[:, :, 0],\n                    {\"units\": self.streamflow_unit},\n                ),\n                \"ET\": (\n                    [\"basin\", \"time\"],\n                    streamflow[:, :, 1],\n                    {\"units\": \"mm/day\"},\n                ),\n            },\n            coords={\n                \"basin\": basins,\n                \"time\": times,\n            },\n        )\n\n    def _cache_forcing_xrdataset(self):\n        \"\"\"Save all daymet basin-forcing data in a netcdf file in the cache directory.\n\n        TODO: ONLY SUPPORT CAMELS-US now\n        \"\"\"\n        cache_npy_file = self.cache_dir.joinpath(\"camels_daymet_forcing.npy\")\n        json_file = self.cache_dir.joinpath(\"camels_daymet_forcing.json\")\n        if (not os.path.isfile(cache_npy_file)) or (not os.path.isfile(json_file)):\n            self.cache_forcing_np_json()\n        daymet_forcing = np.load(cache_npy_file)\n        with open(json_file, \"r\") as fp:\n            daymet_forcing_dict = json.load(\n                fp, object_pairs_hook=collections.OrderedDict\n            )\n        import pint_xarray\n\n        basins = daymet_forcing_dict[\"basin\"]\n        times = pd.date_range(\n            daymet_forcing_dict[\"time\"][0], periods=len(daymet_forcing_dict[\"time\"])\n        )\n        variables = daymet_forcing_dict[\"variable\"]\n        # All units' names are from Pint https://github.com/hgrecco/pint/blob/master/pint/default_en.txt\n        # final is PET's unit. PET comes from the model output of CAMELS-US\n        # Order matches get_relevant_cols(): dayl, pcp_mm, solrad_wm2, swe_mm, airtemp_c_max, airtemp_c_min, vp_hpa, PET\n        units = [\"s\", \"mm/day\", \"W/m^2\", \"mm/day\", \"\u00b0C\", \"\u00b0C\", \"hPa\", \"mm/day\"]\n        return xr.Dataset(\n            data_vars={\n                **{\n                    variables[i]: (\n                        [\"basin\", \"time\"],\n                        daymet_forcing[:, :, i],\n                        {\"units\": units[i]},\n                    )\n                    for i in range(len(variables))\n                }\n            },\n            coords={\n                \"basin\": basins,\n                \"time\": times,\n            },\n            attrs={\"forcing_type\": \"daymet\"},\n        )\n\n    @property\n    def streamflow_unit(self):\n        return \"m^3/s\"\n\n    def read_ts_xrdataset(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        **kwargs,\n    ):\n        if var_lst is None:\n            return None\n        camels_tsnc = self.cache_dir.joinpath(\"camelsus_timeseries.nc\")\n        if not os.path.isfile(camels_tsnc):\n            self.cache_xrdataset()\n        ts = xr.open_dataset(camels_tsnc)\n        all_vars = ts.data_vars\n\n        # Build variable name mapping from _dynamic_variable_mapping\n        # This supports both StandardVariable enums and string names\n        var_name_mapping = {}\n\n        # Add mappings from StandardVariable to specific_name\n        if hasattr(self, \"_dynamic_variable_mapping\"):\n            for std_var, mapping_info in self._dynamic_variable_mapping.items():\n                default_source = mapping_info[\"default_source\"]\n                specific_name = mapping_info[\"sources\"][default_source][\"specific_name\"]\n                var_name_mapping[std_var] = specific_name\n                var_name_mapping[std_var.value if hasattr(std_var, 'value') else str(std_var)] = specific_name\n\n        # Add backward compatibility for old variable names\n        old_to_new = {\n            \"prcp\": \"pcp_mm\",\n            \"srad\": \"solrad_wm2\",\n            \"swe\": \"swe_mm\",\n            \"tmax\": \"airtemp_c_max\",\n            \"tmin\": \"airtemp_c_min\",\n            \"vp\": \"vp_hpa\",\n            \"streamflow\": \"q_cms_obs\",\n        }\n        var_name_mapping.update(old_to_new)\n\n        # Convert variable names using the mapping\n        mapped_var_lst = [var_name_mapping.get(var, var) for var in var_lst]\n\n        if any(var not in ts.variables for var in mapped_var_lst):\n            raise ValueError(f\"var_lst must all be in {all_vars}\")\n        return ts[mapped_var_lst].sel(basin=gage_id_lst, time=slice(t_range[0], t_range[1]))\n\n    def cache_xrdataset(self):\n        \"\"\"Save all data in a netcdf file in the cache directory\"\"\"\n        warnings.warn(\"Check you units of all variables\")\n        self.cache_attributes_xrdataset()\n        ds_streamflow = self._cache_streamflow_xrdataset()\n        ds_forcing = self._cache_forcing_xrdataset()\n        ds = xr.merge([ds_streamflow, ds_forcing])\n        ds.to_netcdf(self.cache_dir.joinpath(\"camelsus_timeseries.nc\"))\n\n    def read_attr_xrdataset(self, gage_id_lst=None, var_lst=None, **kwargs):\n        if var_lst is None or len(var_lst) == 0:\n            return None\n        try:\n            attr = xr.open_dataset(self.cache_dir.joinpath(\"camelsus_attributes.nc\"))\n        except FileNotFoundError:\n            attr = self.cache_attributes_xrdataset()\n            attr.to_netcdf(self.cache_dir.joinpath(\"camelsus_attributes.nc\"))\n\n        # Build variable name mapping from _subclass_static_definitions\n        var_name_mapping = {}\n\n        # Add mappings from standard names to specific_name\n        if hasattr(self, \"_subclass_static_definitions\"):\n            for std_name, definition in self._subclass_static_definitions.items():\n                specific_name = definition[\"specific_name\"]\n                var_name_mapping[std_name] = specific_name\n\n        # Add backward compatibility for old attribute names\n        old_to_new = {\n            \"area_gages2\": \"area\",\n            \"area_geospa_fabric\": \"area\",  # Both map to area\n        }\n        var_name_mapping.update(old_to_new)\n\n        # Convert variable names using the mapping\n        mapped_var_lst = [var_name_mapping.get(var, var) for var in var_lst]\n\n        if \"all_number\" in list(kwargs.keys()) and kwargs[\"all_number\"]:\n            attr_num = map_string_vars(attr)\n            return attr_num[mapped_var_lst].sel(basin=gage_id_lst)\n        print(mapped_var_lst)\n        return attr[mapped_var_lst].sel(basin=gage_id_lst)\n\n    _subclass_static_definitions = {\n        \"huc_02\": {\"specific_name\": \"huc_02\", \"unit\": \"dimensionless\"},\n        \"gauge_lat\": {\"specific_name\": \"gauge_lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"gauge_lon\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"elev_mean\", \"unit\": \"m\"},\n        \"slope_mean\": {\"specific_name\": \"slope_mean\", \"unit\": \"m/km\"},\n        \"area\": {\"specific_name\": \"area\", \"unit\": \"km^2\"},\n        \"geol_1st_class\": {\"specific_name\": \"geol_1st_class\", \"unit\": \"dimensionless\"},\n        \"geol_2nd_class\": {\"specific_name\": \"geol_2nd_class\", \"unit\": \"dimensionless\"},\n        \"geol_porostiy\": {\"specific_name\": \"geol_porostiy\", \"unit\": \"dimensionless\"},\n        \"geol_permeability\": {\"specific_name\": \"geol_permeability\", \"unit\": \"m^2\"},\n        \"frac_forest\": {\"specific_name\": \"frac_forest\", \"unit\": \"dimensionless\"},\n        \"lai_max\": {\"specific_name\": \"lai_max\", \"unit\": \"dimensionless\"},\n        \"lai_diff\": {\"specific_name\": \"lai_diff\", \"unit\": \"dimensionless\"},\n        \"dom_land_cover_frac\": {\"specific_name\": \"dom_land_cover_frac\", \"unit\": \"dimensionless\"},\n        \"dom_land_cover\": {\"specific_name\": \"dom_land_cover\", \"unit\": \"dimensionless\"},\n        \"root_depth_50\": {\"specific_name\": \"root_depth_50\", \"unit\": \"m\"},\n        \"root_depth_99\": {\"specific_name\": \"root_depth_99\", \"unit\": \"m\"},\n        \"soil_depth_statsgo\": {\"specific_name\": \"soil_depth_statsgo\", \"unit\": \"m\"},\n        \"soil_porosity\": {\"specific_name\": \"soil_porosity\", \"unit\": \"dimensionless\"},\n        \"soil_conductivity\": {\"specific_name\": \"soil_conductivity\", \"unit\": \"cm/hr\"},\n        \"max_water_content\": {\"specific_name\": \"max_water_content\", \"unit\": \"m\"},\n        \"pet_mean\": {\"specific_name\": \"pet_mean\", \"unit\": \"mm/day\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"usgs\",\n            \"sources\": {\"usgs\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"}},\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.DAYLIGHT_DURATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"dayl\", \"unit\": \"s\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"swe_mm\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.VAPOR_PRESSURE: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"vp_hpa\", \"unit\": \"hPa\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"sac-sma\",\n            \"sources\": {\"sac-sma\": {\"specific_name\": \"PET\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"sac-sma\",\n            \"sources\": {\"sac-sma\": {\"specific_name\": \"ET\", \"unit\": \"mm/day\"}},\n        },\n    }\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.cache_attributes_xrdataset","title":"<code>cache_attributes_xrdataset()</code>","text":"<p>Convert all the attributes to a single dataframe and save as a netcdf file TODO: now only support CAMELS-US</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.cache_attributes_xrdataset--returns","title":"Returns","text":"<p>None</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def cache_attributes_xrdataset(self):\n    \"\"\"Convert all the attributes to a single dataframe and save as a netcdf file\n    TODO: now only support CAMELS-US\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # NOTICE: although it seems that we don't use pint_xarray, we have to import this package\n    import pint_xarray\n\n    attr_files = self.data_source_dir.glob(\"camels_*.txt\")\n    attrs = {\n        f.stem.split(\"_\")[1]: pd.read_csv(\n            f, sep=\";\", index_col=0, dtype={\"huc_02\": str, \"gauge_id\": str}\n        )\n        for f in attr_files\n    }\n\n    attrs_df = pd.concat(attrs.values(), axis=1)\n\n    # fix station names\n    def fix_station_nm(station_nm):\n        name = station_nm.title().rsplit(\" \", 1)\n        name[0] = name[0] if name[0][-1] == \",\" else f\"{name[0]},\"\n        name[1] = name[1].replace(\".\", \"\")\n        return \" \".join(\n            (name[0], name[1].upper() if len(name[1]) == 2 else name[1].title())\n        )\n\n    attrs_df[\"gauge_name\"] = [fix_station_nm(n) for n in attrs_df[\"gauge_name\"]]\n    obj_cols = attrs_df.columns[attrs_df.dtypes == \"object\"]\n    for c in obj_cols:\n        attrs_df[c] = attrs_df[c].str.strip().astype(str)\n\n    # transform categorical variables to numeric\n    categorical_mappings = {}\n    for column in attrs_df.columns:\n        if attrs_df[column].dtype == \"object\":\n            attrs_df[column] = attrs_df[column].astype(\"category\")\n            categorical_mappings[column] = dict(\n                enumerate(attrs_df[column].cat.categories)\n            )\n            attrs_df[column] = attrs_df[column].cat.codes\n\n    # unify id to basin\n    attrs_df.index.name = \"basin\"\n    # We use xarray dataset to cache all data\n    ds_from_df = attrs_df.to_xarray()\n\n    # Rename variables to match standardized names\n    var_name_mapping = {\n        \"area_gages2\": \"area\",\n        \"area_geospa_fabric\": \"area_geospa_fabric\",  # Keep this one as is\n    }\n    ds_from_df = ds_from_df.rename({\n        old_name: new_name\n        for old_name, new_name in var_name_mapping.items()\n        if old_name in ds_from_df.data_vars\n    })\n\n    units_dict = {\n        \"gauge_lat\": \"degree\",\n        \"gauge_lon\": \"degree\",\n        \"elev_mean\": \"m\",\n        \"slope_mean\": \"m/km\",\n        \"area\": \"km^2\",\n        \"geol_1st_class\": \"dimensionless\",\n        \"glim_1st_class_frac\": \"dimensionless\",\n        \"geol_2nd_class\": \"dimensionless\",\n        \"glim_2nd_class_frac\": \"dimensionless\",\n        \"carbonate_rocks_frac\": \"dimensionless\",\n        \"geol_porostiy\": \"dimensionless\",\n        \"geol_permeability\": \"m^2\",\n        \"frac_forest\": \"dimensionless\",\n        \"lai_max\": \"dimensionless\",\n        \"lai_diff\": \"dimensionless\",\n        \"gvf_max\": \"dimensionless\",\n        \"gvf_diff\": \"dimensionless\",\n        \"dom_land_cover_frac\": \"dimensionless\",\n        \"dom_land_cover\": \"dimensionless\",\n        \"root_depth_50\": \"m\",\n        \"root_depth_99\": \"m\",\n        \"q_mean\": \"mm/day\",\n        \"runoff_ratio\": \"dimensionless\",\n        \"slope_fdc\": \"dimensionless\",\n        \"baseflow_index\": \"dimensionless\",\n        \"stream_elas\": \"dimensionless\",\n        \"q5\": \"mm/day\",\n        \"q95\": \"mm/day\",\n        \"high_q_freq\": \"day/year\",\n        \"high_q_dur\": \"day\",\n        \"low_q_freq\": \"day/year\",\n        \"low_q_dur\": \"day\",\n        \"zero_q_freq\": \"percent\",\n        \"hfd_mean\": \"dimensionless\",\n        \"soil_depth_pelletier\": \"m\",\n        \"soil_depth_statsgo\": \"m\",\n        \"soil_porosity\": \"dimensionless\",\n        \"soil_conductivity\": \"cm/hr\",\n        \"max_water_content\": \"m\",\n        \"sand_frac\": \"percent\",\n        \"silt_frac\": \"percent\",\n        \"clay_frac\": \"percent\",\n        \"water_frac\": \"percent\",\n        \"organic_frac\": \"percent\",\n        \"other_frac\": \"percent\",\n        \"p_mean\": \"mm/day\",\n        \"pet_mean\": \"mm/day\",\n        \"p_seasonality\": \"dimensionless\",\n        \"frac_snow\": \"dimensionless\",\n        \"aridity\": \"dimensionless\",\n        \"high_prec_freq\": \"days/year\",\n        \"high_prec_dur\": \"day\",\n        \"high_prec_timing\": \"dimensionless\",\n        \"low_prec_freq\": \"days/year\",\n        \"low_prec_dur\": \"day\",\n        \"low_prec_timing\": \"dimensionless\",\n        \"huc_02\": \"dimensionless\",\n        \"gauge_name\": \"dimensionless\",\n    }\n\n    # Assign units to the variables in the Dataset\n    for var_name in units_dict:\n        if var_name in ds_from_df.data_vars:\n            ds_from_df[var_name].attrs[\"units\"] = units_dict[var_name]\n\n    # Assign categorical mappings to the variables in the Dataset\n    for column in ds_from_df.data_vars:\n        if column in categorical_mappings:\n            mapping_str = categorical_mappings[column]\n            ds_from_df[column].attrs[\"category_mapping\"] = str(mapping_str)\n    ds_from_df.to_netcdf(self.cache_dir.joinpath(self._attributes_cache_filename))\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.cache_forcing_np_json","title":"<code>cache_forcing_np_json()</code>","text":"<p>Save all daymet basin-forcing data in a numpy array file in the cache directory.</p> <p>Because it takes much time to read data from txt files, it is a good way to cache data as a numpy file to speed up the reading. In addition, we need a document to explain the meaning of all dimensions.</p> <p>TODO: now only support CAMELS-US</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def cache_forcing_np_json(self):\n    \"\"\"\n    Save all daymet basin-forcing data in a numpy array file in the cache directory.\n\n    Because it takes much time to read data from txt files,\n    it is a good way to cache data as a numpy file to speed up the reading.\n    In addition, we need a document to explain the meaning of all dimensions.\n\n    TODO: now only support CAMELS-US\n    \"\"\"\n    cache_npy_file = self.cache_dir.joinpath(\"camels_daymet_forcing.npy\")\n    json_file = self.cache_dir.joinpath(\"camels_daymet_forcing.json\")\n    variables = self.get_relevant_cols()\n    basins = self.sites[\"gauge_id\"].values\n    daymet_t_range = [\"1980-01-01\", \"2015-01-01\"]\n    times = [\n        hydro_time.t2str(tmp)\n        for tmp in hydro_time.t_range_days(daymet_t_range).tolist()\n    ]\n    data_info = collections.OrderedDict(\n        {\n            \"dim\": [\"basin\", \"time\", \"variable\"],\n            \"basin\": basins.tolist(),\n            \"time\": times,\n            \"variable\": variables.tolist(),\n        }\n    )\n    with open(json_file, \"w\") as FP:\n        json.dump(data_info, FP, indent=4)\n    data = self.read_relevant_cols(\n        gage_id_lst=basins.tolist(),\n        t_range=daymet_t_range,\n        var_lst=variables.tolist(),\n    )\n    np.save(cache_npy_file, data)\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.cache_streamflow_np_json","title":"<code>cache_streamflow_np_json()</code>","text":"<p>Save all basins' streamflow data in a numpy array file in the cache directory</p> <p>TODO: now only support CAMELS-US</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def cache_streamflow_np_json(self):\n    \"\"\"\n    Save all basins' streamflow data in a numpy array file in the cache directory\n\n    TODO: now only support CAMELS-US\n    \"\"\"\n    cache_npy_file = self.cache_dir.joinpath(\"camels_streamflow.npy\")\n    json_file = self.cache_dir.joinpath(\"camels_streamflow.json\")\n    variables = self.get_target_cols()\n    basins = self.sites[\"gauge_id\"].values\n    t_range = [\"1980-01-01\", \"2015-01-01\"]\n    times = [\n        hydro_time.t2str(tmp) for tmp in hydro_time.t_range_days(t_range).tolist()\n    ]\n    data_info = collections.OrderedDict(\n        {\n            \"dim\": [\"basin\", \"time\", \"variable\"],\n            \"basin\": basins.tolist(),\n            \"time\": times,\n            \"variable\": variables.tolist(),\n        }\n    )\n    with open(json_file, \"w\") as FP:\n        json.dump(data_info, FP, indent=4)\n    data = self.read_target_cols(\n        gage_id_lst=basins,\n        t_range=t_range,\n        target_cols=variables,\n    )\n    np.save(cache_npy_file, data)\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.cache_xrdataset","title":"<code>cache_xrdataset()</code>","text":"<p>Save all data in a netcdf file in the cache directory</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def cache_xrdataset(self):\n    \"\"\"Save all data in a netcdf file in the cache directory\"\"\"\n    warnings.warn(\"Check you units of all variables\")\n    self.cache_attributes_xrdataset()\n    ds_streamflow = self._cache_streamflow_xrdataset()\n    ds_forcing = self._cache_forcing_xrdataset()\n    ds = xr.merge([ds_streamflow, ds_forcing])\n    ds.to_netcdf(self.cache_dir.joinpath(\"camelsus_timeseries.nc\"))\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.download_data_source","title":"<code>download_data_source()</code>","text":"<p>Download the required zip files</p> <p>Now we only support CAMELS-US's downloading. For others, please download it manually, and put all files of a dataset in one directory. For example, all files of CAMELS_AUS should be put in \"camels_aus\"</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.download_data_source--returns","title":"Returns","text":"<p>None</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def download_data_source(self) -&gt; None:\n    \"\"\"\n    Download the required zip files\n\n    Now we only support CAMELS-US's downloading.\n    For others, please download it manually,\n    and put all files of a dataset in one directory.\n    For example, all files of CAMELS_AUS should be put in \"camels_aus\"\n\n    Returns\n    -------\n    None\n    \"\"\"\n    camels_config = self.data_source_description\n    self.data_source_dir.mkdir(exist_ok=True)\n    links = camels_config[\"CAMELS_DOWNLOAD_URL_LST\"]\n    for url in links:\n        fzip = Path(self.data_source_dir, url.rsplit(\"/\", 1)[1])\n        if fzip.exists():\n            with urlopen(url) as response:\n                if int(response.info()[\"Content-length\"]) != fzip.stat().st_size:\n                    fzip.unlink()\n    to_dl = [\n        url\n        for url in links\n        if not Path(self.data_source_dir, url.rsplit(\"/\", 1)[1]).exists()\n    ]\n    hydro_file.download_zip_files(to_dl, self.data_source_dir)\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.get_constant_cols","title":"<code>get_constant_cols()</code>","text":"<p>all readable attrs in CAMELS</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.get_constant_cols--returns","title":"Returns","text":"<p>np.array     attribute types</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def get_constant_cols(self) -&gt; np.ndarray:\n    \"\"\"\n    all readable attrs in CAMELS\n\n    Returns\n    -------\n    np.array\n        attribute types\n    \"\"\"\n    data_folder = self.data_source_description[\"CAMELS_ATTR_DIR\"]\n    return self._get_constant_cols_some(data_folder, \"camels_\", \".txt\", \";\")\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.get_relevant_cols","title":"<code>get_relevant_cols()</code>","text":"<p>all readable forcing types</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.get_relevant_cols--returns","title":"Returns","text":"<p>np.array     forcing types</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def get_relevant_cols(self) -&gt; np.ndarray:\n    \"\"\"\n    all readable forcing types\n\n    Returns\n    -------\n    np.array\n        forcing types\n    \"\"\"\n    # PET is from model_output file in CAMELS-US\n    return np.array([\"dayl\", \"pcp_mm\", \"solrad_wm2\", \"swe_mm\", \"airtemp_c_max\", \"airtemp_c_min\", \"vp_hpa\", \"PET\"])\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.get_target_cols","title":"<code>get_target_cols()</code>","text":"<p>For CAMELS, the target vars are streamflows</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.get_target_cols--returns","title":"Returns","text":"<p>np.array     streamflow types</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def get_target_cols(self) -&gt; np.ndarray:\n    \"\"\"\n    For CAMELS, the target vars are streamflows\n\n    Returns\n    -------\n    np.array\n        streamflow types\n    \"\"\"\n    return np.array([\"q_cms_obs\", \"ET\"])\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_camels_streamflow","title":"<code>read_camels_streamflow(usgs_id, t_range)</code>","text":"<p>read streamflow data of a station for date after 2015 from CAMELS-US</p> <p>The streamflow data is downloaded from USGS website by HyRivers tools</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_camels_streamflow--parameters","title":"Parameters","text":"<p>usgs_id     the station id t_range     the time range, for example, [\"2015-01-01\", \"2022-01-01\"]</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_camels_streamflow--returns","title":"Returns","text":"<p>np.array     streamflow data of one station for a given time range     Unit: m^3/s (converted from original foot^3/s)</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_camels_streamflow(self, usgs_id, t_range):\n    \"\"\"\n    read streamflow data of a station for date after 2015 from CAMELS-US\n\n    The streamflow data is downloaded from USGS website by HyRivers tools\n\n    Parameters\n    ----------\n    usgs_id\n        the station id\n    t_range\n        the time range, for example, [\"2015-01-01\", \"2022-01-01\"]\n\n    Returns\n    -------\n    np.array\n        streamflow data of one station for a given time range\n        Unit: m^3/s (converted from original foot^3/s)\n    \"\"\"\n    logging.debug(\"reading %s streamflow data after 2015\", usgs_id)\n    gage_id_df = self.sites\n    huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n    usgs_file = os.path.join(\n        self.data_source_description[\"CAMELS_FLOW_AFTER2015_DIR\"],\n        huc,\n        usgs_id + \"_streamflow_qc.txt\",\n    )\n    data_temp = pd.read_csv(usgs_file, sep=\",\", header=None, skiprows=1)\n    obs = data_temp[4].values\n    obs[obs &lt; 0] = np.nan\n\n    # Convert from foot^3/s to m^3/s\n    # Conversion factor: 1 foot^3/s = 0.0283168 m^3/s\n    obs = obs * 0.0283168\n\n    t_lst = hydro_time.t_range_days(t_range)\n    nt = t_lst.shape[0]\n    return (\n        self._read_usgs_gage_for_some(nt, data_temp, t_lst, obs)\n        if len(obs) != nt\n        else obs\n    )\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_camels_us_model_output_data","title":"<code>read_camels_us_model_output_data(gage_id_lst=None, t_range=None, var_lst=None, forcing_type='daymet')</code>","text":"<p>Read model output data of CAMELS-US, including SWE, PRCP, RAIM, TAIR, PET, ET, MOD_RUN, OBS_RUN Date starts from 1980-10-01 to 2014-12-31</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_camels_us_model_output_data--parameters","title":"Parameters","text":"<p>gage_id_lst : list     the station id list var_lst : list     the variable list t_range : list     the time range, for example, [\"1990-01-01\", \"2000-01-01\"] forcing_type : str, optional     by default \"daymet\"</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_camels_us_model_output_data(\n    self,\n    gage_id_lst: list = None,\n    t_range: list = None,\n    var_lst: list = None,\n    forcing_type=\"daymet\",\n) -&gt; np.array:\n    \"\"\"\n    Read model output data of CAMELS-US, including SWE, PRCP, RAIM, TAIR, PET, ET, MOD_RUN, OBS_RUN\n    Date starts from 1980-10-01 to 2014-12-31\n\n    Parameters\n    ----------\n    gage_id_lst : list\n        the station id list\n    var_lst : list\n        the variable list\n    t_range : list\n        the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n    forcing_type : str, optional\n        by default \"daymet\"\n    \"\"\"\n    t_range_list = hydro_time.t_range_days(t_range)\n    model_out_put_var_lst = [\n        \"SWE\",\n        \"PRCP\",\n        \"RAIM\",\n        \"TAIR\",\n        \"PET\",\n        \"ET\",\n        \"MOD_RUN\",\n        \"OBS_RUN\",\n    ]\n    if not set(var_lst).issubset(set(model_out_put_var_lst)):\n        raise RuntimeError(\"not in this list\")\n    nt = t_range_list.shape[0]\n    chosen_camels_mods = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n    count = 0\n    for usgs_id in tqdm(gage_id_lst, desc=\"Read model output data of CAMELS-US\"):\n        gage_id_df = self.sites\n        huc02_ = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n        file_path_dir = os.path.join(\n            self.data_source_dir,\n            \"basin_timeseries_v1p2_modelOutput_\" + forcing_type,\n            \"model_output_\" + forcing_type,\n            \"model_output\",\n            \"flow_timeseries\",\n            forcing_type,\n            huc02_,\n        )\n        sac_random_seeds = [\n            \"05\",\n            \"11\",\n            \"27\",\n            \"33\",\n            \"48\",\n            \"59\",\n            \"66\",\n            \"72\",\n            \"80\",\n            \"94\",\n        ]\n        files = [\n            os.path.join(\n                file_path_dir, usgs_id + \"_\" + random_seed + \"_model_output.txt\"\n            )\n            for random_seed in sac_random_seeds\n        ]\n        results = []\n        for file in files:\n            result = pd.read_csv(file, sep=\"\\s+\")\n            df_date = result[[\"YR\", \"MNTH\", \"DY\"]]\n            df_date.columns = [\"year\", \"month\", \"day\"]\n            date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n            [c, ind1, ind2] = np.intersect1d(\n                date, t_range_list, return_indices=True\n            )\n            results.append(result[var_lst].values[ind1])\n        result_np = np.array(results)\n        chosen_camels_mods[count, ind2, :] = np.mean(result_np, axis=0)\n        count = count + 1\n    return chosen_camels_mods\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_constant_cols","title":"<code>read_constant_cols(gage_id_lst=None, var_lst=None, is_return_dict=False, **kwargs)</code>","text":"<p>Read Attributes data</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_constant_cols--parameters","title":"Parameters","text":"<p>gage_id_lst     station ids var_lst     attribute variable types is_return_dict     if true, return var_dict and f_dict for CAMELS_US Returns</p> <p>Union[tuple, np.array]     if attr var type is str, return factorized data.     When we need to know what a factorized value represents, we need return a tuple;     otherwise just return an array</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_constant_cols(\n    self, gage_id_lst=None, var_lst=None, is_return_dict=False, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"\n    Read Attributes data\n\n    Parameters\n    ----------\n    gage_id_lst\n        station ids\n    var_lst\n        attribute variable types\n    is_return_dict\n        if true, return var_dict and f_dict for CAMELS_US\n    Returns\n    -------\n    Union[tuple, np.array]\n        if attr var type is str, return factorized data.\n        When we need to know what a factorized value represents, we need return a tuple;\n        otherwise just return an array\n    \"\"\"\n    attr_all, var_lst_all, var_dict, f_dict = self.read_attr_all()\n    ind_var = [var_lst_all.index(var) for var in var_lst]\n    id_lst_all = self.read_object_ids()\n    # Notice the sequence of station ids ! Some id_lst_all are not sorted, so don't use np.intersect1d\n    ind_grid = [id_lst_all.tolist().index(tmp) for tmp in gage_id_lst]\n    temp = attr_all[ind_grid, :]\n    out = temp[:, ind_var]\n    return (out, var_dict, f_dict) if is_return_dict else out\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_mean_prcp","title":"<code>read_mean_prcp(gage_id_lst, unit='mm/d')</code>","text":"<p>Read mean precipitation data</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_mean_prcp--parameters","title":"Parameters","text":"<p>gage_id_lst : list     station ids unit : str, optional     the unit of mean_prcp, by default \"mm/d\"</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_mean_prcp--returns","title":"Returns","text":"<p>xr.Dataset     TODO: now only support CAMELS-US</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_mean_prcp--raises","title":"Raises","text":"<p>NotImplementedError     some regions are not supported ValueError     unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_mean_prcp(self, gage_id_lst, unit=\"mm/d\") -&gt; xr.Dataset:\n    \"\"\"Read mean precipitation data\n\n    Parameters\n    ----------\n    gage_id_lst : list\n        station ids\n    unit : str, optional\n        the unit of mean_prcp, by default \"mm/d\"\n\n    Returns\n    -------\n    xr.Dataset\n        TODO: now only support CAMELS-US\n\n    Raises\n    ------\n    NotImplementedError\n        some regions are not supported\n    ValueError\n        unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\n    \"\"\"\n    data = self.read_attr_xrdataset(gage_id_lst, [\"p_mean\"])\n    if unit in [\"mm/d\", \"mm/day\"]:\n        converted_data = data\n    elif unit in [\"mm/h\", \"mm/hour\"]:\n        converted_data = data / 24\n    elif unit in [\"mm/3h\", \"mm/3hour\"]:\n        converted_data = data / 8\n    elif unit in [\"mm/8d\", \"mm/8day\"]:\n        converted_data = data * 8\n    else:\n        raise ValueError(\n            \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n        )\n    return converted_data\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_object_ids","title":"<code>read_object_ids(**kwargs)</code>","text":"<p>read station ids</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_object_ids--parameters","title":"Parameters","text":"<p>**kwargs     optional params if needed</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_object_ids--returns","title":"Returns","text":"<p>np.array     gage/station ids</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_object_ids(self, **kwargs) -&gt; np.ndarray:\n    \"\"\"\n    read station ids\n\n    Parameters\n    ----------\n    **kwargs\n        optional params if needed\n\n    Returns\n    -------\n    np.array\n        gage/station ids\n    \"\"\"\n    return np.sort(self.sites[\"gauge_id\"].values)\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_relevant_cols","title":"<code>read_relevant_cols(gage_id_lst=None, t_range=None, var_lst=None, forcing_type='daymet', **kwargs)</code>","text":"<p>Read forcing data</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_relevant_cols--parameters","title":"Parameters","text":"<p>gage_id_lst     station ids t_range     the time range, for example, [\"1990-01-01\", \"2000-01-01\"] var_lst     forcing variable types forcing_type     now only for CAMELS-US, there are three types: daymet, nldas, maurer Returns</p> <p>np.array     forcing data</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_relevant_cols(\n    self,\n    gage_id_lst: list = None,\n    t_range: list = None,\n    var_lst: list = None,\n    forcing_type=\"daymet\",\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Read forcing data\n\n    Parameters\n    ----------\n    gage_id_lst\n        station ids\n    t_range\n        the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n    var_lst\n        forcing variable types\n    forcing_type\n        now only for CAMELS-US, there are three types: daymet, nldas, maurer\n    Returns\n    -------\n    np.array\n        forcing data\n    \"\"\"\n    t_range_list = hydro_time.t_range_days(t_range)\n    nt = t_range_list.shape[0]\n    x = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n    for k in tqdm(range(len(gage_id_lst)), desc=\"Read forcing data of CAMELS-US\"):\n        if \"PET\" in var_lst:\n            pet_idx = var_lst.index(\"PET\")\n            data_pet = self.read_camels_us_model_output_data(\n                gage_id_lst[k : k + 1], t_range, [\"PET\"]\n            )\n            x[k, :, pet_idx : pet_idx + 1] = data_pet\n            no_pet_var_lst = [x for x in var_lst if x != \"PET\"]\n            data = self.read_forcing_gage(\n                gage_id_lst[k],\n                no_pet_var_lst,\n                t_range_list,\n                forcing_type=forcing_type,\n            )\n            var_indices = [var_lst.index(var) for var in no_pet_var_lst]\n            x[k : k + 1, :, var_indices] = data\n        else:\n            data = self.read_forcing_gage(\n                gage_id_lst[k],\n                var_lst,\n                t_range_list,\n                forcing_type=forcing_type,\n            )\n            x[k, :, :] = data\n    return x\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_site_info","title":"<code>read_site_info()</code>","text":"<p>Read the basic information of gages in a CAMELS dataset</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_site_info--returns","title":"Returns","text":"<p>pd.DataFrame     basic info of gages</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_site_info(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Read the basic information of gages in a CAMELS dataset\n\n    Returns\n    -------\n    pd.DataFrame\n        basic info of gages\n    \"\"\"\n    camels_file = self.data_source_description[\"CAMELS_GAUGE_FILE\"]\n    return pd.read_csv(camels_file, sep=\";\", dtype={\"gauge_id\": str, \"huc_02\": str})\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_target_cols","title":"<code>read_target_cols(gage_id_lst=None, t_range=None, target_cols=None, **kwargs)</code>","text":"<p>read target values; for CAMELS, they are streamflows</p> <p>default target_cols is an one-value list Notice: the unit of target outputs in different regions are not totally same</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_target_cols--parameters","title":"Parameters","text":"<p>gage_id_lst     station ids t_range     the time range, for example, [\"1990-01-01\", \"2000-01-01\"] target_cols     the default is None, but we neea at least one default target.     For CAMELS-US, it is [\"usgsFlow\"]; kwargs     some other params if needed</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_target_cols--returns","title":"Returns","text":"<p>np.array     streamflow data, 3-dim [station, time, streamflow]</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_target_cols(\n    self,\n    gage_id_lst: Union[list, np.array] = None,\n    t_range: list = None,\n    target_cols: Union[list, np.array] = None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    read target values; for CAMELS, they are streamflows\n\n    default target_cols is an one-value list\n    Notice: the unit of target outputs in different regions are not totally same\n\n    Parameters\n    ----------\n    gage_id_lst\n        station ids\n    t_range\n        the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n    target_cols\n        the default is None, but we neea at least one default target.\n        For CAMELS-US, it is [\"usgsFlow\"];\n    kwargs\n        some other params if needed\n\n    Returns\n    -------\n    np.array\n        streamflow data, 3-dim [station, time, streamflow]\n    \"\"\"\n    if target_cols is None:\n        return np.array([])\n    else:\n        nf = len(target_cols)\n    t_range_list = hydro_time.t_range_days(t_range)\n    nt = t_range_list.shape[0]\n    y = np.full([len(gage_id_lst), nt, nf], np.nan)\n    for k in tqdm(\n        range(len(gage_id_lst)), desc=\"Read streamflow data of CAMELS-US\"\n    ):\n        for j in range(len(target_cols)):\n            if target_cols[j] == \"ET\":\n                data_et = self.read_camels_us_model_output_data(\n                    gage_id_lst[k : k + 1], t_range, [\"ET\"]\n                )\n                y[k, :, j : j + 1] = data_et\n            else:\n                data_obs = self._read_augmented_camels_streamflow(\n                    gage_id_lst, t_range, t_range_list, k\n                )\n                y[k, :, j] = data_obs\n    return y\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_usgs_gage","title":"<code>read_usgs_gage(usgs_id, t_range)</code>","text":"<p>read streamflow data of a station for date before 2015-01-01 from CAMELS-US</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_usgs_gage--parameters","title":"Parameters","text":"<p>usgs_id     the station id t_range     the time range, for example, [\"1990-01-01\", \"2000-01-01\"]</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.read_usgs_gage--returns","title":"Returns","text":"<p>np.array     streamflow data of one station for a given time range     Unit: m^3/s (converted from original foot^3/s)</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_usgs_gage(self, usgs_id, t_range):\n    \"\"\"\n    read streamflow data of a station for date before 2015-01-01 from CAMELS-US\n\n    Parameters\n    ----------\n    usgs_id\n        the station id\n    t_range\n        the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n\n    Returns\n    -------\n    np.array\n        streamflow data of one station for a given time range\n        Unit: m^3/s (converted from original foot^3/s)\n    \"\"\"\n    logging.debug(\"reading %s streamflow data before 2015\", usgs_id)\n    gage_id_df = self.sites\n    huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n    usgs_file = os.path.join(\n        self.data_source_description[\"CAMELS_FLOW_DIR\"],\n        huc,\n        usgs_id + \"_streamflow_qc.txt\",\n    )\n    data_temp = pd.read_csv(usgs_file, sep=r\"\\s+\", header=None)\n    obs = data_temp[4].values\n    obs[obs &lt; 0] = np.nan\n\n    # Convert from foot^3/s to m^3/s\n    # Conversion factor: 1 foot^3/s = 0.0283168 m^3/s\n    obs = obs * 0.0283168\n\n    t_lst = hydro_time.t_range_days(t_range)\n    nt = t_lst.shape[0]\n    return (\n        self._read_usgs_gage_for_some(nt, data_temp, t_lst, obs)\n        if len(obs) != nt\n        else obs\n    )\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.Camels.set_data_source_describe","title":"<code>set_data_source_describe()</code>","text":"<p>the files in the dataset and their location in file system</p>"},{"location":"api/hydrodataset/#hydrodataset.Camels.set_data_source_describe--returns","title":"Returns","text":"<p>collections.OrderedDict     the description for a CAMELS dataset</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def set_data_source_describe(self) -&gt; collections.OrderedDict:\n    \"\"\"\n    the files in the dataset and their location in file system\n\n    Returns\n    -------\n    collections.OrderedDict\n        the description for a CAMELS dataset\n    \"\"\"\n    camels_db = self.data_source_dir\n    return self._set_data_source_camelsus_describe(camels_db)\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.CamelsUs","title":"<code>CamelsUs</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_US dataset class.</p> <p>This class is a wrapper around the CAMELS_US class from the <code>aqua_fetch</code> package. It standardizes the dataset into a NetCDF format for easy use with hydrological models. It also includes custom logic to read the PET variable from model output files.</p> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>class CamelsUs(HydroDataset):\n    \"\"\"CAMELS_US dataset class.\n\n    This class is a wrapper around the CAMELS_US class from the `aqua_fetch` package.\n    It standardizes the dataset into a NetCDF format for easy use with hydrological models.\n    It also includes custom logic to read the PET variable from model output files.\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize CAMELS_US dataset.\n\n        Args:\n            data_path: Path to the CAMELS_US data directory. This is where the data will be stored.\n            region: Geographic region identifier (optional, defaults to US).\n            download: Whether to download data automatically (not used, handled by aqua_fetch).\n        \"\"\"\n        super().__init__(data_path)\n        self.region = \"US\" if region is None else region\n\n        # Instantiate the custom class defined at module level\n        self.aqua_fetch = CAMELS_US(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_us_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_us_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1980-01-01\", \"2014-12-31\"]\n\n    def _dynamic_features(self) -&gt; list:\n        \"\"\"\n        Overrides the base method to include 'PET' as a dynamic feature.\n        \"\"\"\n        # Get the default features from the parent class (from aquafetch)\n        features = super()._dynamic_features()\n        # Add the custom PET and ET variables\n        features.extend([\"PET\", \"ET\"])\n        return features\n\n    def read_camels_us_model_output_data(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        forcing_type=\"daymet\",\n    ) -&gt; np.array:\n        \"\"\"\n        Read model output data of CAMELS-US, including PET.\n        This is a legacy function migrated from the old camels.py.\n        \"\"\"\n        # Fetch HUC codes for the requested basins on-the-fly\n        try:\n            huc_ds = self.read_attr_xrdataset(\n                gage_id_lst=gage_id_lst, var_lst=[\"huc_02\"], to_numeric=False\n            )\n            huc_df = huc_ds.to_dataframe()\n        except Exception as e:\n            raise RuntimeError(\n                f\"Could not read HUC attributes to get model output data: {e}\"\n            )\n\n        t_range_list = pd.date_range(start=t_range[0], end=t_range[1], freq=\"D\").values\n        model_out_put_var_lst = [\n            \"SWE\",\n            \"PRCP\",\n            \"RAIM\",\n            \"TAIR\",\n            \"PET\",\n            \"ET\",\n            \"MOD_RUN\",\n            \"OBS_RUN\",\n        ]\n        if not set(var_lst).issubset(set(model_out_put_var_lst)):\n            raise RuntimeError(\n                f\"Requested variables not in model output list: {var_lst}\"\n            )\n\n        nt = len(t_range_list)\n        chosen_camels_mods = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n\n        for i, usgs_id in enumerate(\n            tqdm(gage_id_lst, desc=\"Read model output data (PET and ET) for CAMELS-US\")\n        ):\n            try:\n                huc02_ = huc_df.loc[usgs_id, \"huc_02\"]\n                # Convert to 2-digit string with leading zeros if needed\n                huc02_ = f\"{int(huc02_):02d}\"\n            except KeyError:\n                print(\n                    f\"Warning: No HUC attribute found for {usgs_id}, skipping PET and ET reading.\"\n                )\n                continue\n\n            # Construct path to model output files\n            file_path_dir = os.path.join(\n                self.data_source_dir,\n                \"CAMELS_US\",\n                \"basin_timeseries_v1p2_modelOutput_\" + forcing_type,\n                \"model_output_\" + forcing_type,\n                \"model_output\",\n                \"flow_timeseries\",\n                forcing_type,\n                huc02_,\n            )\n\n            if not os.path.isdir(file_path_dir):\n                # This warning is kept for cases where the directory might be missing for a valid HUC\n                # print(f\"Warning: Model output directory not found: {file_path_dir}\")\n                continue\n\n            sac_random_seeds = [\n                \"05\",\n                \"11\",\n                \"27\",\n                \"33\",\n                \"48\",\n                \"59\",\n                \"66\",\n                \"72\",\n                \"80\",\n                \"94\",\n            ]\n            files = [\n                os.path.join(file_path_dir, f\"{usgs_id}_{seed}_model_output.txt\")\n                for seed in sac_random_seeds\n            ]\n\n            results = []\n            for file in files:\n                if not os.path.exists(file):\n                    continue\n                try:\n                    result = pd.read_csv(file, sep=r\"\\s+\")\n                    df_date = result[[\"YR\", \"MNTH\", \"DY\"]]\n                    df_date.columns = [\"year\", \"month\", \"day\"]\n                    date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n\n                    c, ind1, ind2 = np.intersect1d(\n                        date, t_range_list, return_indices=True\n                    )\n                    if len(c) &gt; 0:\n                        temp_data = np.full([nt, len(var_lst)], np.nan)\n                        temp_data[ind2, :] = result[var_lst].values[ind1]\n                        results.append(temp_data)\n                except Exception as e:\n                    print(f\"Warning: Failed to read {file}: {e}\")\n\n            if results:\n                result_np = np.array(results)\n                # Calculate mean across different random seeds\n                with np.errstate(\n                    invalid=\"ignore\"\n                ):  # Ignore warnings from all-NaN slices\n                    chosen_camels_mods[i, :, :] = np.nanmean(result_np, axis=0)\n\n        return chosen_camels_mods\n\n    def cache_timeseries_xrdataset(self):\n        \"\"\"\n        Overrides the base method to create a complete cache file including PET.\n\n        This method first calls the parent implementation to create the base cache\n        from aquafetch data, then reads the custom PET data and merges it into the\n        same cache file.\n        \"\"\"\n        # First, create the base cache file using the parent method\n        print(\"Creating base time-series cache from aquafetch...\")\n        super().cache_timeseries_xrdataset()\n\n        # Now, read the PET and ET data for all basins for the default time range\n        print(\"Reading PET and ET data to add to the cache...\")\n        gage_id_lst = self.read_object_ids().tolist()\n        model_output_data = self.read_camels_us_model_output_data(\n            gage_id_lst=gage_id_lst, t_range=self.default_t_range, var_lst=[\"PET\", \"ET\"]\n        )\n\n        cache_file = self.cache_dir.joinpath(self._timeseries_cache_filename)\n\n        # Use a with statement to ensure the dataset is closed before writing\n        with xr.open_dataset(cache_file) as ds:\n            print(f\"Variables in base cache: {list(ds.data_vars.keys())}\")\n            # Create xarray.DataArrays for PET and ET\n            pet_da = xr.DataArray(\n                model_output_data[:, :, 0],  # PET data\n                coords={\"basin\": gage_id_lst, \"time\": ds.time},\n                dims=[\"basin\", \"time\"],\n                attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n                name=\"PET\",\n            )\n            et_da = xr.DataArray(\n                model_output_data[:, :, 1],  # ET data\n                coords={\"basin\": gage_id_lst, \"time\": ds.time},\n                dims=[\"basin\", \"time\"],\n                attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n                name=\"ET\",\n            )\n            # Merge PET and ET into the main dataset\n            # Load the dataset into memory to avoid issues with lazy loading\n            merged_ds = ds.load().merge(pet_da).merge(et_da)\n\n        # Now that the original file is closed, we can safely overwrite it\n        print(\"Saving final cache file with merged PET and ET data...\")\n        print(f\"Variables in merged dataset: {list(merged_ds.data_vars.keys())}\")\n        merged_ds.to_netcdf(cache_file, mode=\"w\")\n        print(f\"Successfully saved final cache to: {cache_file}\")\n\n    _subclass_static_definitions = {\n        \"huc_02\": {\"specific_name\": \"huc_02\", \"unit\": \"dimensionless\"},\n        \"gauge_lat\": {\"specific_name\": \"lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"long\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"elev_mean\", \"unit\": \"m\"},\n        \"slope_mean\": {\"specific_name\": \"slope_mkm1\", \"unit\": \"m/km\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"geol_1st_class\": {\"specific_name\": \"geol_1st_class\", \"unit\": \"dimensionless\"},\n        \"geol_2nd_class\": {\"specific_name\": \"geol_2nd_class\", \"unit\": \"dimensionless\"},\n        \"geol_porostiy\": {\"specific_name\": \"geol_porostiy\", \"unit\": \"dimensionless\"},\n        \"geol_permeability\": {\"specific_name\": \"geol_permeability\", \"unit\": \"m^2\"},\n        \"frac_forest\": {\"specific_name\": \"frac_forest\", \"unit\": \"dimensionless\"},\n        \"lai_max\": {\"specific_name\": \"lai_max\", \"unit\": \"dimensionless\"},\n        \"lai_diff\": {\"specific_name\": \"lai_diff\", \"unit\": \"dimensionless\"},\n        \"dom_land_cover_frac\": {\n            \"specific_name\": \"dom_land_cover_frac\",\n            \"unit\": \"dimensionless\",\n        },\n        \"dom_land_cover\": {\"specific_name\": \"dom_land_cover\", \"unit\": \"dimensionless\"},\n        \"root_depth_50\": {\"specific_name\": \"root_depth_50\", \"unit\": \"m\"},\n        \"root_depth_99\": {\"specific_name\": \"root_depth_99\", \"unit\": \"m\"},\n        \"soil_depth_statsgo\": {\"specific_name\": \"soil_depth_statsgo\", \"unit\": \"m\"},\n        \"soil_porosity\": {\"specific_name\": \"soil_porosity\", \"unit\": \"dimensionless\"},\n        \"soil_conductivity\": {\"specific_name\": \"soil_conductivity\", \"unit\": \"cm/hr\"},\n        \"max_water_content\": {\"specific_name\": \"max_water_content\", \"unit\": \"m\"},\n        \"pet_mean\": {\"specific_name\": \"pet_mean\", \"unit\": \"mm/day\"},\n    }\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"usgs\",\n            \"sources\": {\"usgs\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"}},\n        },\n        # TODO: For maurer and nldas, we have not checked the specific names and units.\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n                \"maurer\": {\"specific_name\": \"prcp_maurer\", \"unit\": \"mm/day\"},\n                \"nldas\": {\"specific_name\": \"prcp_nldas\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"},\n                \"maurer\": {\"specific_name\": \"tmax_maurer\", \"unit\": \"\u00b0C\"},\n                \"nldas\": {\"specific_name\": \"tmax_nldas\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"},\n                \"maurer\": {\"specific_name\": \"tmin_maurer\", \"unit\": \"\u00b0C\"},\n                \"nldas\": {\"specific_name\": \"tmin_nldas\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.DAYLIGHT_DURATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"dayl\", \"unit\": \"s\"},\n                \"maurer\": {\"specific_name\": \"dayl_maurer\", \"unit\": \"s\"},\n                \"nldas\": {\"specific_name\": \"dayl_nldas\", \"unit\": \"s\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"},\n                \"maurer\": {\"specific_name\": \"srad_maurer\", \"unit\": \"W/m^2\"},\n                \"nldas\": {\"specific_name\": \"srad_nldas\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"swe_mm\", \"unit\": \"mm/day\"},\n                \"maurer\": {\"specific_name\": \"swe_maurer\", \"unit\": \"mm/day\"},\n                \"nldas\": {\"specific_name\": \"swe_nldas\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.VAPOR_PRESSURE: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"vp_hpa\", \"unit\": \"hPa\"},\n                \"maurer\": {\"specific_name\": \"vp_maurer\", \"unit\": \"hPa\"},\n                \"nldas\": {\"specific_name\": \"vp_nldas\", \"unit\": \"hPa\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"sac-sma\",\n            \"sources\": {\"sac-sma\": {\"specific_name\": \"PET\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"sac-sma\",\n            \"sources\": {\"sac-sma\": {\"specific_name\": \"ET\", \"unit\": \"mm/day\"}},\n        },\n    }\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.CamelsUs.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_US dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELS_US data directory. This is where the data will be stored.</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional, defaults to US).</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (not used, handled by aqua_fetch).</p> <code>False</code> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize CAMELS_US dataset.\n\n    Args:\n        data_path: Path to the CAMELS_US data directory. This is where the data will be stored.\n        region: Geographic region identifier (optional, defaults to US).\n        download: Whether to download data automatically (not used, handled by aqua_fetch).\n    \"\"\"\n    super().__init__(data_path)\n    self.region = \"US\" if region is None else region\n\n    # Instantiate the custom class defined at module level\n    self.aqua_fetch = CAMELS_US(data_path)\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.CamelsUs.cache_timeseries_xrdataset","title":"<code>cache_timeseries_xrdataset()</code>","text":"<p>Overrides the base method to create a complete cache file including PET.</p> <p>This method first calls the parent implementation to create the base cache from aquafetch data, then reads the custom PET data and merges it into the same cache file.</p> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>def cache_timeseries_xrdataset(self):\n    \"\"\"\n    Overrides the base method to create a complete cache file including PET.\n\n    This method first calls the parent implementation to create the base cache\n    from aquafetch data, then reads the custom PET data and merges it into the\n    same cache file.\n    \"\"\"\n    # First, create the base cache file using the parent method\n    print(\"Creating base time-series cache from aquafetch...\")\n    super().cache_timeseries_xrdataset()\n\n    # Now, read the PET and ET data for all basins for the default time range\n    print(\"Reading PET and ET data to add to the cache...\")\n    gage_id_lst = self.read_object_ids().tolist()\n    model_output_data = self.read_camels_us_model_output_data(\n        gage_id_lst=gage_id_lst, t_range=self.default_t_range, var_lst=[\"PET\", \"ET\"]\n    )\n\n    cache_file = self.cache_dir.joinpath(self._timeseries_cache_filename)\n\n    # Use a with statement to ensure the dataset is closed before writing\n    with xr.open_dataset(cache_file) as ds:\n        print(f\"Variables in base cache: {list(ds.data_vars.keys())}\")\n        # Create xarray.DataArrays for PET and ET\n        pet_da = xr.DataArray(\n            model_output_data[:, :, 0],  # PET data\n            coords={\"basin\": gage_id_lst, \"time\": ds.time},\n            dims=[\"basin\", \"time\"],\n            attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n            name=\"PET\",\n        )\n        et_da = xr.DataArray(\n            model_output_data[:, :, 1],  # ET data\n            coords={\"basin\": gage_id_lst, \"time\": ds.time},\n            dims=[\"basin\", \"time\"],\n            attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n            name=\"ET\",\n        )\n        # Merge PET and ET into the main dataset\n        # Load the dataset into memory to avoid issues with lazy loading\n        merged_ds = ds.load().merge(pet_da).merge(et_da)\n\n    # Now that the original file is closed, we can safely overwrite it\n    print(\"Saving final cache file with merged PET and ET data...\")\n    print(f\"Variables in merged dataset: {list(merged_ds.data_vars.keys())}\")\n    merged_ds.to_netcdf(cache_file, mode=\"w\")\n    print(f\"Successfully saved final cache to: {cache_file}\")\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.CamelsUs.read_camels_us_model_output_data","title":"<code>read_camels_us_model_output_data(gage_id_lst=None, t_range=None, var_lst=None, forcing_type='daymet')</code>","text":"<p>Read model output data of CAMELS-US, including PET. This is a legacy function migrated from the old camels.py.</p> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>def read_camels_us_model_output_data(\n    self,\n    gage_id_lst: list = None,\n    t_range: list = None,\n    var_lst: list = None,\n    forcing_type=\"daymet\",\n) -&gt; np.array:\n    \"\"\"\n    Read model output data of CAMELS-US, including PET.\n    This is a legacy function migrated from the old camels.py.\n    \"\"\"\n    # Fetch HUC codes for the requested basins on-the-fly\n    try:\n        huc_ds = self.read_attr_xrdataset(\n            gage_id_lst=gage_id_lst, var_lst=[\"huc_02\"], to_numeric=False\n        )\n        huc_df = huc_ds.to_dataframe()\n    except Exception as e:\n        raise RuntimeError(\n            f\"Could not read HUC attributes to get model output data: {e}\"\n        )\n\n    t_range_list = pd.date_range(start=t_range[0], end=t_range[1], freq=\"D\").values\n    model_out_put_var_lst = [\n        \"SWE\",\n        \"PRCP\",\n        \"RAIM\",\n        \"TAIR\",\n        \"PET\",\n        \"ET\",\n        \"MOD_RUN\",\n        \"OBS_RUN\",\n    ]\n    if not set(var_lst).issubset(set(model_out_put_var_lst)):\n        raise RuntimeError(\n            f\"Requested variables not in model output list: {var_lst}\"\n        )\n\n    nt = len(t_range_list)\n    chosen_camels_mods = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n\n    for i, usgs_id in enumerate(\n        tqdm(gage_id_lst, desc=\"Read model output data (PET and ET) for CAMELS-US\")\n    ):\n        try:\n            huc02_ = huc_df.loc[usgs_id, \"huc_02\"]\n            # Convert to 2-digit string with leading zeros if needed\n            huc02_ = f\"{int(huc02_):02d}\"\n        except KeyError:\n            print(\n                f\"Warning: No HUC attribute found for {usgs_id}, skipping PET and ET reading.\"\n            )\n            continue\n\n        # Construct path to model output files\n        file_path_dir = os.path.join(\n            self.data_source_dir,\n            \"CAMELS_US\",\n            \"basin_timeseries_v1p2_modelOutput_\" + forcing_type,\n            \"model_output_\" + forcing_type,\n            \"model_output\",\n            \"flow_timeseries\",\n            forcing_type,\n            huc02_,\n        )\n\n        if not os.path.isdir(file_path_dir):\n            # This warning is kept for cases where the directory might be missing for a valid HUC\n            # print(f\"Warning: Model output directory not found: {file_path_dir}\")\n            continue\n\n        sac_random_seeds = [\n            \"05\",\n            \"11\",\n            \"27\",\n            \"33\",\n            \"48\",\n            \"59\",\n            \"66\",\n            \"72\",\n            \"80\",\n            \"94\",\n        ]\n        files = [\n            os.path.join(file_path_dir, f\"{usgs_id}_{seed}_model_output.txt\")\n            for seed in sac_random_seeds\n        ]\n\n        results = []\n        for file in files:\n            if not os.path.exists(file):\n                continue\n            try:\n                result = pd.read_csv(file, sep=r\"\\s+\")\n                df_date = result[[\"YR\", \"MNTH\", \"DY\"]]\n                df_date.columns = [\"year\", \"month\", \"day\"]\n                date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n\n                c, ind1, ind2 = np.intersect1d(\n                    date, t_range_list, return_indices=True\n                )\n                if len(c) &gt; 0:\n                    temp_data = np.full([nt, len(var_lst)], np.nan)\n                    temp_data[ind2, :] = result[var_lst].values[ind1]\n                    results.append(temp_data)\n            except Exception as e:\n                print(f\"Warning: Failed to read {file}: {e}\")\n\n        if results:\n            result_np = np.array(results)\n            # Calculate mean across different random seeds\n            with np.errstate(\n                invalid=\"ignore\"\n            ):  # Ignore warnings from all-NaN slices\n                chosen_camels_mods[i, :, :] = np.nanmean(result_np, axis=0)\n\n    return chosen_camels_mods\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.CamelshKr","title":"<code>CamelshKr</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELSH_KR dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELSH_KR dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camelsh_kr.py</code> <pre><code>class CamelshKr(HydroDataset):\n    \"\"\"CAMELSH_KR dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELSH_KR dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        region: Optional[str] = None,\n        download: bool = False,\n        cache_path: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize CAMELSH_KR dataset.\n\n        Args:\n            data_path: Path to the CAMELSH_KR data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n            cache_path: Path to the cache directory\n        \"\"\"\n        super().__init__(data_path, cache_path=cache_path)\n        self.region = region\n        self.download = download\n        # In aqua_fetch, CAMELS_SK is the alias of CAMELSH_KR\n        self.aqua_fetch = CAMELS_SK(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_sk_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_sk_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"2000-01-01\", \"2019-12-31\"]\n\n    # not find information of features\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"obs\",\n            \"sources\": {\n                \"obs\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n            },\n        },\n        StandardVariable.WATER_LEVEL: {\n            \"default_source\": \"obs\",\n            \"sources\": {\n                \"obs\": {\"specific_name\": \"water_level\", \"unit\": \"m\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"total_precipitation\", \"unit\": \"mm/day\"},\n                \"obs\": {\"specific_name\": \"precip_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"temperature_2m\", \"unit\": \"\u00b0C\"},\n                \"obs\": {\"specific_name\": \"air_temp_obs\", \"unit\": \"\u00b0C\"},\n                \"dewpoint\": {\"specific_name\": \"dewpoint_temperature_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.VAPOR_PRESSURE: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"dewpoint_temperature_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.SNOW_DEPTH: {\n            \"default_source\": \"era5_depth\",\n            \"sources\": {\n                \"era5_depth\": {\"specific_name\": \"snow_depth\", \"unit\": \"m\"},\n            },\n        },\n        StandardVariable.SNOW_COVER: {\n            \"default_source\": \"era5_cover\",\n            \"sources\": {\n                \"era5_cover\": {\"specific_name\": \"snow_cover\", \"unit\": \"fraction\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"potential_evaporation\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"u_component_of_wind_10m\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"v_component_of_wind_10m\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.WIND_SPEED: {\n            \"default_source\": \"obs_speed\",\n            \"sources\": {\n                \"obs_speed\": {\"specific_name\": \"wind_sp_obs\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.WIND_DIR: {\n            \"default_source\": \"obs_dir\",\n            \"sources\": {\n                \"obs_dir\": {\"specific_name\": \"wind_dir_obs\", \"unit\": \"degree\"},\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"surface_pressure\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.THERMAL_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_thermal_radiation\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\n                    \"specific_name\": \"surface_net_solar_radiation\",\n                    \"unit\": \"W/m^2\",\n                },\n            },\n        },\n    }\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.CamelshKr.__init__","title":"<code>__init__(data_path, region=None, download=False, cache_path=None)</code>","text":"<p>Initialize CAMELSH_KR dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the CAMELSH_KR data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> <code>cache_path</code> <code>Optional[str]</code> <p>Path to the cache directory</p> <code>None</code> Source code in <code>hydrodataset/camelsh_kr.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    region: Optional[str] = None,\n    download: bool = False,\n    cache_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize CAMELSH_KR dataset.\n\n    Args:\n        data_path: Path to the CAMELSH_KR data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n        cache_path: Path to the cache directory\n    \"\"\"\n    super().__init__(data_path, cache_path=cache_path)\n    self.region = region\n    self.download = download\n    # In aqua_fetch, CAMELS_SK is the alias of CAMELSH_KR\n    self.aqua_fetch = CAMELS_SK(data_path)\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset","title":"<code>HydroDataset</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An interface for Hydrological Dataset</p> <p>For unit, we use Pint package's unit system -- unit registry</p>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset--parameters","title":"Parameters","text":"<p>ABC : type description</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>class HydroDataset(ABC):\n    \"\"\"An interface for Hydrological Dataset\n\n    For unit, we use Pint package's unit system -- unit registry\n\n    Parameters\n    ----------\n    ABC : _type_\n        _description_\n    \"\"\"\n\n    # A unified definition for static variables, including name mapping and units\n    _base_static_definitions = {\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n    }\n    # variable name map for timeseries\n    _dynamic_variable_mapping = {}\n\n    def __init__(self, data_path, cache_path=None):\n        self.data_source_dir = Path(ROOT_DIR, data_path)\n        if not self.data_source_dir.is_dir():\n            self.data_source_dir.mkdir(parents=True)\n        if cache_path is None:\n            self.cache_dir = Path(CACHE_DIR)\n        else:\n            self.cache_dir = Path(cache_path)\n        if not self.cache_dir.is_dir():\n            self.cache_dir.mkdir(parents=True)\n\n        # Merge static variable definitions\n        self._static_variable_definitions = self._base_static_definitions.copy()\n        if hasattr(self.__class__, \"_subclass_static_definitions\"):\n            self._static_variable_definitions.update(self._subclass_static_definitions)\n\n    def get_name(self):\n        raise NotImplementedError\n\n    def set_data_source_describe(self):\n        raise NotImplementedError\n\n    def download_data_source(self):\n        raise NotImplementedError\n\n    def is_data_ready(self):\n        raise NotImplementedError\n\n    def read_object_ids(self) -&gt; np.ndarray:\n        \"\"\"Read watershed station ID list.\"\"\"\n        if hasattr(self, \"aqua_fetch\"):\n            stations_list = self.aqua_fetch.stations()\n            return np.sort(np.array(stations_list))\n        raise NotImplementedError\n\n    def read_target_cols(\n        self, gage_id_lst=None, t_range=None, target_cols=None, **kwargs\n    ) -&gt; np.ndarray:\n        raise NotImplementedError\n\n    def read_relevant_cols(\n        self, gage_id_lst=None, t_range=None, var_lst=None, forcing_type=None, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"3d data (site_num * time_length * var_num), time-series data\"\"\"\n        raise NotImplementedError\n\n    def read_constant_cols(\n        self, gage_id_lst=None, var_lst=None, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"2d data (site_num * var_num), non-time-series data\"\"\"\n        raise NotImplementedError\n\n    def read_other_cols(\n        self, object_ids=None, other_cols: dict = None, **kwargs\n    ) -&gt; dict:\n        \"\"\"some data which cannot be easily treated as constant vars or time-series with same length as relevant vars\n        CONVENTION: other_cols is a dict, where each item is also a dict with all params in it\n        \"\"\"\n        raise NotImplementedError\n\n    def get_constant_cols(self) -&gt; np.ndarray:\n        \"\"\"the constant cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def get_relevant_cols(self) -&gt; np.ndarray:\n        \"\"\"the relevant cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def get_target_cols(self) -&gt; np.ndarray:\n        \"\"\"the target cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def get_other_cols(self) -&gt; dict:\n        \"\"\"the other cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def _dynamic_features(self) -&gt; list:\n        \"\"\"the dynamic features in this data_source\"\"\"\n        if hasattr(self, \"aqua_fetch\"):\n            original_features = self.aqua_fetch.dynamic_features\n            return self._clean_feature_names(original_features)\n        raise NotImplementedError\n\n    @staticmethod\n    def _clean_feature_names(feature_names):\n        \"\"\"Clean feature names to be compatible with NetCDF format and our internal standard.\n\n        The cleaning process follows these steps:\n        1. Remove units in parentheses (along with any preceding whitespace)\n           e.g., 'Prcp(mm/day)' -&gt; 'Prcp' or 'Temp (\u00b0C)' -&gt; 'Temp'\n        2. Convert all characters to lowercase\n           e.g., 'Prcp' -&gt; 'prcp'\n        3. Remove any remaining invalid characters (only keep a-z, 0-9, and _)\n           This ensures NetCDF variable naming compliance\n\n        Args:\n            feature_names (list or pd.Index): Original feature names that may contain\n                units and special characters\n\n        Returns:\n            list: Cleaned feature names with only lowercase letters, numbers, and underscores\n\n        Examples:\n            &gt;&gt;&gt; _clean_feature_names(['Prcp(mm/day)_daymet', 'Temp (\u00b0C)'])\n            ['prcp_daymet', 'temp']\n        \"\"\"\n        if not isinstance(feature_names, pd.Index):\n            feature_names = pd.Index(feature_names)\n\n        # Remove units in parentheses, then convert to lowercase\n        cleaned_names = feature_names.str.replace(\n            r\"\\s*\\([^)]*\\)\", \"\", regex=True\n        ).str.lower()\n        # Replace any remaining invalid characters\n        cleaned_names = cleaned_names.str.replace(r\"\"\"[^a-z0-9_]\"\"\", \"\", regex=True)\n        return cleaned_names.tolist()\n\n    def _static_features(self) -&gt; list:\n        \"\"\"the static features in this data_source\"\"\"\n        if hasattr(self, \"aqua_fetch\"):\n            original_features = self.aqua_fetch.static_features\n            return self._clean_feature_names(original_features)\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def _attributes_cache_filename(self):\n        pass\n\n    @property\n    @abstractmethod\n    def _timeseries_cache_filename(self):\n        pass\n\n    @property\n    @abstractmethod\n    def default_t_range(self):\n        pass\n\n    def cache_timeseries_xrdataset(self):\n        if hasattr(self, \"aqua_fetch\"):\n            # Build a lookup map from specific name to unit\n            unit_lookup = {}\n            if hasattr(self, \"_dynamic_variable_mapping\"):\n                for (\n                    std_name,\n                    mapping_info,\n                ) in self._dynamic_variable_mapping.items():\n                    for source, source_info in mapping_info[\"sources\"].items():\n                        unit_lookup[source_info[\"specific_name\"]] = source_info[\"unit\"]\n\n            gage_id_lst = self.read_object_ids().tolist()\n            original_var_lst = self.aqua_fetch.dynamic_features\n            cleaned_var_lst = self._clean_feature_names(original_var_lst)\n            # Create a mapping from original variable names to cleaned names\n            # to ensure correct correspondence even if list order changes\n            var_name_mapping = dict(zip(original_var_lst, cleaned_var_lst))\n\n            batch_data = self.aqua_fetch.fetch_stations_features(\n                stations=gage_id_lst,\n                dynamic_features=original_var_lst,\n                static_features=None,\n                st=self.default_t_range[0],\n                en=self.default_t_range[1],\n                as_dataframe=False,\n            )\n\n            dynamic_data = (\n                batch_data[1] if isinstance(batch_data, tuple) else batch_data\n            )\n\n            new_data_vars = {}\n            time_coord = dynamic_data.coords[\"time\"]\n\n            # Process only the variables that exist in the data source\n            # Subclasses can add additional variables in their override methods\n            for original_var in tqdm(\n                original_var_lst,\n                desc=\"Processing variables\",\n                total=len(original_var_lst),\n            ):\n                cleaned_var = var_name_mapping[original_var]\n                var_data = []\n                for station in gage_id_lst:\n                    if station in dynamic_data.data_vars:\n                        station_data = dynamic_data[station].sel(\n                            dynamic_features=original_var\n                        )\n                        if \"dynamic_features\" in station_data.coords:\n                            station_data = station_data.drop(\"dynamic_features\")\n                        var_data.append(station_data)\n\n                if var_data:\n                    combined = xr.concat(var_data, dim=\"basin\")\n                    combined[\"basin\"] = gage_id_lst\n                    combined.attrs[\"units\"] = unit_lookup.get(cleaned_var, \"unknown\")\n                    new_data_vars[cleaned_var] = combined\n\n            new_ds = xr.Dataset(\n                data_vars=new_data_vars,\n                coords={\n                    \"basin\": gage_id_lst,\n                    \"time\": time_coord,\n                },\n            )\n\n            batch_filepath = self.cache_dir.joinpath(self._timeseries_cache_filename)\n            batch_filepath.parent.mkdir(parents=True, exist_ok=True)\n            new_ds.to_netcdf(batch_filepath)\n            print(f\"\u6210\u529f\u4fdd\u5b58\u5230: {batch_filepath}\")\n        else:\n            raise NotImplementedError\n\n    def _assign_units_to_dataset(self, ds, units_map):\n        def get_unit_by_prefix(var_name):\n            for prefix, unit in units_map.items():\n                if var_name.startswith(prefix):\n                    return unit\n            return None\n\n        def get_unit(var_name):\n            prefix_unit = get_unit_by_prefix(var_name)\n            if prefix_unit:\n                return prefix_unit\n            return \"undefined\"\n\n        for var in ds.data_vars:\n            unit = get_unit(var)\n            ds[var].attrs[\"units\"] = unit\n            if unit == \"class\":\n                ds[var].attrs[\"description\"] = \"Classification code\"\n        return ds\n\n        return ds\n\n    def _get_attribute_units(self) -&gt; dict:\n        \"\"\"Builds a unit dictionary from the static variable definitions.\"\"\"\n        return {\n            info[\"specific_name\"]: info[\"unit\"]\n            for std_name, info in self._static_variable_definitions.items()\n        }\n\n    def cache_attributes_xrdataset(self):\n        if hasattr(self, \"aqua_fetch\"):\n            df_attr = self.aqua_fetch.fetch_static_features()\n            print(df_attr.columns)\n            # Clean column names using the unified method\n            df_attr.columns = self._clean_feature_names(df_attr.columns)\n            # Remove duplicate columns if any (keep first occurrence)\n            if df_attr.columns.duplicated().any():\n                df_attr = df_attr.loc[:, ~df_attr.columns.duplicated()]\n            # Ensure index is string type for basin IDs\n            df_attr.index = df_attr.index.astype(str)\n            ds_attr = df_attr.to_xarray()\n            # Check if the coordinate is named 'basin', if not rename it\n            coord_names = list(ds_attr.dims.keys())\n            if len(coord_names) &gt; 0 and coord_names[0] != \"basin\":\n                ds_attr = ds_attr.rename({coord_names[0]: \"basin\"})\n            units_map = self._get_attribute_units()\n            ds_attr = self._assign_units_to_dataset(ds_attr, units_map)\n            ds_attr.to_netcdf(self.cache_dir.joinpath(self._attributes_cache_filename))\n        else:\n            raise NotImplementedError\n\n    def read_attr_xrdataset(\n        self,\n        gage_id_lst: list = None,\n        var_lst: list = None,\n        to_numeric: bool = True,\n        **kwargs,\n    ) -&gt; xr.Dataset:\n        \"\"\"Reads attribute data for a list of basins using standardized variable names.\n\n        Args:\n            gage_id_lst: A list of basin identifiers.\n            var_lst: A list of **standard** attribute names to retrieve.\n                If None, nothing will be returned.\n            to_numeric: If True, converts all non-numeric variables to numeric codes\n                and stores the original labels in the variable's attributes.\n                Defaults to True.\n\n        Returns:\n            An xarray Dataset containing the attribute data for the requested basins,\n            with variables named using the standard names.\n        \"\"\"\n        if not var_lst:\n            return None\n\n        # 1. Translate standard names to dataset-specific names\n        target_vars_to_fetch = []\n        rename_map = {}\n        for std_name in var_lst:\n            if std_name not in self._static_variable_definitions:\n                raise ValueError(\n                    f\"'{std_name}' is not a recognized standard static variable.\"\n                )\n            actual_var_name = self._static_variable_definitions[std_name][\n                \"specific_name\"\n            ]\n            target_vars_to_fetch.append(actual_var_name)\n            rename_map[actual_var_name] = std_name\n\n        # 2. Read data from cache using actual variable names\n        attr_cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n        try:\n            attr_ds = xr.open_dataset(attr_cache_file)\n        except FileNotFoundError:\n            self.cache_attributes_xrdataset()\n            attr_ds = xr.open_dataset(attr_cache_file)\n\n        # 3. Select variables and basins\n        ds_subset = attr_ds[target_vars_to_fetch]\n        if gage_id_lst is not None:\n            gage_id_lst = [str(gid) for gid in gage_id_lst]\n            ds_selected = ds_subset.sel(basin=gage_id_lst)\n        else:\n            ds_selected = ds_subset\n\n        # 4. Rename to standard names\n        final_ds = ds_selected.rename(rename_map)\n\n        if not to_numeric:\n            return final_ds\n\n        # 5. If to_numeric is True, perform conversion\n        converted_ds = xr.Dataset(coords=final_ds.coords)\n        for var_name, da in final_ds.data_vars.items():\n            if np.issubdtype(da.dtype, np.number):\n                converted_ds[var_name] = da\n            else:\n                # Assumes string-like array that needs factorizing\n                numeric_vals, labels = pd.factorize(da.values, sort=True)\n                new_da = xr.DataArray(\n                    numeric_vals,\n                    coords=da.coords,\n                    dims=da.dims,\n                    name=da.name,\n                    attrs=da.attrs,  # Preserve original attributes\n                )\n                new_da.attrs[\"labels\"] = labels.tolist()\n                converted_ds[var_name] = new_da\n        return converted_ds\n\n    def _load_ts_dataset(self, **kwargs):\n        \"\"\"\n        Loads the time series dataset from cache.\n\n        This method can be overridden by subclasses to implement different loading\n        strategies (e.g., loading multiple files).\n\n        Args:\n            **kwargs: Additional keyword arguments for loading.\n\n        Returns:\n            xarray.Dataset: The loaded time series dataset.\n        \"\"\"\n        ts_cache_file = self.cache_dir.joinpath(self._timeseries_cache_filename)\n\n        if not os.path.isfile(ts_cache_file):\n            self.cache_timeseries_xrdataset()\n\n        return xr.open_dataset(ts_cache_file)\n\n    def read_ts_xrdataset(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        sources: dict = None,\n        **kwargs,\n    ):\n        if (\n            not hasattr(self, \"_dynamic_variable_mapping\")\n            or not self._dynamic_variable_mapping\n        ):\n            raise NotImplementedError(\n                \"This dataset does not support the standardized variable mapping.\"\n            )\n\n        if var_lst is None:\n            var_lst = list(self._dynamic_variable_mapping.keys())\n\n        if t_range is None:\n            t_range = self.default_t_range\n\n        target_vars_to_fetch = []\n        rename_map = {}\n\n        for std_name in var_lst:\n            if std_name not in self._dynamic_variable_mapping:\n                raise ValueError(\n                    f\"'{std_name}' is not a recognized standard variable for this dataset.\"\n                )\n\n            mapping_info = self._dynamic_variable_mapping[std_name]\n\n            # Determine which source(s) to use and if they were explicitly requested\n            is_explicit_source = sources and std_name in sources\n            sources_to_use = []\n            if is_explicit_source:\n                provided_sources = sources[std_name]\n                if isinstance(provided_sources, list):\n                    sources_to_use.extend(provided_sources)\n                else:\n                    sources_to_use.append(provided_sources)\n            else:\n                sources_to_use.append(mapping_info[\"default_source\"])\n\n            # A suffix is only needed if the user explicitly requested multiple sources\n            needs_suffix = is_explicit_source and len(sources_to_use) &gt; 1\n            for source in sources_to_use:\n                if source not in mapping_info[\"sources\"]:\n                    raise ValueError(\n                        f\"Source '{source}' is not available for variable '{std_name}'.\"\n                    )\n\n                actual_var_name = mapping_info[\"sources\"][source][\"specific_name\"]\n                target_vars_to_fetch.append(actual_var_name)\n                output_name = f\"{std_name}_{source}\" if needs_suffix else std_name\n                rename_map[actual_var_name] = output_name\n\n        # Read data from cache using actual variable names\n        ts = self._load_ts_dataset(**kwargs)\n        missing_vars = [v for v in target_vars_to_fetch if v not in ts.data_vars]\n        if missing_vars:\n            # To provide a better error message, map back to standard names\n            reverse_rename_map = {v: k for k, v in rename_map.items()}\n            missing_std_vars = [reverse_rename_map.get(v, v) for v in missing_vars]\n            raise ValueError(\n                f\"The following variables are missing from the cache file: {missing_std_vars}\"\n            )\n\n        ds_subset = ts[target_vars_to_fetch]\n        ds_selected = ds_subset.sel(\n            basin=gage_id_lst, time=slice(t_range[0], t_range[1])\n        )\n        final_ds = ds_selected.rename(rename_map)\n        return final_ds\n\n    def get_available_dynamic_features(self) -&gt; dict:\n        \"\"\"\n        Returns a dictionary of available standard dynamic feature names\n        and their possible sources.\n        \"\"\"\n        if (\n            not hasattr(self, \"_dynamic_variable_mapping\")\n            or not self._dynamic_variable_mapping\n        ):\n            return {}\n\n        feature_info = {}\n        for std_name, mapping_info in self._dynamic_variable_mapping.items():\n            feature_info[std_name] = {\n                \"default_source\": mapping_info.get(\"default_source\"),\n                \"available_sources\": list(mapping_info.get(\"sources\", {}).keys()),\n            }\n        return feature_info\n\n    def get_available_static_features(self) -&gt; list:\n        \"\"\"Returns a list of available standard static feature names.\"\"\"\n        return list(self._static_variable_definitions.keys())\n\n    @property\n    def available_static_features(self) -&gt; list:\n        \"\"\"Returns a list of available static attribute names.\"\"\"\n        return self.get_available_static_features()\n\n    @property\n    def available_dynamic_features(self) -&gt; dict:\n        \"\"\"Returns a dictionary of available dynamic feature names and their possible sources.\"\"\"\n        return self.get_available_dynamic_features()\n\n    def read_area(self, gage_id_lst: list[str]) -&gt; xr.Dataset:\n        \"\"\"Reads the catchment area for a list of basins.\n\n        Args:\n            gage_id_lst: A list of basin identifiers for which to retrieve the area.\n\n        Returns:\n            An xarray Dataset containing the area data for the requested basins.\n        \"\"\"\n        data_ds = self.read_attr_xrdataset(gage_id_lst=gage_id_lst, var_lst=[\"area\"])\n        return data_ds\n\n    def read_mean_prcp(self, gage_id_lst: list[str], unit: str = \"mm/d\") -&gt; xr.Dataset:\n        \"\"\"Reads the mean daily precipitation for a list of basins, with unit conversion.\n\n        Args:\n            gage_id_lst: A list of basin identifiers.\n            unit: The desired unit for the output precipitation. Defaults to \"mm/d\".\n                Supported units: ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h',\n                'mm/3hour', 'mm/8d', 'mm/8day'].\n\n        Returns:\n            An xarray Dataset containing the mean precipitation data in the specified units.\n\n        Raises:\n            ValueError: If an unsupported unit is provided.\n        \"\"\"\n        prcp_var_name = \"p_mean\"\n        data_ds = self.read_attr_xrdataset(\n            gage_id_lst=gage_id_lst, var_lst=[prcp_var_name]\n        )\n        # No conversion needed\n        if unit in [\"mm/d\", \"mm/day\"]:\n            return data_ds\n\n        # Conversion needed, create a new dataset\n        converted_ds = data_ds.copy()\n        # After renaming, the variable in the dataset is now the standard name\n        if unit in [\"mm/h\", \"mm/hour\"]:\n            converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 24\n        elif unit in [\"mm/3h\", \"mm/3hour\"]:\n            converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 8\n        elif unit in [\"mm/8d\", \"mm/8day\"]:\n            converted_ds[prcp_var_name] = data_ds[prcp_var_name] * 8\n        else:\n            raise ValueError(\n                \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n            )\n        return converted_ds\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.available_dynamic_features","title":"<code>available_dynamic_features</code>  <code>property</code>","text":"<p>Returns a dictionary of available dynamic feature names and their possible sources.</p>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.available_static_features","title":"<code>available_static_features</code>  <code>property</code>","text":"<p>Returns a list of available static attribute names.</p>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.get_available_dynamic_features","title":"<code>get_available_dynamic_features()</code>","text":"<p>Returns a dictionary of available standard dynamic feature names and their possible sources.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_available_dynamic_features(self) -&gt; dict:\n    \"\"\"\n    Returns a dictionary of available standard dynamic feature names\n    and their possible sources.\n    \"\"\"\n    if (\n        not hasattr(self, \"_dynamic_variable_mapping\")\n        or not self._dynamic_variable_mapping\n    ):\n        return {}\n\n    feature_info = {}\n    for std_name, mapping_info in self._dynamic_variable_mapping.items():\n        feature_info[std_name] = {\n            \"default_source\": mapping_info.get(\"default_source\"),\n            \"available_sources\": list(mapping_info.get(\"sources\", {}).keys()),\n        }\n    return feature_info\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.get_available_static_features","title":"<code>get_available_static_features()</code>","text":"<p>Returns a list of available standard static feature names.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_available_static_features(self) -&gt; list:\n    \"\"\"Returns a list of available standard static feature names.\"\"\"\n    return list(self._static_variable_definitions.keys())\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.get_constant_cols","title":"<code>get_constant_cols()</code>","text":"<p>the constant cols in this data_source</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_constant_cols(self) -&gt; np.ndarray:\n    \"\"\"the constant cols in this data_source\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.get_other_cols","title":"<code>get_other_cols()</code>","text":"<p>the other cols in this data_source</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_other_cols(self) -&gt; dict:\n    \"\"\"the other cols in this data_source\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.get_relevant_cols","title":"<code>get_relevant_cols()</code>","text":"<p>the relevant cols in this data_source</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_relevant_cols(self) -&gt; np.ndarray:\n    \"\"\"the relevant cols in this data_source\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.get_target_cols","title":"<code>get_target_cols()</code>","text":"<p>the target cols in this data_source</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_target_cols(self) -&gt; np.ndarray:\n    \"\"\"the target cols in this data_source\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.read_area","title":"<code>read_area(gage_id_lst)</code>","text":"<p>Reads the catchment area for a list of basins.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list[str]</code> <p>A list of basin identifiers for which to retrieve the area.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the area data for the requested basins.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_area(self, gage_id_lst: list[str]) -&gt; xr.Dataset:\n    \"\"\"Reads the catchment area for a list of basins.\n\n    Args:\n        gage_id_lst: A list of basin identifiers for which to retrieve the area.\n\n    Returns:\n        An xarray Dataset containing the area data for the requested basins.\n    \"\"\"\n    data_ds = self.read_attr_xrdataset(gage_id_lst=gage_id_lst, var_lst=[\"area\"])\n    return data_ds\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.read_attr_xrdataset","title":"<code>read_attr_xrdataset(gage_id_lst=None, var_lst=None, to_numeric=True, **kwargs)</code>","text":"<p>Reads attribute data for a list of basins using standardized variable names.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list</code> <p>A list of basin identifiers.</p> <code>None</code> <code>var_lst</code> <code>list</code> <p>A list of standard attribute names to retrieve. If None, nothing will be returned.</p> <code>None</code> <code>to_numeric</code> <code>bool</code> <p>If True, converts all non-numeric variables to numeric codes and stores the original labels in the variable's attributes. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the attribute data for the requested basins,</p> <code>Dataset</code> <p>with variables named using the standard names.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_attr_xrdataset(\n    self,\n    gage_id_lst: list = None,\n    var_lst: list = None,\n    to_numeric: bool = True,\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"Reads attribute data for a list of basins using standardized variable names.\n\n    Args:\n        gage_id_lst: A list of basin identifiers.\n        var_lst: A list of **standard** attribute names to retrieve.\n            If None, nothing will be returned.\n        to_numeric: If True, converts all non-numeric variables to numeric codes\n            and stores the original labels in the variable's attributes.\n            Defaults to True.\n\n    Returns:\n        An xarray Dataset containing the attribute data for the requested basins,\n        with variables named using the standard names.\n    \"\"\"\n    if not var_lst:\n        return None\n\n    # 1. Translate standard names to dataset-specific names\n    target_vars_to_fetch = []\n    rename_map = {}\n    for std_name in var_lst:\n        if std_name not in self._static_variable_definitions:\n            raise ValueError(\n                f\"'{std_name}' is not a recognized standard static variable.\"\n            )\n        actual_var_name = self._static_variable_definitions[std_name][\n            \"specific_name\"\n        ]\n        target_vars_to_fetch.append(actual_var_name)\n        rename_map[actual_var_name] = std_name\n\n    # 2. Read data from cache using actual variable names\n    attr_cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n    try:\n        attr_ds = xr.open_dataset(attr_cache_file)\n    except FileNotFoundError:\n        self.cache_attributes_xrdataset()\n        attr_ds = xr.open_dataset(attr_cache_file)\n\n    # 3. Select variables and basins\n    ds_subset = attr_ds[target_vars_to_fetch]\n    if gage_id_lst is not None:\n        gage_id_lst = [str(gid) for gid in gage_id_lst]\n        ds_selected = ds_subset.sel(basin=gage_id_lst)\n    else:\n        ds_selected = ds_subset\n\n    # 4. Rename to standard names\n    final_ds = ds_selected.rename(rename_map)\n\n    if not to_numeric:\n        return final_ds\n\n    # 5. If to_numeric is True, perform conversion\n    converted_ds = xr.Dataset(coords=final_ds.coords)\n    for var_name, da in final_ds.data_vars.items():\n        if np.issubdtype(da.dtype, np.number):\n            converted_ds[var_name] = da\n        else:\n            # Assumes string-like array that needs factorizing\n            numeric_vals, labels = pd.factorize(da.values, sort=True)\n            new_da = xr.DataArray(\n                numeric_vals,\n                coords=da.coords,\n                dims=da.dims,\n                name=da.name,\n                attrs=da.attrs,  # Preserve original attributes\n            )\n            new_da.attrs[\"labels\"] = labels.tolist()\n            converted_ds[var_name] = new_da\n    return converted_ds\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.read_constant_cols","title":"<code>read_constant_cols(gage_id_lst=None, var_lst=None, **kwargs)</code>","text":"<p>2d data (site_num * var_num), non-time-series data</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_constant_cols(\n    self, gage_id_lst=None, var_lst=None, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"2d data (site_num * var_num), non-time-series data\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.read_mean_prcp","title":"<code>read_mean_prcp(gage_id_lst, unit='mm/d')</code>","text":"<p>Reads the mean daily precipitation for a list of basins, with unit conversion.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list[str]</code> <p>A list of basin identifiers.</p> required <code>unit</code> <code>str</code> <p>The desired unit for the output precipitation. Defaults to \"mm/d\". Supported units: ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day'].</p> <code>'mm/d'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the mean precipitation data in the specified units.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported unit is provided.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_mean_prcp(self, gage_id_lst: list[str], unit: str = \"mm/d\") -&gt; xr.Dataset:\n    \"\"\"Reads the mean daily precipitation for a list of basins, with unit conversion.\n\n    Args:\n        gage_id_lst: A list of basin identifiers.\n        unit: The desired unit for the output precipitation. Defaults to \"mm/d\".\n            Supported units: ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h',\n            'mm/3hour', 'mm/8d', 'mm/8day'].\n\n    Returns:\n        An xarray Dataset containing the mean precipitation data in the specified units.\n\n    Raises:\n        ValueError: If an unsupported unit is provided.\n    \"\"\"\n    prcp_var_name = \"p_mean\"\n    data_ds = self.read_attr_xrdataset(\n        gage_id_lst=gage_id_lst, var_lst=[prcp_var_name]\n    )\n    # No conversion needed\n    if unit in [\"mm/d\", \"mm/day\"]:\n        return data_ds\n\n    # Conversion needed, create a new dataset\n    converted_ds = data_ds.copy()\n    # After renaming, the variable in the dataset is now the standard name\n    if unit in [\"mm/h\", \"mm/hour\"]:\n        converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 24\n    elif unit in [\"mm/3h\", \"mm/3hour\"]:\n        converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 8\n    elif unit in [\"mm/8d\", \"mm/8day\"]:\n        converted_ds[prcp_var_name] = data_ds[prcp_var_name] * 8\n    else:\n        raise ValueError(\n            \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n        )\n    return converted_ds\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.read_object_ids","title":"<code>read_object_ids()</code>","text":"<p>Read watershed station ID list.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_object_ids(self) -&gt; np.ndarray:\n    \"\"\"Read watershed station ID list.\"\"\"\n    if hasattr(self, \"aqua_fetch\"):\n        stations_list = self.aqua_fetch.stations()\n        return np.sort(np.array(stations_list))\n    raise NotImplementedError\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.read_other_cols","title":"<code>read_other_cols(object_ids=None, other_cols=None, **kwargs)</code>","text":"<p>some data which cannot be easily treated as constant vars or time-series with same length as relevant vars CONVENTION: other_cols is a dict, where each item is also a dict with all params in it</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_other_cols(\n    self, object_ids=None, other_cols: dict = None, **kwargs\n) -&gt; dict:\n    \"\"\"some data which cannot be easily treated as constant vars or time-series with same length as relevant vars\n    CONVENTION: other_cols is a dict, where each item is also a dict with all params in it\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.HydroDataset.read_relevant_cols","title":"<code>read_relevant_cols(gage_id_lst=None, t_range=None, var_lst=None, forcing_type=None, **kwargs)</code>","text":"<p>3d data (site_num * time_length * var_num), time-series data</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_relevant_cols(\n    self, gage_id_lst=None, t_range=None, var_lst=None, forcing_type=None, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"3d data (site_num * time_length * var_num), time-series data\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.LamaHCE","title":"<code>LamaHCE</code>","text":"<p>               Bases: <code>LamaHCE</code></p> <p>Custom LamaHCE class that overrides fetch_static_features to have default value 'all' and overrides path properties to adapt to the actual dataset structure.</p> Source code in <code>hydrodataset/lamah_ce.py</code> <pre><code>class LamaHCE(_AquaFetchLamaHCE):\n    \"\"\"\n    Custom LamaHCE class that overrides fetch_static_features to have default value 'all'\n    and overrides path properties to adapt to the actual dataset structure.\n    \"\"\"\n\n    @property\n    def data_type_dir(self):\n        \"\"\"Override to adapt to the actual LamaH-CE dataset structure.\n\n        The actual structure is:\n        lamaHCE/\n            2_LamaH-CE_daily/\n                A_basins_total_upstrm/\n                B_basins_intermediate_all/\n                ...\n            1_LamaH-CE_daily_hourly/\n                A_basins_total_upstrm/\n                ...\n\n        Original AquaFetch code expected:\n        lamaHCE/\n            A_basins_total_upstrm/\n            B_basins_intermediate_all/\n            ...\n        \"\"\"\n        SEP = os.sep\n\n        # Determine which parent folder based on timestep\n        if self.timestep == \"H\":\n            parent_folder = \"1_LamaH-CE_daily_hourly\"\n        else:\n            parent_folder = \"2_LamaH-CE_daily\"\n\n        # Find the folder that ends with data_type\n        parent_path = os.path.join(self.path, parent_folder)\n\n        # List all directories in parent folder\n        if os.path.exists(parent_path):\n            dirs = [f for f in os.listdir(parent_path) if f.endswith(self.data_type)]\n            if dirs:\n                f = dirs[0]\n                return os.path.join(parent_path, f)\n\n        # Fallback: try original behavior if new structure doesn't exist\n        dirs = [f for f in os.listdir(self.path) if f.endswith(self.data_type)]\n        if dirs:\n            f = dirs[0]\n            return os.path.join(self.path, f)\n\n        raise FileNotFoundError(\n            f\"Could not find directory ending with '{self.data_type}' \"\n            f\"in {self.path} or {parent_path}\"\n        )\n\n    @property\n    def q_dir(self):\n        \"\"\"Override to adapt to the actual dataset structure.\"\"\"\n        SEP = os.sep\n\n        # Determine which parent folder based on timestep\n        if self.timestep == \"H\":\n            parent_folder = \"1_LamaH-CE_daily_hourly\"\n        else:\n            parent_folder = \"2_LamaH-CE_daily\"\n\n        # Try new structure first\n        new_path = os.path.join(self.path, parent_folder, \"D_gauges\", \"2_timeseries\")\n        if os.path.exists(new_path):\n            return new_path\n\n        # Fallback to original structure\n        return os.path.join(self.path, \"D_gauges\", \"2_timeseries\")\n\n    def gauge_attributes(self) -&gt; pd.DataFrame:\n        \"\"\"Override to adapt to the actual dataset structure.\n\n        Original code expected:\n        lamaHCE/D_gauges/1_attributes/Gauge_attributes.csv\n\n        Actual structure:\n        lamaHCE/2_LamaH-CE_daily/D_gauges/1_attributes/Gauge_attributes.csv\n        \"\"\"\n        # Determine which parent folder based on timestep\n        if self.timestep == \"H\":\n            parent_folder = \"1_LamaH-CE_daily_hourly\"\n        else:\n            parent_folder = \"2_LamaH-CE_daily\"\n\n        # Try new structure first\n        fname = os.path.join(\n            self.path, parent_folder, \"D_gauges\", \"1_attributes\", \"Gauge_attributes.csv\"\n        )\n\n        if not os.path.exists(fname):\n            # Fallback to original structure\n            fname = os.path.join(\n                self.path, \"D_gauges\", \"1_attributes\", \"Gauge_attributes.csv\"\n            )\n\n        df = pd.read_csv(fname, sep=\";\", index_col=\"ID\")\n        df.index = df.index.astype(str)\n        return df\n\n    def fetch_static_features(\n        self,\n        stations: Union[str, List[str]] = \"all\",\n        static_features: Union[str, List[str]] = \"all\",  # Changed from None to 'all'\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        static features of LamaHCE\n\n        Modified to have default static_features='all' instead of None\n\n        Parameters\n        ----------\n            stations : str\n                name/id of station of which to extract the data\n            static_features : list/str, optional (default=\"all\")\n                The name/names of features to fetch. By default, all available\n                static features are returned.\n\n        Examples\n        --------\n            &gt;&gt;&gt; from aqua_fetch import LamaHCE\n            &gt;&gt;&gt; dataset = LamaHCE(timestep='D', data_type='total_upstrm')\n            &gt;&gt;&gt; df = dataset.fetch_static_features('99')  # (1, 61)\n            ...  # get list of all static features\n            &gt;&gt;&gt; dataset.static_features\n            &gt;&gt;&gt; dataset.fetch_static_features('99',\n            &gt;&gt;&gt; static_features=['area_calc', 'elev_mean', 'agr_fra', 'sand_fra'])  # (1, 4)\n        \"\"\"\n\n        df = self.static_data()\n\n        static_features = check_attributes(\n            static_features, self.static_features, \"static features\"\n        )\n        stations = check_attributes(stations, self.stations(), \"stations\")\n\n        df = df[static_features]\n\n        df.index = df.index.astype(str)\n        df = df.loc[stations]\n        if isinstance(df, pd.Series):\n            df = pd.DataFrame(df).transpose()\n\n        return df\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.LamaHCE.data_type_dir","title":"<code>data_type_dir</code>  <code>property</code>","text":"<p>Override to adapt to the actual LamaH-CE dataset structure.</p> <p>The actual structure is: lamaHCE/     2_LamaH-CE_daily/         A_basins_total_upstrm/         B_basins_intermediate_all/         ...     1_LamaH-CE_daily_hourly/         A_basins_total_upstrm/         ...</p> <p>Original AquaFetch code expected: lamaHCE/     A_basins_total_upstrm/     B_basins_intermediate_all/     ...</p>"},{"location":"api/hydrodataset/#hydrodataset.LamaHCE.q_dir","title":"<code>q_dir</code>  <code>property</code>","text":"<p>Override to adapt to the actual dataset structure.</p>"},{"location":"api/hydrodataset/#hydrodataset.LamaHCE.fetch_static_features","title":"<code>fetch_static_features(stations='all', static_features='all')</code>","text":"<p>static features of LamaHCE</p> <p>Modified to have default static_features='all' instead of None</p>"},{"location":"api/hydrodataset/#hydrodataset.LamaHCE.fetch_static_features--parameters","title":"Parameters","text":"<pre><code>stations : str\n    name/id of station of which to extract the data\nstatic_features : list/str, optional (default=\"all\")\n    The name/names of features to fetch. By default, all available\n    static features are returned.\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.LamaHCE.fetch_static_features--examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from aqua_fetch import LamaHCE\n&gt;&gt;&gt; dataset = LamaHCE(timestep='D', data_type='total_upstrm')\n&gt;&gt;&gt; df = dataset.fetch_static_features('99')  # (1, 61)\n...  # get list of all static features\n&gt;&gt;&gt; dataset.static_features\n&gt;&gt;&gt; dataset.fetch_static_features('99',\n&gt;&gt;&gt; static_features=['area_calc', 'elev_mean', 'agr_fra', 'sand_fra'])  # (1, 4)\n</code></pre> Source code in <code>hydrodataset/lamah_ce.py</code> <pre><code>def fetch_static_features(\n    self,\n    stations: Union[str, List[str]] = \"all\",\n    static_features: Union[str, List[str]] = \"all\",  # Changed from None to 'all'\n) -&gt; pd.DataFrame:\n    \"\"\"\n    static features of LamaHCE\n\n    Modified to have default static_features='all' instead of None\n\n    Parameters\n    ----------\n        stations : str\n            name/id of station of which to extract the data\n        static_features : list/str, optional (default=\"all\")\n            The name/names of features to fetch. By default, all available\n            static features are returned.\n\n    Examples\n    --------\n        &gt;&gt;&gt; from aqua_fetch import LamaHCE\n        &gt;&gt;&gt; dataset = LamaHCE(timestep='D', data_type='total_upstrm')\n        &gt;&gt;&gt; df = dataset.fetch_static_features('99')  # (1, 61)\n        ...  # get list of all static features\n        &gt;&gt;&gt; dataset.static_features\n        &gt;&gt;&gt; dataset.fetch_static_features('99',\n        &gt;&gt;&gt; static_features=['area_calc', 'elev_mean', 'agr_fra', 'sand_fra'])  # (1, 4)\n    \"\"\"\n\n    df = self.static_data()\n\n    static_features = check_attributes(\n        static_features, self.static_features, \"static features\"\n    )\n    stations = check_attributes(stations, self.stations(), \"stations\")\n\n    df = df[static_features]\n\n    df.index = df.index.astype(str)\n    df = df.loc[stations]\n    if isinstance(df, pd.Series):\n        df = pd.DataFrame(df).transpose()\n\n    return df\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.LamaHCE.gauge_attributes","title":"<code>gauge_attributes()</code>","text":"<p>Override to adapt to the actual dataset structure.</p> <p>Original code expected: lamaHCE/D_gauges/1_attributes/Gauge_attributes.csv</p> <p>Actual structure: lamaHCE/2_LamaH-CE_daily/D_gauges/1_attributes/Gauge_attributes.csv</p> Source code in <code>hydrodataset/lamah_ce.py</code> <pre><code>def gauge_attributes(self) -&gt; pd.DataFrame:\n    \"\"\"Override to adapt to the actual dataset structure.\n\n    Original code expected:\n    lamaHCE/D_gauges/1_attributes/Gauge_attributes.csv\n\n    Actual structure:\n    lamaHCE/2_LamaH-CE_daily/D_gauges/1_attributes/Gauge_attributes.csv\n    \"\"\"\n    # Determine which parent folder based on timestep\n    if self.timestep == \"H\":\n        parent_folder = \"1_LamaH-CE_daily_hourly\"\n    else:\n        parent_folder = \"2_LamaH-CE_daily\"\n\n    # Try new structure first\n    fname = os.path.join(\n        self.path, parent_folder, \"D_gauges\", \"1_attributes\", \"Gauge_attributes.csv\"\n    )\n\n    if not os.path.exists(fname):\n        # Fallback to original structure\n        fname = os.path.join(\n            self.path, \"D_gauges\", \"1_attributes\", \"Gauge_attributes.csv\"\n        )\n\n    df = pd.read_csv(fname, sep=\";\", index_col=\"ID\")\n    df.index = df.index.astype(str)\n    return df\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.LamahCe","title":"<code>LamahCe</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>LamaHCE dataset class extending HydroDataset.</p> <p>This class provides access to the LamaHCE dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/lamah_ce.py</code> <pre><code>class LamahCe(HydroDataset):\n    \"\"\"LamaHCE dataset class extending HydroDataset.\n\n    This class provides access to the LamaHCE dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        region: Optional[str] = None,\n        download: bool = False,\n        cache_path: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize LamaHCE dataset.\n\n        Args:\n            data_path: Path to the LamaHCE data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n            cache_path: Path to the cache directory\n        \"\"\"\n        super().__init__(data_path, cache_path=cache_path)\n        self.region = region\n        self.download = download\n        # Use the custom LamaHCE class defined at module level\n        self.aqua_fetch = LamaHCE(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"lamahce_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"lamahce_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1981-01-01\", \"2019-12-31\"]\n\n    # get the information of features from table 3 in \"https://doi.org/10.5194/essd-13-4529-2021\"\n    # Static variable definitions based on inspected data\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    # Dynamic variable mapping based on inspected data\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"},\n                \"dp\": {\"specific_name\": \"dptemp_c_max_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"},\n                \"dp\": {\"specific_name\": \"dptemp_c_min_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"},\n                \"dp\": {\"specific_name\": \"dptemp_c_mean_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"total_et\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"swe_mm\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"solrad_wm2_max\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.THERMAL_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"thermrad_wm2\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.THERMAL_RADIATION_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"thermrad_wm2_max\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airpres_hpa\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"windspeedu_mps\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"windspeedv_mps\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"volsw_123\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"volsw_4\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.LamahCe.__init__","title":"<code>__init__(data_path, region=None, download=False, cache_path=None)</code>","text":"<p>Initialize LamaHCE dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the LamaHCE data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> <code>cache_path</code> <code>Optional[str]</code> <p>Path to the cache directory</p> <code>None</code> Source code in <code>hydrodataset/lamah_ce.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    region: Optional[str] = None,\n    download: bool = False,\n    cache_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize LamaHCE dataset.\n\n    Args:\n        data_path: Path to the LamaHCE data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n        cache_path: Path to the cache directory\n    \"\"\"\n    super().__init__(data_path, cache_path=cache_path)\n    self.region = region\n    self.download = download\n    # Use the custom LamaHCE class defined at module level\n    self.aqua_fetch = LamaHCE(data_path)\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.StandardVariable","title":"<code>StandardVariable</code>","text":"<p>A class to hold standardized variable names as constants.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>class StandardVariable:\n    \"\"\"A class to hold standardized variable names as constants.\"\"\"\n\n    STREAMFLOW = \"streamflow\"\n    WATER_LEVEL = \"water_level\"\n\n    PRECIPITATION = \"precipitation\"\n    CRAINF_FRAC = \"crainf_frac\"  # Fraction of total precipitation that is convective\n    PRECIPITATION_MIN = \"precipitation_min\"\n    PRECIPITATION_MAX = \"precipitation_max\"\n    PRECIPITATION_MEDIAN = \"precipitation_median\"\n\n    TEMPERATURE_MAX = \"temperature_max\"\n    TEMPERATURE_MIN = \"temperature_min\"\n    TEMPERATURE_MEAN = \"temperature_mean\"\n\n    DAYLIGHT_DURATION = \"daylight_duration\"\n    RELATIVE_DAYLIGHT_DURATION = \"relative_daylight_duration\"\n\n    SOLAR_RADIATION = \"solar_radiation\"\n    SOLAR_RADIATION_MIN = \"solar_radiation_min\"\n    SOLAR_RADIATION_MAX = \"solar_radiation_max\"\n    SOLAR_RADIATION_MEDIAN = \"solar_radiation_median\"\n    THERMAL_RADIATION = \"thermal_radiation\"\n    THERMAL_RADIATION_MIN = \"thermal_radiation_min\"\n    THERMAL_RADIATION_MAX = \"thermal_radiation_max\"\n    LONGWAVE_SOLAR_RADIATION = \"longwave_solar_radiation\"\n\n    SNOW_WATER_EQUIVALENT = \"snow_water_equivalent\"\n    SNOW_WATER_EQUIVALENT_MIN = \"snow_water_equivalent_min\"\n    SNOW_WATER_EQUIVALENT_MAX = \"snow_water_equivalent_max\"\n    SNOW_DEPTH = \"snow_depth\"\n    SNOW_COVER = \"snow_cover\"\n    SNOW_SUBLIMATION = \"snow_sublimation\"\n    SNOW_DENSITY = \"snow_density\"\n\n    VAPOR_PRESSURE = \"vapor_pressure\"\n\n    SURFACE_PRESSURE = \"surface_pressure\"\n    SURFACE_PRESSURE_MIN = \"surface_pressure_min\"\n    SURFACE_PRESSURE_MAX = \"surface_pressure_max\"\n\n    WIND_SPEED = \"wind_speed\"\n    U_WIND_SPEED = \"u_wind_speed\"\n    U_WIND_SPEED_MIN = \"u_wind_speed_min\"\n    U_WIND_SPEED_MAX = \"u_wind_speed_max\"\n    V_WIND_SPEED = \"v_wind_speed\"\n    V_WIND_SPEED_MIN = \"v_wind_speed_min\"\n    V_WIND_SPEED_MAX = \"v_wind_speed_max\"\n    WIND_DIR = \"wind_dir\"\n    LOW_LEVEL_WIND_SHEAR = \"low_level_wind_shear\"\n    DEEP_LEVEL_WIND_SHEAR = \"deep_level_wind_shear\"\n\n    RELATIVE_HUMIDITY = \"relative_humidity\"\n    SPECIFIC_HUMIDITY = \"specific_humidity\"\n    RELATIVE_HUMIDITY_MIN = \"relative_humidity_min\"\n    RELATIVE_HUMIDITY_MAX = \"relative_humidity_max\"\n    RELATIVE_HUMIDITY_MEDIAN = \"relative_humidity_median\"\n    TOTAL_COLUMN_WATER_VAPOUR = \"total_column_water_vapour\"\n\n    CAPE = \"cape\"  # Convective available potential energy\n    CIN = \"cin\"  # Convective inhibition\n\n    POTENTIAL_EVAPOTRANSPIRATION = \"potential_evapotranspiration\"\n    EVAPORATION = \"evaporation\"\n    EVAPOTRANSPIRATION = \"evapotranspiration\"\n\n    SOIL_MOISTURE = \"soil_moisture\"\n    VOLUMETRIC_SOIL_WATER_LAYER1 = \"volumetric_soil_water_layer1\"  # 0-7cm\n    VOLUMETRIC_SOIL_WATER_LAYER1_MIN = \"volumetric_soil_water_layer1_min\"\n    VOLUMETRIC_SOIL_WATER_LAYER1_MAX = \"volumetric_soil_water_layer1_max\"\n    VOLUMETRIC_SOIL_WATER_LAYER2 = \"volumetric_soil_water_layer2\"  # 7-28cm\n    VOLUMETRIC_SOIL_WATER_LAYER2_MIN = \"volumetric_soil_water_layer2_min\"\n    VOLUMETRIC_SOIL_WATER_LAYER2_MAX = \"volumetric_soil_water_layer2_max\"\n    VOLUMETRIC_SOIL_WATER_LAYER3 = \"volumetric_soil_water_layer3\"  # 28-100cm\n    VOLUMETRIC_SOIL_WATER_LAYER3_MIN = \"volumetric_soil_water_layer3_min\"\n    VOLUMETRIC_SOIL_WATER_LAYER3_MAX = \"volumetric_soil_water_layer3_max\"\n    VOLUMETRIC_SOIL_WATER_LAYER4 = \"volumetric_soil_water_layer4\"  # 100-289cm\n    VOLUMETRIC_SOIL_WATER_LAYER4_MIN = \"volumetric_soil_water_layer4_min\"\n    VOLUMETRIC_SOIL_WATER_LAYER4_MAX = \"volumetric_soil_water_layer4_max\"\n\n    MIN_RAIN_RATE = \"min_rain_rate\"\n    MAX_RAIN_RATE = \"max_rain_rate\"\n\n    GROUND_HEAT_FLUX = \"ground_heat_flux\"\n</code></pre>"},{"location":"api/hydrodataset/#hydrodataset.time_intersect_dynamic_data","title":"<code>time_intersect_dynamic_data(obs, date, t_range)</code>","text":"<p>chose data from obs in the t_range</p>"},{"location":"api/hydrodataset/#hydrodataset.time_intersect_dynamic_data--parameters","title":"Parameters","text":"<p>obs     a np array date     all periods for obs t_range     the time range we need, such as [\"1990-01-01\",\"2000-01-01\"]</p>"},{"location":"api/hydrodataset/#hydrodataset.time_intersect_dynamic_data--returns","title":"Returns","text":"<p>np.array     the chosen data</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def time_intersect_dynamic_data(obs: np.array, date: np.array, t_range: list):\n    \"\"\"\n    chose data from obs in the t_range\n\n    Parameters\n    ----------\n    obs\n        a np array\n    date\n        all periods for obs\n    t_range\n        the time range we need, such as [\"1990-01-01\",\"2000-01-01\"]\n\n    Returns\n    -------\n    np.array\n        the chosen data\n    \"\"\"\n    t_lst = pd.date_range(start=t_range[0], end=t_range[1], freq=\"D\").values\n    nt = t_lst.shape[0]\n    if len(obs) != nt:\n        out = np.full([nt], np.nan)\n        [c, ind1, ind2] = np.intersect1d(date, t_lst, return_indices=True)\n        out[ind2] = obs[ind1]\n    else:\n        out = obs\n    return out\n</code></pre>"},{"location":"api/hysets/","title":"HYSETS","text":""},{"location":"api/hysets/#overview","title":"Overview","text":"<p>HYSETS is a hydrological dataset for North America. North American hydrological dataset with extensive coverage across Canada and parts of the United States.</p>"},{"location":"api/hysets/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: North America</li> <li>Module: <code>hydrodataset.hysets</code></li> <li>Class: <code>Hysets</code></li> </ul>"},{"location":"api/hysets/#features","title":"Features","text":""},{"location":"api/hysets/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/hysets/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available: - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - And additional variables depending on the dataset</p>"},{"location":"api/hysets/#usage","title":"Usage","text":""},{"location":"api/hysets/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.hysets import Hysets\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = Hysets(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Check default time range\nprint(f\"Default time range: {ds.default_t_range}\")\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/hysets/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"2020-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/hysets/#working-with-multiple-data-sources","title":"Working with Multiple Data Sources","text":"<p>If the dataset provides multiple sources for variables:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/hysets/#api-reference","title":"API Reference","text":""},{"location":"api/hysets/#hydrodataset.hysets.Hysets","title":"<code>hydrodataset.hysets.Hysets</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>HYsets dataset class extending RainfallRunoff.</p> <p>This class provides access to the HYsets dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/hysets.py</code> <pre><code>class Hysets(HydroDataset):\n    \"\"\"HYsets dataset class extending RainfallRunoff.\n\n    This class provides access to the HYsets dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize HYsets dataset.\n\n        Args:\n            data_path: Path to the HYsets data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = HYSETS(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"hysets_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"hysets_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1950-01-01\", \"2023-12-31\"]\n\n    def cache_attributes_xrdataset(self):\n        \"\"\"Override base method to add calculated p_mean from precipitation timeseries.\n\n        This method:\n        1. Calls parent method to create base attribute cache\n        2. Reads precipitation timeseries data\n        3. Calculates mean precipitation (p_mean) for each basin\n        4. Adds p_mean to the attribute dataset\n        5. Saves the updated cache\n        \"\"\"\n        # Step 1: Create base attribute cache using parent method\n        print(\"Creating base attribute cache...\")\n        super().cache_attributes_xrdataset()\n\n        # Step 2: Load the base cache file\n        cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n        with xr.open_dataset(cache_file) as ds_attr:\n            ds_attr = ds_attr.load()  # Load into memory\n\n        print(\"Calculating p_mean from precipitation timeseries...\")\n\n        # Step 3: Read precipitation timeseries for all basins\n        basin_ids = self.read_object_ids().tolist()\n\n        try:\n            # Read full precipitation timeseries\n            prcp_ts = self.read_ts_xrdataset(\n                gage_id_lst=basin_ids,\n                t_range=self.default_t_range,\n                var_lst=[\"precipitation\"],\n            )\n\n            # Step 4: Calculate temporal mean for each basin\n            # The result is a DataArray with dimension (basin,)\n            p_mean_values = prcp_ts[\"precipitation\"].mean(dim=\"time\")\n\n            # Add units attribute\n            p_mean_values.attrs[\"units\"] = \"mm/day\"\n            p_mean_values.attrs[\"description\"] = (\n                \"Mean daily precipitation (calculated from timeseries)\"\n            )\n\n            # Step 5: Add p_mean to the attribute dataset\n            ds_attr[\"p_mean\"] = p_mean_values\n\n            print(f\"Successfully calculated p_mean for {len(basin_ids)} basins\")\n\n        except Exception as e:\n            print(f\"Warning: Could not calculate p_mean from precipitation data: {e}\")\n            print(\"Creating p_mean with NaN values as placeholder\")\n            # Create p_mean with NaN values if calculation fails\n            p_mean_nan = xr.DataArray(\n                np.full(len(basin_ids), np.nan),\n                coords={\"basin\": basin_ids},\n                dims=[\"basin\"],\n                attrs={\n                    \"units\": \"mm/day\",\n                    \"description\": \"Mean daily precipitation (not available)\",\n                },\n            )\n            ds_attr[\"p_mean\"] = p_mean_nan\n\n        # Step 6: Save the updated cache file\n        print(f\"Saving updated attribute cache with p_mean to: {cache_file}\")\n        ds_attr.to_netcdf(cache_file, mode=\"w\")\n        print(\"Successfully saved attribute cache with p_mean\")\n\n    _subclass_static_definitions = {\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations_cms\",\n            \"sources\": {\n                \"observations_cms\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n                \"observations_mm\": {\"specific_name\": \"q_mm_obs\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"observations\",\n            \"sources\": {\"observations\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_c_2m_max\", \"unit\": \"\u00b0C\"}\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_c_2m_min\", \"unit\": \"\u00b0C\"}\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"dptemp_c_mean_2m\", \"unit\": \"\u00b0C\"}\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"},\n                \"net\": {\"specific_name\": \"solradnet_wm2\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.EVAPORATION: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"evap_mm\", \"unit\": \"mm/day\"},\n                \"snow\": {\"specific_name\": \"evap_mm_snow\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"observations\",\n            \"sources\": {\"observations\": {\"specific_name\": \"swe_mm\", \"unit\": \"mm\"}},\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airpres_hpa\", \"unit\": \"hPa\"}\n            },\n        },\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"windspeedu_mps\", \"unit\": \"m/s\"}\n            },\n        },\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"windspeedv_mps\", \"unit\": \"m/s\"}\n            },\n        },\n        StandardVariable.LONGWAVE_SOLAR_RADIATION: {\n            \"default_source\": \"downward\",\n            \"sources\": {\n                \"downward\": {\"specific_name\": \"lwdownrad_wm2\", \"unit\": \"W/m^2\"},\n                \"net\": {\"specific_name\": \"lwnetrad_wm2\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SNOW_DENSITY: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"snowdensity_kgm3\", \"unit\": \"kg/m^3\"}\n            },\n        },\n    }\n</code></pre>"},{"location":"api/hysets/#hydrodataset.hysets.Hysets.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/hysets/#hydrodataset.hysets.Hysets.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize HYsets dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the HYsets data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/hysets.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize HYsets dataset.\n\n    Args:\n        data_path: Path to the HYsets data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = HYSETS(data_path)\n</code></pre>"},{"location":"api/lamah_ce/","title":"LamaH-CE","text":""},{"location":"api/lamah_ce/#overview","title":"Overview","text":"<p>LamaH-CE is the Central Europe large-sample hydrological dataset. Large-sample hydrological dataset for Central Europe, covering diverse Alpine and pre-Alpine catchments with high-quality data.</p>"},{"location":"api/lamah_ce/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Central Europe</li> <li>Project: LamaH (Large-sample hydrological data and models)</li> <li>Module: <code>hydrodataset.lamah_ce</code></li> <li>Class: <code>LamahCe</code></li> </ul>"},{"location":"api/lamah_ce/#about-lamah","title":"About LamaH","text":"<p>LamaH (Large-sample hydrological data and models) provides comprehensive hydrological data for research and modeling:</p>"},{"location":"api/lamah_ce/#key-features","title":"Key Features","text":"<ul> <li>High-quality, quality-controlled data</li> <li>Extensive catchment attributes</li> <li>Multiple temporal resolutions</li> <li>Detailed metadata</li> <li>Suitable for large-sample hydrology studies</li> </ul>"},{"location":"api/lamah_ce/#research-applications","title":"Research Applications","text":"<ul> <li>Hydrological model development and testing</li> <li>Climate change impact studies</li> <li>Regionalization studies</li> <li>Machine learning applications</li> <li>Comparative hydrology</li> </ul>"},{"location":"api/lamah_ce/#features","title":"Features","text":""},{"location":"api/lamah_ce/#static-attributes","title":"Static Attributes","text":"<p>Comprehensive static catchment attributes: - Basin geometry and area - Topographic characteristics (elevation, slope) - Land cover information - Soil properties and classes - Geological characteristics - Climate indices - Human influence indicators</p>"},{"location":"api/lamah_ce/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available: - Streamflow (observed) - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Snow water equivalent - Solar radiation - Humidity - And more...</p>"},{"location":"api/lamah_ce/#usage","title":"Usage","text":""},{"location":"api/lamah_ce/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.lamah_ce import LamahCe\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = LamahCe(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Check default time range\nprint(f\"Default time range: {ds.default_t_range}\")\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/lamah_ce/#advanced-analysis","title":"Advanced Analysis","text":"<pre><code># Read multiple variables for detailed analysis\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"2020-12-31\"],\n    var_lst=[\n        \"streamflow\",\n        \"precipitation\", \n        \"temperature_mean\",\n        \"temperature_min\",\n        \"temperature_max\",\n        \"pet\",\n        \"snow_water_equivalent\"\n    ]\n)\n\n# Analyze snow-influenced catchments\nimport xarray as xr\nwinter_months = ts_data.sel(time=ts_data.time.dt.month.isin([12, 1, 2]))\nmean_swe = winter_months[\"snow_water_equivalent\"].mean(dim=\"time\")\nprint(\"Mean winter SWE:\", mean_swe)\n</code></pre>"},{"location":"api/lamah_ce/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"2000-01-01\", \"2010-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/lamah_ce/#data-quality-and-completeness","title":"Data Quality and Completeness","text":"<p>LamaH datasets feature: - Rigorous quality control procedures - Documentation of data gaps - Metadata completeness - Peer-reviewed methodology - Regular updates</p>"},{"location":"api/lamah_ce/#regional-characteristics","title":"Regional Characteristics","text":""},{"location":"api/lamah_ce/#lamah-ce_1","title":"LamaH-CE","text":"<ul> <li>Alpine and pre-Alpine catchments</li> <li>Snow-influenced hydrology</li> <li>Elevation range from lowlands to high mountains</li> <li>Mixed land use patterns</li> </ul>"},{"location":"api/lamah_ce/#lamah-ice","title":"LamaH-ICE","text":"<ul> <li>Volcanic landscapes</li> <li>Glacial-influenced catchments</li> <li>Geothermal activity impact</li> <li>Unique geological conditions</li> </ul>"},{"location":"api/lamah_ce/#api-reference","title":"API Reference","text":""},{"location":"api/lamah_ce/#hydrodataset.lamah_ce.LamahCe","title":"<code>hydrodataset.lamah_ce.LamahCe</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>LamaHCE dataset class extending HydroDataset.</p> <p>This class provides access to the LamaHCE dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/lamah_ce.py</code> <pre><code>class LamahCe(HydroDataset):\n    \"\"\"LamaHCE dataset class extending HydroDataset.\n\n    This class provides access to the LamaHCE dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        region: Optional[str] = None,\n        download: bool = False,\n        cache_path: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize LamaHCE dataset.\n\n        Args:\n            data_path: Path to the LamaHCE data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n            cache_path: Path to the cache directory\n        \"\"\"\n        super().__init__(data_path, cache_path=cache_path)\n        self.region = region\n        self.download = download\n        # Use the custom LamaHCE class defined at module level\n        self.aqua_fetch = LamaHCE(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"lamahce_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"lamahce_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1981-01-01\", \"2019-12-31\"]\n\n    # get the information of features from table 3 in \"https://doi.org/10.5194/essd-13-4529-2021\"\n    # Static variable definitions based on inspected data\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    # Dynamic variable mapping based on inspected data\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"},\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"},\n                \"dp\": {\"specific_name\": \"dptemp_c_max_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"},\n                \"dp\": {\"specific_name\": \"dptemp_c_min_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"},\n                \"dp\": {\"specific_name\": \"dptemp_c_mean_2m\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"total_et\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"swe_mm\", \"unit\": \"mm\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"solrad_wm2_max\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.THERMAL_RADIATION: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"thermrad_wm2\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.THERMAL_RADIATION_MAX: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"thermrad_wm2_max\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"airpres_hpa\", \"unit\": \"Pa\"},\n            },\n        },\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"windspeedu_mps\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"windspeedv_mps\", \"unit\": \"m/s\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"volsw_123\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4: {\n            \"default_source\": \"era5\",\n            \"sources\": {\n                \"era5\": {\"specific_name\": \"volsw_4\", \"unit\": \"m^3/m^3\"},\n            },\n        },\n    }\n</code></pre>"},{"location":"api/lamah_ce/#hydrodataset.lamah_ce.LamahCe.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/lamah_ce/#hydrodataset.lamah_ce.LamahCe.__init__","title":"<code>__init__(data_path, region=None, download=False, cache_path=None)</code>","text":"<p>Initialize LamaHCE dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the LamaHCE data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> <code>cache_path</code> <code>Optional[str]</code> <p>Path to the cache directory</p> <code>None</code> Source code in <code>hydrodataset/lamah_ce.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    region: Optional[str] = None,\n    download: bool = False,\n    cache_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize LamaHCE dataset.\n\n    Args:\n        data_path: Path to the LamaHCE data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n        cache_path: Path to the cache directory\n    \"\"\"\n    super().__init__(data_path, cache_path=cache_path)\n    self.region = region\n    self.download = download\n    # Use the custom LamaHCE class defined at module level\n    self.aqua_fetch = LamaHCE(data_path)\n</code></pre>"},{"location":"api/lamah_ice/","title":"LamaH-ICE","text":""},{"location":"api/lamah_ice/#overview","title":"Overview","text":"<p>LamaH-ICE is the Iceland large-sample hydrological dataset. Large-sample hydrological dataset for Iceland, featuring volcanic and glacial-influenced catchments with unique characteristics.</p>"},{"location":"api/lamah_ice/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Iceland</li> <li>Project: LamaH (Large-sample hydrological data and models)</li> <li>Module: <code>hydrodataset.lamah_ice</code></li> <li>Class: <code>LamahIce</code></li> </ul>"},{"location":"api/lamah_ice/#about-lamah","title":"About LamaH","text":"<p>LamaH (Large-sample hydrological data and models) provides comprehensive hydrological data for research and modeling:</p>"},{"location":"api/lamah_ice/#key-features","title":"Key Features","text":"<ul> <li>High-quality, quality-controlled data</li> <li>Extensive catchment attributes</li> <li>Multiple temporal resolutions</li> <li>Detailed metadata</li> <li>Suitable for large-sample hydrology studies</li> </ul>"},{"location":"api/lamah_ice/#research-applications","title":"Research Applications","text":"<ul> <li>Hydrological model development and testing</li> <li>Climate change impact studies</li> <li>Regionalization studies</li> <li>Machine learning applications</li> <li>Comparative hydrology</li> </ul>"},{"location":"api/lamah_ice/#features","title":"Features","text":""},{"location":"api/lamah_ice/#static-attributes","title":"Static Attributes","text":"<p>Comprehensive static catchment attributes: - Basin geometry and area - Topographic characteristics (elevation, slope) - Land cover information - Soil properties and classes - Geological characteristics - Climate indices - Human influence indicators</p>"},{"location":"api/lamah_ice/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available: - Streamflow (observed) - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - Snow water equivalent - Solar radiation - Humidity - And more...</p>"},{"location":"api/lamah_ice/#usage","title":"Usage","text":""},{"location":"api/lamah_ice/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.lamah_ice import LamahIce\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = LamahIce(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Check default time range\nprint(f\"Default time range: {ds.default_t_range}\")\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/lamah_ice/#advanced-analysis","title":"Advanced Analysis","text":"<pre><code># Read multiple variables for detailed analysis\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"2020-12-31\"],\n    var_lst=[\n        \"streamflow\",\n        \"precipitation\", \n        \"temperature_mean\",\n        \"temperature_min\",\n        \"temperature_max\",\n        \"pet\",\n        \"snow_water_equivalent\"\n    ]\n)\n\n# Analyze snow-influenced catchments\nimport xarray as xr\nwinter_months = ts_data.sel(time=ts_data.time.dt.month.isin([12, 1, 2]))\nmean_swe = winter_months[\"snow_water_equivalent\"].mean(dim=\"time\")\nprint(\"Mean winter SWE:\", mean_swe)\n</code></pre>"},{"location":"api/lamah_ice/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"2000-01-01\", \"2010-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/lamah_ice/#data-quality-and-completeness","title":"Data Quality and Completeness","text":"<p>LamaH datasets feature: - Rigorous quality control procedures - Documentation of data gaps - Metadata completeness - Peer-reviewed methodology - Regular updates</p>"},{"location":"api/lamah_ice/#regional-characteristics","title":"Regional Characteristics","text":""},{"location":"api/lamah_ice/#lamah-ce","title":"LamaH-CE","text":"<ul> <li>Alpine and pre-Alpine catchments</li> <li>Snow-influenced hydrology</li> <li>Elevation range from lowlands to high mountains</li> <li>Mixed land use patterns</li> </ul>"},{"location":"api/lamah_ice/#lamah-ice_1","title":"LamaH-ICE","text":"<ul> <li>Volcanic landscapes</li> <li>Glacial-influenced catchments</li> <li>Geothermal activity impact</li> <li>Unique geological conditions</li> </ul>"},{"location":"api/lamah_ice/#api-reference","title":"API Reference","text":""},{"location":"api/lamah_ice/#hydrodataset.lamah_ice.LamahIce","title":"<code>hydrodataset.lamah_ice.LamahIce</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>LamaHICE dataset class extending HydroDataset.</p> <p>This class uses a custom data reading implementation to support a newer dataset version than the one supported by the underlying aquafetch library. It overrides the download URLs and provides updated methods.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> Source code in <code>hydrodataset/lamah_ice.py</code> <pre><code>class LamahIce(HydroDataset):\n    \"\"\"LamaHICE dataset class extending HydroDataset.\n\n    This class uses a custom data reading implementation to support a newer\n    dataset version than the one supported by the underlying aquafetch library.\n    It overrides the download URLs and provides updated methods.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize LamaHICE dataset.\n\n        Args:\n            data_path: Path to the LamaHICE data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n\n        # Use the custom LamaHIce class defined at module level\n        self.aqua_fetch = LamaHIce(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"lamahice_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"lamahice_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1950-01-01\", \"2021-12-31\"]\n\n    # Define standardized static variable mappings\n    # Based on aqua_fetch LamaHIce static_map\n    # information of features get from pdf  https://www.hydroshare.org/resource/705d69c0f77c48538d83cf383f8c63d6/\n    _subclass_static_definitions = {\n        \"p_mean\": {\"specific_name\": \"p_mean_basin\", \"unit\": \"mm\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n    }\n\n    # Define standardized dynamic variable mappings\n    # Based on aqua_fetch LamaHIce dyn_map\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\"lamah_ice\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"}},\n            \"sources\": {\"carra\": {\"specific_name\": \"runoff_carra\", \"unit\": \"mm\"}},\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\"lamah_ice\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm\"}},\n            \"sources\": {\"carra\": {\"specific_name\": \"prec_carra\", \"unit\": \"mm\"}},\n            \"sources\": {\"rav\": {\"specific_name\": \"prec_rav\", \"unit\": \"mm\"}},\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\n                \"lamah_ice\": {\"specific_name\": \"airtemp_c_2m_min\", \"unit\": \"\u00b0C\"}\n            },\n            \"sources\": {\"dp\": {\"specific_name\": \"2m_dp_temp_min\", \"unit\": \"\u00b0C\"}},\n            \"sources\": {\"carra\": {\"specific_name\": \"2m_temp_min_carra\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\n                \"lamah_ice\": {\"specific_name\": \"airtemp_c_2m_max\", \"unit\": \"\u00b0C\"}\n            },\n            \"sources\": {\"dp\": {\"specific_name\": \"2m_dp_temp_max\", \"unit\": \"\u00b0C\"}},\n            \"sources\": {\"carra\": {\"specific_name\": \"2m_temp_max_carra\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\n                \"lamah_ice\": {\"specific_name\": \"airtemp_c_mean_2m\", \"unit\": \"\u00b0C\"}\n            },\n            \"sources\": {\"dp\": {\"specific_name\": \"2m_dp_temp_mean\", \"unit\": \"\u00b0C\"}},\n            \"sources\": {\"rav\": {\"specific_name\": \"2m_temp_rav\", \"unit\": \"\u00b0C\"}},\n            \"sources\": {\"carra\": {\"specific_name\": \"2m_temp_carra\", \"unit\": \"\u00b0C\"}},\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\"lamah_ice\": {\"specific_name\": \"pet_mm\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"rav\",\n            \"sources\": {\"ref\": {\"specific_name\": \"ref_et_mm\", \"unit\": \"mm/day\"}},\n            \"sources\": {\"rav\": {\"specific_name\": \"total_et_rav\", \"unit\": \"mm/day\"}},\n            \"sources\": {\"carra\": {\"specific_name\": \"total_et_carra\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.U_WIND_SPEED: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\"lamah_ice\": {\"specific_name\": \"10m_wind_u\", \"unit\": \"m/s\"}},\n            \"sources\": {\"rav\": {\"specific_name\": \"10m_wind_u_rav\", \"unit\": \"m/s\"}},\n        },\n        StandardVariable.V_WIND_SPEED: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\"lamah_ice\": {\"specific_name\": \"10m_wind_v\", \"unit\": \"m/s\"}},\n            \"sources\": {\"rav\": {\"specific_name\": \"10m_wind_v_rav\", \"unit\": \"m/s\"}},\n        },\n        StandardVariable.WIND_SPEED: {\n            \"default_source\": \"carra\",\n            \"sources\": {\n                \"carra\": {\"specific_name\": \"10m_wind_speed_carra\", \"unit\": \"m/s\"}\n            },\n        },\n        StandardVariable.WIND_DIR: {\n            \"default_source\": \"carra\",\n            \"sources\": {\n                \"carra\": {\"specific_name\": \"10m_wind_dir_carra\", \"unit\": \"degree\"}\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\"lamah_ice\": {\"specific_name\": \"swe\", \"unit\": \"mm\"}},\n            \"sources\": {\"carra\": {\"specific_name\": \"swe_carra\", \"unit\": \"mm\"}},\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\n                \"lamah_ice\": {\n                    \"specific_name\": \"surf_net_solar_rad_mean\",\n                    \"unit\": \"W/m^2\",\n                }\n            },\n            \"sources\": {\n                \"rav\": {\"specific_name\": \"surf_dwn_solar_rad_rav\", \"unit\": \"W/m^2\"}\n            },\n            \"sources\": {\n                \"carra\": {\"specific_name\": \"surf_net_solar_rad_carra\", \"unit\": \"W/m^2\"}\n            },\n            \"sources\": {\n                \"dwn_carra\": {\n                    \"specific_name\": \"surf_dwn_solar_rad_carra\",\n                    \"unit\": \"W/m^2\",\n                }\n            },\n        },\n        StandardVariable.SOLAR_RADIATION_MAX: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\n                \"lamah_ice\": {\n                    \"specific_name\": \"surf_net_solar_rad_max\",\n                    \"unit\": \"W/m^2\",\n                }\n            },\n        },\n        StandardVariable.THERMAL_RADIATION: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\n                \"lamah_ice\": {\n                    \"specific_name\": \"surf_net_therm_rad_mean\",\n                    \"unit\": \"W/m^2\",\n                }\n            },\n            \"sources\": {\n                \"outg\": {\"specific_name\": \"surf_outg_therm_rad_rav\", \"unit\": \"W/m^2\"}\n            },\n            \"sources\": {\n                \"dwn\": {\"specific_name\": \"surf_dwn_therm_rad_rav\", \"unit\": \"W/m^2\"}\n            },\n            \"sources\": {\n                \"carra\": {\"specific_name\": \"surf_net_therm_rad_carra\", \"unit\": \"W/m^2\"}\n            },\n            \"sources\": {\n                \"dwn_carra\": {\n                    \"specific_name\": \"surf_dwn_therm_rad_carra\",\n                    \"unit\": \"W/m^2\",\n                }\n            },\n        },\n        StandardVariable.THERMAL_RADIATION_MAX: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\n                \"lamah_ice\": {\n                    \"specific_name\": \"surf_net_therm_rad_max\",\n                    \"unit\": \"W/m^2\",\n                }\n            },\n        },\n        StandardVariable.SURFACE_PRESSURE: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\"lamah_ice\": {\"specific_name\": \"surf_press\", \"unit\": \"Pa\"}},\n            \"sources\": {\"rav\": {\"specific_name\": \"surf_press_rav\", \"unit\": \"Pa\"}},\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"lamah_ice\",\n            \"sources\": {\"lamah_ice\": {\"specific_name\": \"pet_mm\", \"unit\": \"mm\"}},\n            \"sources\": {\n                \"caravan\": {\n                    \"specific_name\": \"potential_evaporation_sum_fao_penman_monteith_from_caravan\",\n                    \"unit\": \"mm/day\",\n                }\n            },\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER1: {\n            \"default_source\": \"rav\",\n            \"sources\": {\"rav\": {\"specific_name\": \"volsw_123\", \"unit\": \"mm\"}},\n        },\n        StandardVariable.VOLUMETRIC_SOIL_WATER_LAYER4: {\n            \"default_source\": \"rav\",\n            \"sources\": {\"rav\": {\"specific_name\": \"volsw_4\", \"unit\": \"mm\"}},\n        },\n        StandardVariable.RELATIVE_HUMIDITY: {\n            \"default_source\": \"rav\",\n            \"sources\": {\"rav\": {\"specific_name\": \"2m_qv_rav\", \"unit\": \"m/s\"}},\n            \"sources\": {\"carra\": {\"specific_name\": \"2m_rel_hum_carra\", \"unit\": \"m/s\"}},\n        },\n        StandardVariable.SPECIFIC_HUMIDITY: {\n            \"default_source\": \"carra\",\n            \"sources\": {\"carra\": {\"specific_name\": \"2m_spec_hum_carra\", \"unit\": \"m/s\"}},\n        },\n        StandardVariable.GROUND_HEAT_FLUX: {\n            \"default_source\": \"rav\",\n            \"sources\": {\"rav\": {\"specific_name\": \"grdflx_rav\", \"unit\": \"W/m^2\"}},\n            \"sources\": {\n                \"sens\": {\n                    \"specific_name\": \"surf_dwn_sens_heat_flux_carra\",\n                    \"unit\": \"W/m^2\",\n                }\n            },\n            \"sources\": {\n                \"lat\": {\n                    \"specific_name\": \"surf_dwn_lat_heat_flux_carra\",\n                    \"unit\": \"W/m^2\",\n                }\n            },\n        },\n        StandardVariable.SNOW_SUBLIMATION: {\n            \"default_source\": \"carra\",\n            \"sources\": {\n                \"carra\": {\"specific_name\": \"snow_sublimation_carra\", \"unit\": \"mm\"}\n            },\n        },\n        StandardVariable.SOIL_MOISTURE: {\n            \"default_source\": \"carra\",\n            \"sources\": {\"carra\": {\"specific_name\": \"percolation_carra\", \"unit\": \"mm\"}},\n        },\n    }\n</code></pre>"},{"location":"api/lamah_ice/#hydrodataset.lamah_ice.LamahIce.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/lamah_ice/#hydrodataset.lamah_ice.LamahIce.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize LamaHICE dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the LamaHICE data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/lamah_ice.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize LamaHICE dataset.\n\n    Args:\n        data_path: Path to the LamaHICE data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n\n    # Use the custom LamaHIce class defined at module level\n    self.aqua_fetch = LamaHIce(data_path)\n</code></pre>"},{"location":"api/simbi/","title":"SIMBI","text":""},{"location":"api/simbi/#overview","title":"Overview","text":"<p>SIMBI is a hydrological dataset for Multiple Regions. Multi-region dataset providing hydrological data from diverse global catchments.</p>"},{"location":"api/simbi/#dataset-information","title":"Dataset Information","text":"<ul> <li>Region: Multiple Regions</li> <li>Module: <code>hydrodataset.simbi</code></li> <li>Class: <code>simbi</code></li> </ul>"},{"location":"api/simbi/#features","title":"Features","text":""},{"location":"api/simbi/#static-attributes","title":"Static Attributes","text":"<p>Static catchment attributes include: - Basin area - Mean precipitation - Topographic characteristics - Land cover information - Soil properties - Climate indices</p>"},{"location":"api/simbi/#dynamic-variables","title":"Dynamic Variables","text":"<p>Timeseries variables available: - Streamflow - Precipitation - Temperature (min, max, mean) - Potential evapotranspiration - And additional variables depending on the dataset</p>"},{"location":"api/simbi/#usage","title":"Usage","text":""},{"location":"api/simbi/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset.simbi import simbi\nfrom hydrodataset import SETTING\n\n# Initialize dataset\ndata_path = SETTING[\"local_data_path\"][\"datasets-origin\"]\nds = simbi(data_path)\n\n# Get basin IDs\nbasin_ids = ds.read_object_ids()\nprint(f\"Number of basins: {len(basin_ids)}\")\n\n# Check available features\nprint(\"Static features:\", ds.available_static_features)\nprint(\"Dynamic features:\", ds.available_dynamic_features)\n\n# Check default time range\nprint(f\"Default time range: {ds.default_t_range}\")\n\n# Read timeseries data\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=ds.default_t_range,\n    var_lst=[\"streamflow\", \"precipitation\"]\n)\nprint(timeseries)\n\n# Read attribute data\nattributes = ds.read_attr_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    var_lst=[\"area\", \"p_mean\"]\n)\nprint(attributes)\n</code></pre>"},{"location":"api/simbi/#reading-specific-variables","title":"Reading Specific Variables","text":"<pre><code># Read with specific time range\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:10],\n    t_range=[\"1990-01-01\", \"2020-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n\n# Read basin area\nareas = ds.read_area(gage_id_lst=basin_ids[:10])\n\n# Read mean precipitation\nmean_precip = ds.read_mean_prcp(gage_id_lst=basin_ids[:10])\n</code></pre>"},{"location":"api/simbi/#working-with-multiple-data-sources","title":"Working with Multiple Data Sources","text":"<p>If the dataset provides multiple sources for variables:</p> <pre><code># Request specific data source\nts_data = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids[:5],\n    t_range=[\"1990-01-01\", \"2000-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/simbi/#api-reference","title":"API Reference","text":""},{"location":"api/simbi/#hydrodataset.simbi.simbi","title":"<code>hydrodataset.simbi.simbi</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>simbi dataset class extending RainfallRunoff.</p> <p>This class provides access to the simbi dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/simbi.py</code> <pre><code>class simbi(HydroDataset):\n    \"\"\"simbi dataset class extending RainfallRunoff.\n\n    This class provides access to the simbi dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(\n        self, data_path: str, region: Optional[str] = None, download: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize simbi dataset.\n\n        Args:\n            data_path: Path to the simbi data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n        \"\"\"\n        super().__init__(data_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = Simbi(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"simbi_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"simbi_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1920-01-01\", \"2005-12-31\"]\n\n    # get the information of features from dataset file \"SIMBI_README\"\n    _subclass_static_definitions = {\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mon_avg\", \"unit\": \"mm/month\"},\n    }\n\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"}\n            },\n        },\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"observations\",\n            \"sources\": {\"observations\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.TEMPERATURE_MEAN: {\n            \"default_source\": \"observations\",\n            \"sources\": {\n                \"observations\": {\"specific_name\": \"airtemp_c_mean\", \"unit\": \"\u00b0C\"}\n            },\n        },\n    }\n</code></pre>"},{"location":"api/simbi/#hydrodataset.simbi.simbi.default_t_range","title":"<code>default_t_range</code>  <code>property</code>","text":""},{"location":"api/simbi/#hydrodataset.simbi.simbi.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize simbi dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the simbi data directory</p> required <code>region</code> <code>Optional[str]</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> Source code in <code>hydrodataset/simbi.py</code> <pre><code>def __init__(\n    self, data_path: str, region: Optional[str] = None, download: bool = False\n) -&gt; None:\n    \"\"\"Initialize simbi dataset.\n\n    Args:\n        data_path: Path to the simbi data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n    \"\"\"\n    super().__init__(data_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = Simbi(data_path)\n</code></pre>"},{"location":"api/standard_variables/","title":"Standard Variables","text":""},{"location":"api/standard_variables/#overview","title":"Overview","text":"<p>The <code>StandardVariable</code> class defines standardized variable names used across all datasets in hydrodataset. This ensures consistency when accessing data from different hydrological datasets, even when the underlying dataset uses different naming conventions.</p>"},{"location":"api/standard_variables/#purpose","title":"Purpose","text":"<p>Different hydrological datasets use various naming conventions for the same variables: - CAMELS-US might use \"dayl\" for daylight duration - CAMELS-AUS might use \"solarrad_AWAP\" for solar radiation - CARAVAN might use \"temperature_2m_mean\" for mean temperature</p> <p>StandardVariable provides a unified naming system so users can request <code>StandardVariable.TEMPERATURE_MEAN</code> regardless of the underlying dataset.</p>"},{"location":"api/standard_variables/#variable-categories","title":"Variable Categories","text":""},{"location":"api/standard_variables/#streamflow","title":"Streamflow","text":"<ul> <li><code>STREAMFLOW</code>: River discharge/streamflow</li> </ul>"},{"location":"api/standard_variables/#precipitation","title":"Precipitation","text":"<ul> <li><code>PRECIPITATION</code>: Precipitation/rainfall</li> </ul>"},{"location":"api/standard_variables/#temperature","title":"Temperature","text":"<ul> <li><code>TEMPERATURE_MAX</code>: Maximum air temperature</li> <li><code>TEMPERATURE_MIN</code>: Minimum air temperature</li> <li><code>TEMPERATURE_MEAN</code>: Mean air temperature</li> </ul>"},{"location":"api/standard_variables/#evapotranspiration","title":"Evapotranspiration","text":"<ul> <li><code>POTENTIAL_EVAPOTRANSPIRATION</code>: Potential evapotranspiration (PET)</li> <li><code>EVAPOTRANSPIRATION</code>: Actual evapotranspiration (ET/AET)</li> <li><code>EVAPORATION</code>: Evaporation from water surfaces</li> </ul>"},{"location":"api/standard_variables/#radiation","title":"Radiation","text":"<ul> <li><code>SOLAR_RADIATION</code>: Solar/shortwave radiation</li> <li><code>SOLAR_RADIATION_MIN</code>: Minimum solar radiation</li> <li><code>SOLAR_RADIATION_MAX</code>: Maximum solar radiation</li> <li><code>THERMAL_RADIATION</code>: Thermal/longwave radiation</li> <li><code>THERMAL_RADIATION_MIN</code>: Minimum thermal radiation</li> <li><code>THERMAL_RADIATION_MAX</code>: Maximum thermal radiation</li> </ul>"},{"location":"api/standard_variables/#snow","title":"Snow","text":"<ul> <li><code>SNOW_WATER_EQUIVALENT</code>: Snow water equivalent</li> <li><code>SNOW_WATER_EQUIVALENT_MIN</code>: Minimum snow water equivalent</li> <li><code>SNOW_WATER_EQUIVALENT_MAX</code>: Maximum snow water equivalent</li> <li><code>SNOW_DEPTH</code>: Snow depth</li> </ul>"},{"location":"api/standard_variables/#wind","title":"Wind","text":"<ul> <li><code>WIND_SPEED</code>: Wind speed</li> <li><code>U_WIND_SPEED</code>: U-component of wind speed</li> <li><code>U_WIND_SPEED_MIN</code>: Minimum U-component wind speed</li> <li><code>U_WIND_SPEED_MAX</code>: Maximum U-component wind speed</li> <li><code>V_WIND_SPEED</code>: V-component of wind speed</li> <li><code>V_WIND_SPEED_MIN</code>: Minimum V-component wind speed</li> <li><code>V_WIND_SPEED_MAX</code>: Maximum V-component wind speed</li> </ul>"},{"location":"api/standard_variables/#atmospheric","title":"Atmospheric","text":"<ul> <li><code>VAPOR_PRESSURE</code>: Vapor pressure</li> <li><code>SPECIFIC_HUMIDITY</code>: Specific humidity</li> <li><code>RELATIVE_HUMIDITY</code>: Relative humidity</li> <li><code>SURFACE_PRESSURE</code>: Surface atmospheric pressure</li> <li><code>SURFACE_PRESSURE_MIN</code>: Minimum surface pressure</li> <li><code>SURFACE_PRESSURE_MAX</code>: Maximum surface pressure</li> </ul>"},{"location":"api/standard_variables/#soil","title":"Soil","text":"<ul> <li><code>SOIL_MOISTURE</code>: Soil moisture (general)</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER1</code>: Volumetric soil water content, layer 1</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER1_MIN</code>: Minimum soil water, layer 1</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER1_MAX</code>: Maximum soil water, layer 1</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER2</code>: Volumetric soil water content, layer 2</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER2_MIN</code>: Minimum soil water, layer 2</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER2_MAX</code>: Maximum soil water, layer 2</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER3</code>: Volumetric soil water content, layer 3</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER3_MIN</code>: Minimum soil water, layer 3</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER3_MAX</code>: Maximum soil water, layer 3</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER4</code>: Volumetric soil water content, layer 4</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER4_MIN</code>: Minimum soil water, layer 4</li> <li><code>VOLUMETRIC_SOIL_WATER_LAYER4_MAX</code>: Maximum soil water, layer 4</li> </ul>"},{"location":"api/standard_variables/#daylight","title":"Daylight","text":"<ul> <li><code>DAYLIGHT_DURATION</code>: Hours of daylight</li> </ul>"},{"location":"api/standard_variables/#usage","title":"Usage","text":""},{"location":"api/standard_variables/#basic-usage","title":"Basic Usage","text":"<pre><code>from hydrodataset import StandardVariable\nfrom hydrodataset.camels_us import CamelsUs\n\nds = CamelsUs(data_path)\n\n# Request data using standard variable names\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids,\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        StandardVariable.STREAMFLOW,\n        StandardVariable.PRECIPITATION,\n        StandardVariable.TEMPERATURE_MEAN\n    ]\n)\n</code></pre>"},{"location":"api/standard_variables/#using-string-names","title":"Using String Names","text":"<p>You can also use lowercase string versions of the standard variable names:</p> <pre><code># These are equivalent\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids,\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\"streamflow\", \"precipitation\", \"temperature_mean\"]\n)\n</code></pre>"},{"location":"api/standard_variables/#multiple-data-sources","title":"Multiple Data Sources","text":"<p>Some datasets provide the same variable from multiple sources. You can specify the source:</p> <pre><code># Request precipitation from ERA5-Land source instead of default\ntimeseries = ds.read_ts_xrdataset(\n    gage_id_lst=basin_ids,\n    t_range=[\"1990-01-01\", \"1995-12-31\"],\n    var_lst=[\n        (\"precipitation\", \"era5land\"),  # Specify source\n        \"streamflow\"  # Use default source\n    ]\n)\n</code></pre>"},{"location":"api/standard_variables/#checking-available-variables","title":"Checking Available Variables","text":"<pre><code># List all available standard variables for a dataset\nprint(ds.available_dynamic_features)\nprint(ds.available_static_features)\n</code></pre>"},{"location":"api/standard_variables/#variable-mapping-implementation","title":"Variable Mapping Implementation","text":"<p>When implementing a new dataset, map StandardVariable constants to dataset-specific names:</p> <pre><code>_dynamic_variable_mapping = {\n    StandardVariable.STREAMFLOW: {\n        \"default_source\": \"observations\",\n        \"sources\": {\n            \"observations\": {\"specific_name\": \"q_cms\", \"unit\": \"m^3/s\"},\n            \"simulated\": {\"specific_name\": \"q_sim\", \"unit\": \"m^3/s\"},\n        },\n    },\n    StandardVariable.PRECIPITATION: {\n        \"default_source\": \"gauge\",\n        \"sources\": {\n            \"gauge\": {\"specific_name\": \"precip_mm\", \"unit\": \"mm/day\"},\n            \"era5\": {\"specific_name\": \"tp_era5\", \"unit\": \"mm/day\"},\n            \"chirps\": {\"specific_name\": \"precip_chirps\", \"unit\": \"mm/day\"},\n        },\n    },\n}\n</code></pre>"},{"location":"api/standard_variables/#api-reference","title":"API Reference","text":""},{"location":"api/standard_variables/#hydrodataset.hydro_dataset.StandardVariable","title":"<code>hydrodataset.hydro_dataset.StandardVariable</code>","text":"<p>A class to hold standardized variable names as constants.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>class StandardVariable:\n    \"\"\"A class to hold standardized variable names as constants.\"\"\"\n\n    STREAMFLOW = \"streamflow\"\n    WATER_LEVEL = \"water_level\"\n\n    PRECIPITATION = \"precipitation\"\n    CRAINF_FRAC = \"crainf_frac\"  # Fraction of total precipitation that is convective\n    PRECIPITATION_MIN = \"precipitation_min\"\n    PRECIPITATION_MAX = \"precipitation_max\"\n    PRECIPITATION_MEDIAN = \"precipitation_median\"\n\n    TEMPERATURE_MAX = \"temperature_max\"\n    TEMPERATURE_MIN = \"temperature_min\"\n    TEMPERATURE_MEAN = \"temperature_mean\"\n\n    DAYLIGHT_DURATION = \"daylight_duration\"\n    RELATIVE_DAYLIGHT_DURATION = \"relative_daylight_duration\"\n\n    SOLAR_RADIATION = \"solar_radiation\"\n    SOLAR_RADIATION_MIN = \"solar_radiation_min\"\n    SOLAR_RADIATION_MAX = \"solar_radiation_max\"\n    SOLAR_RADIATION_MEDIAN = \"solar_radiation_median\"\n    THERMAL_RADIATION = \"thermal_radiation\"\n    THERMAL_RADIATION_MIN = \"thermal_radiation_min\"\n    THERMAL_RADIATION_MAX = \"thermal_radiation_max\"\n    LONGWAVE_SOLAR_RADIATION = \"longwave_solar_radiation\"\n\n    SNOW_WATER_EQUIVALENT = \"snow_water_equivalent\"\n    SNOW_WATER_EQUIVALENT_MIN = \"snow_water_equivalent_min\"\n    SNOW_WATER_EQUIVALENT_MAX = \"snow_water_equivalent_max\"\n    SNOW_DEPTH = \"snow_depth\"\n    SNOW_COVER = \"snow_cover\"\n    SNOW_SUBLIMATION = \"snow_sublimation\"\n    SNOW_DENSITY = \"snow_density\"\n\n    VAPOR_PRESSURE = \"vapor_pressure\"\n\n    SURFACE_PRESSURE = \"surface_pressure\"\n    SURFACE_PRESSURE_MIN = \"surface_pressure_min\"\n    SURFACE_PRESSURE_MAX = \"surface_pressure_max\"\n\n    WIND_SPEED = \"wind_speed\"\n    U_WIND_SPEED = \"u_wind_speed\"\n    U_WIND_SPEED_MIN = \"u_wind_speed_min\"\n    U_WIND_SPEED_MAX = \"u_wind_speed_max\"\n    V_WIND_SPEED = \"v_wind_speed\"\n    V_WIND_SPEED_MIN = \"v_wind_speed_min\"\n    V_WIND_SPEED_MAX = \"v_wind_speed_max\"\n    WIND_DIR = \"wind_dir\"\n    LOW_LEVEL_WIND_SHEAR = \"low_level_wind_shear\"\n    DEEP_LEVEL_WIND_SHEAR = \"deep_level_wind_shear\"\n\n    RELATIVE_HUMIDITY = \"relative_humidity\"\n    SPECIFIC_HUMIDITY = \"specific_humidity\"\n    RELATIVE_HUMIDITY_MIN = \"relative_humidity_min\"\n    RELATIVE_HUMIDITY_MAX = \"relative_humidity_max\"\n    RELATIVE_HUMIDITY_MEDIAN = \"relative_humidity_median\"\n    TOTAL_COLUMN_WATER_VAPOUR = \"total_column_water_vapour\"\n\n    CAPE = \"cape\"  # Convective available potential energy\n    CIN = \"cin\"  # Convective inhibition\n\n    POTENTIAL_EVAPOTRANSPIRATION = \"potential_evapotranspiration\"\n    EVAPORATION = \"evaporation\"\n    EVAPOTRANSPIRATION = \"evapotranspiration\"\n\n    SOIL_MOISTURE = \"soil_moisture\"\n    VOLUMETRIC_SOIL_WATER_LAYER1 = \"volumetric_soil_water_layer1\"  # 0-7cm\n    VOLUMETRIC_SOIL_WATER_LAYER1_MIN = \"volumetric_soil_water_layer1_min\"\n    VOLUMETRIC_SOIL_WATER_LAYER1_MAX = \"volumetric_soil_water_layer1_max\"\n    VOLUMETRIC_SOIL_WATER_LAYER2 = \"volumetric_soil_water_layer2\"  # 7-28cm\n    VOLUMETRIC_SOIL_WATER_LAYER2_MIN = \"volumetric_soil_water_layer2_min\"\n    VOLUMETRIC_SOIL_WATER_LAYER2_MAX = \"volumetric_soil_water_layer2_max\"\n    VOLUMETRIC_SOIL_WATER_LAYER3 = \"volumetric_soil_water_layer3\"  # 28-100cm\n    VOLUMETRIC_SOIL_WATER_LAYER3_MIN = \"volumetric_soil_water_layer3_min\"\n    VOLUMETRIC_SOIL_WATER_LAYER3_MAX = \"volumetric_soil_water_layer3_max\"\n    VOLUMETRIC_SOIL_WATER_LAYER4 = \"volumetric_soil_water_layer4\"  # 100-289cm\n    VOLUMETRIC_SOIL_WATER_LAYER4_MIN = \"volumetric_soil_water_layer4_min\"\n    VOLUMETRIC_SOIL_WATER_LAYER4_MAX = \"volumetric_soil_water_layer4_max\"\n\n    MIN_RAIN_RATE = \"min_rain_rate\"\n    MAX_RAIN_RATE = \"max_rain_rate\"\n\n    GROUND_HEAT_FLUX = \"ground_heat_flux\"\n</code></pre>"}]}