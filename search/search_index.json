{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to hydrodataset","text":"<p>A Python package for downloading and reading hydrological datasets</p> <ul> <li>Free software: MIT license</li> <li>Documentation: https://OuyangWenyu.github.io/hydrodataset</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>TODO</li> </ul>"},{"location":"#credits","title":"Credits","text":"<p>This package was created with Cookiecutter and the giswqs/pypackage project template.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/OuyangWenyu/hydrodataset/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>HydroDataset could always use more documentation, whether as part of the official HydroDataset docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/OuyangWenyu/hydrodataset/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up hydrodataset for local development.</p> <ol> <li> <p>Fork the hydrodataset repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/hydrodataset.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv hydrodataset\n$ cd hydrodataset/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 hydrodataset tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy. Check https://github.com/OuyangWenyu/hydrodataset/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"hydrodataset/","title":"hydrodataset module","text":""},{"location":"hydrodataset/#hydrodataset-module","title":"hydrodataset module","text":"<p>Author: Wenyu Ouyang Date: 2022-09-05 23:20:24 LastEditTime: 2025-10-30 11:13:29 LastEditors: Wenyu Ouyang Description: set file dir FilePath: \\hydrodataset\\hydrodataset__init__.py Copyright (c) 2021-2022 Wenyu Ouyang. All rights reserved.</p>"},{"location":"hydrodataset/#hydrodataset.Camels","title":"<code>Camels</code>","text":"<p>               Bases: <code>HydroDataset</code></p> Source code in <code>hydrodataset/camels.py</code> <pre><code>class Camels(HydroDataset):\n    def __init__(\n        self,\n        data_path,\n        download=False,\n        region: str = \"US\",\n    ):\n        print(\"Initializing Camels class...\")\n        \"\"\"\n        Initialization for CAMELS series dataset\n\n        Parameters\n        ----------\n        data_path\n            where we put the dataset.\n            we already set the ROOT directory for hydrodataset,\n            so here just set it as a relative path,\n            by default \"camels/camels_us\"\n        download\n            if true, download, by defaulf False\n        region\n            the default is CAMELS(-US), since it's the first CAMELS dataset.\n            All are included in CAMELS_REGIONS\n        \"\"\"\n        self.data_path = os.path.join(data_path, \"CAMELS_US\")\n        super().__init__(self.data_path)\n        if region not in CAMELS_REGIONS:\n            raise NotImplementedError(\n                f\"Please chose one region in: {str(CAMELS_REGIONS)}\"\n            )\n        self.region = region\n        self.data_source_description = self.set_data_source_describe()\n        check_download = False\n        for url in self.data_source_description[\"CAMELS_DOWNLOAD_URL_LST\"]:\n            fpath = os.path.join(self.data_source_dir, url.rsplit(\"/\", 1)[1])\n            if not os.path.exists(fpath):\n                check_download = True\n                break\n        if check_download:\n            if download:\n                self.download_data_source()\n        check_zip_extract = False\n        # Check if zip files have been extracted\n        for url in self.data_source_description[\"CAMELS_DOWNLOAD_URL_LST\"]:\n            filename = url.rsplit(\"/\", 1)[1]\n            # Only check for zip files\n            if filename.endswith(\".zip\"):\n                # The extracted directory name (without .zip extension)\n                extracted_dir = self.data_source_dir / filename[:-4]\n                if not extracted_dir.exists():\n                    check_zip_extract = True\n                    break\n        if check_zip_extract:\n            hydro_file.zip_extract(self.data_source_description[\"CAMELS_DIR\"])\n        self.sites = self.read_site_info()\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camelsus_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camelsus_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1980-01-01\", \"2014-12-31\"]\n\n    def _get_attribute_units(self):\n        return {\n            \"gauge_lat\": \"degree\",\n            \"gauge_lon\": \"degree\",\n            \"elev_mean\": \"m\",\n            \"slope_mean\": \"m/km\",\n            \"area_gages2\": \"km^2\",\n            \"area_geospa_fabric\": \"km^2\",\n            \"geol_1st_class\": \"dimensionless\",\n            \"glim_1st_class_frac\": \"dimensionless\",\n            \"geol_2nd_class\": \"dimensionless\",\n            \"glim_2nd_class_frac\": \"dimensionless\",\n            \"carbonate_rocks_frac\": \"dimensionless\",\n            \"geol_porostiy\": \"dimensionless\",\n            \"geol_permeability\": \"m^2\",\n            \"frac_forest\": \"dimensionless\",\n            \"lai_max\": \"dimensionless\",\n            \"lai_diff\": \"dimensionless\",\n            \"gvf_max\": \"dimensionless\",\n            \"gvf_diff\": \"dimensionless\",\n            \"dom_land_cover_frac\": \"dimensionless\",\n            \"dom_land_cover\": \"dimensionless\",\n            \"root_depth_50\": \"m\",\n            \"root_depth_99\": \"m\",\n            \"q_mean\": \"mm/day\",\n            \"runoff_ratio\": \"dimensionless\",\n            \"slope_fdc\": \"dimensionless\",\n            \"baseflow_index\": \"dimensionless\",\n            \"stream_elas\": \"dimensionless\",\n            \"q5\": \"mm/day\",\n            \"q95\": \"mm/day\",\n            \"high_q_freq\": \"day/year\",\n            \"high_q_dur\": \"day\",\n            \"low_q_freq\": \"day/year\",\n            \"low_q_dur\": \"day\",\n            \"zero_q_freq\": \"percent\",\n            \"hfd_mean\": \"dimensionless\",\n            \"soil_depth_pelletier\": \"m\",\n            \"soil_depth_statsgo\": \"m\",\n            \"soil_porosity\": \"dimensionless\",\n            \"soil_conductivity\": \"cm/hr\",\n            \"max_water_content\": \"m\",\n            \"sand_frac\": \"percent\",\n            \"silt_frac\": \"percent\",\n            \"clay_frac\": \"percent\",\n            \"water_frac\": \"percent\",\n            \"organic_frac\": \"percent\",\n            \"other_frac\": \"percent\",\n            \"p_mean\": \"mm/day\",\n            \"pet_mean\": \"mm/day\",\n            \"p_seasonality\": \"dimensionless\",\n            \"frac_snow\": \"dimensionless\",\n            \"aridity\": \"dimensionless\",\n            \"high_prec_freq\": \"days/year\",\n            \"high_prec_dur\": \"day\",\n            \"high_prec_timing\": \"dimensionless\",\n            \"low_prec_freq\": \"days/year\",\n            \"low_prec_dur\": \"day\",\n            \"low_prec_timing\": \"dimensionless\",\n            \"huc_02\": \"dimensionless\",\n            \"gauge_name\": \"dimensionless\",\n        }\n\n    def _get_timeseries_units(self):\n        return [\"s\", \"mm/day\", \"W/m^2\", \"mm\", \"\u00b0C\", \"\u00b0C\", \"Pa\", \"mm/day\"]\n\n    def get_name(self):\n        return \"CAMELS_\" + self.region\n\n    def _dynamic_features(self) -&gt; list:\n        return self.get_relevant_cols().tolist() + [\"streamflow\"]\n\n    def _static_features(self) -&gt; list:\n        return self.get_constant_cols().tolist()\n\n    def set_data_source_describe(self) -&gt; collections.OrderedDict:\n        \"\"\"\n        the files in the dataset and their location in file system\n\n        Returns\n        -------\n        collections.OrderedDict\n            the description for a CAMELS dataset\n        \"\"\"\n        camels_db = self.data_source_dir\n        return self._set_data_source_camelsus_describe(camels_db)\n\n    def _set_data_source_camelsus_describe(self, camels_db):\n        # shp file of basins\n        camels_shp_file = camels_db.joinpath(\n            \"basin_set_full_res\", \"HCDN_nhru_final_671.shp\"\n        )\n        # config of flow data\n        flow_dir = camels_db.joinpath(\n            \"basin_timeseries_v1p2_metForcing_obsFlow\",\n            \"basin_dataset_public_v1p2\",\n            \"usgs_streamflow\",\n        )\n        flow_after_2015_dir = camels_db.joinpath(\n            \"camels_streamflow\", \"camels_streamflow\"\n        )\n        # forcing\n        forcing_dir = camels_db.joinpath(\n            \"basin_timeseries_v1p2_metForcing_obsFlow\",\n            \"basin_dataset_public_v1p2\",\n            \"basin_mean_forcing\",\n        )\n        forcing_types = [\"daymet\", \"maurer\", \"nldas\"]\n        # attr\n        attr_dir = camels_db\n        gauge_id_file = attr_dir.joinpath(\"camels_name.txt\")\n        attr_key_lst = [\"topo\", \"clim\", \"hydro\", \"vege\", \"soil\", \"geol\"]\n        base_url = \"https://gdex.ucar.edu/dataset/camels\"\n        download_url_lst = [\n            f\"{base_url}/file/basin_set_full_res.zip\",\n            # f\"{base_url}/file/basin_timeseries_v1p2_metForcing_obsFlow.zip\",\n            f\"{base_url}/file/camels_attributes_v2.0.xlsx\",\n            f\"{base_url}/file/camels_clim.txt\",\n            f\"{base_url}/file/camels_geol.txt\",\n            f\"{base_url}/file/camels_hydro.txt\",\n            f\"{base_url}/file/camels_name.txt\",\n            f\"{base_url}/file/camels_soil.txt\",\n            f\"{base_url}/file/camels_topo.txt\",\n            f\"{base_url}/file/camels_vege.txt\",\n        ]\n\n        return collections.OrderedDict(\n            CAMELS_DIR=camels_db,\n            CAMELS_FLOW_DIR=flow_dir,\n            CAMELS_FLOW_AFTER2015_DIR=flow_after_2015_dir,\n            CAMELS_FORCING_DIR=forcing_dir,\n            CAMELS_FORCING_TYPE=forcing_types,\n            CAMELS_ATTR_DIR=attr_dir,\n            CAMELS_ATTR_KEY_LST=attr_key_lst,\n            CAMELS_GAUGE_FILE=gauge_id_file,\n            CAMELS_BASINS_SHP_FILE=camels_shp_file,\n            CAMELS_DOWNLOAD_URL_LST=download_url_lst,\n        )\n\n    def download_data_source(self) -&gt; None:\n        \"\"\"\n        Download the required zip files\n\n        Now we only support CAMELS-US's downloading.\n        For others, please download it manually,\n        and put all files of a dataset in one directory.\n        For example, all files of CAMELS_AUS should be put in \"camels_aus\"\n\n        Returns\n        -------\n        None\n        \"\"\"\n        camels_config = self.data_source_description\n        self.data_source_dir.mkdir(exist_ok=True)\n        links = camels_config[\"CAMELS_DOWNLOAD_URL_LST\"]\n        for url in links:\n            fzip = Path(self.data_source_dir, url.rsplit(\"/\", 1)[1])\n            if fzip.exists():\n                with urlopen(url) as response:\n                    if int(response.info()[\"Content-length\"]) != fzip.stat().st_size:\n                        fzip.unlink()\n        to_dl = [\n            url\n            for url in links\n            if not Path(self.data_source_dir, url.rsplit(\"/\", 1)[1]).exists()\n        ]\n        hydro_file.download_zip_files(to_dl, self.data_source_dir)\n\n    def read_site_info(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Read the basic information of gages in a CAMELS dataset\n\n        Returns\n        -------\n        pd.DataFrame\n            basic info of gages\n        \"\"\"\n        camels_file = self.data_source_description[\"CAMELS_GAUGE_FILE\"]\n        return pd.read_csv(camels_file, sep=\";\", dtype={\"gauge_id\": str, \"huc_02\": str})\n\n    def get_constant_cols(self) -&gt; np.ndarray:\n        \"\"\"\n        all readable attrs in CAMELS\n\n        Returns\n        -------\n        np.array\n            attribute types\n        \"\"\"\n        data_folder = self.data_source_description[\"CAMELS_ATTR_DIR\"]\n        return self._get_constant_cols_some(data_folder, \"camels_\", \".txt\", \";\")\n\n    def _get_constant_cols_some(self, data_folder, prefix, postfix, sep):\n        var_dict = {}\n        var_lst = []\n        key_lst = self.data_source_description[\"CAMELS_ATTR_KEY_LST\"]\n        for key in key_lst:\n            data_file = os.path.join(data_folder, prefix + key + postfix)\n            data_temp = pd.read_csv(data_file, sep=sep)\n            var_lst_temp = list(data_temp.columns[1:])\n            var_dict[key] = var_lst_temp\n            var_lst.extend(var_lst_temp)\n        return np.array(var_lst)\n\n    def get_relevant_cols(self) -&gt; np.ndarray:\n        \"\"\"\n        all readable forcing types\n\n        Returns\n        -------\n        np.array\n            forcing types\n        \"\"\"\n        # PET is from model_output file in CAMELS-US\n        return np.array([\"dayl\", \"prcp\", \"srad\", \"swe\", \"tmax\", \"tmin\", \"vp\", \"PET\"])\n\n    def get_target_cols(self) -&gt; np.ndarray:\n        \"\"\"\n        For CAMELS, the target vars are streamflows\n\n        Returns\n        -------\n        np.array\n            streamflow types\n        \"\"\"\n        return np.array([\"usgsFlow\", \"ET\"])\n\n    def read_object_ids(self, **kwargs) -&gt; np.ndarray:\n        \"\"\"\n        read station ids\n\n        Parameters\n        ----------\n        **kwargs\n            optional params if needed\n\n        Returns\n        -------\n        np.array\n            gage/station ids\n        \"\"\"\n        return np.sort(self.sites[\"gauge_id\"].values)\n\n    def read_usgs_gage(self, usgs_id, t_range):\n        \"\"\"\n        read streamflow data of a station for date before 2015-01-01 from CAMELS-US\n\n        Parameters\n        ----------\n        usgs_id\n            the station id\n        t_range\n            the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n\n        Returns\n        -------\n        np.array\n            streamflow data of one station for a given time range\n        \"\"\"\n        logging.debug(\"reading %s streamflow data before 2015\", usgs_id)\n        gage_id_df = self.sites\n        huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n        usgs_file = os.path.join(\n            self.data_source_description[\"CAMELS_FLOW_DIR\"],\n            huc,\n            usgs_id + \"_streamflow_qc.txt\",\n        )\n        data_temp = pd.read_csv(usgs_file, sep=r\"\\s+\", header=None)\n        obs = data_temp[4].values\n        obs[obs &lt; 0] = np.nan\n        t_lst = hydro_time.t_range_days(t_range)\n        nt = t_lst.shape[0]\n        return (\n            self._read_usgs_gage_for_some(nt, data_temp, t_lst, obs)\n            if len(obs) != nt\n            else obs\n        )\n\n    def _read_usgs_gage_for_some(self, nt, data_temp, t_lst, obs):\n        result = np.full([nt], np.nan)\n        df_date = data_temp[[1, 2, 3]]\n        df_date.columns = [\"year\", \"month\", \"day\"]\n        date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n        [C, ind1, ind2] = np.intersect1d(date, t_lst, return_indices=True)\n        result[ind2] = obs[ind1]\n        return result\n\n    def read_camels_streamflow(self, usgs_id, t_range):\n        \"\"\"\n        read streamflow data of a station for date after 2015 from CAMELS-US\n\n        The streamflow data is downloaded from USGS website by HyRivers tools\n\n        Parameters\n        ----------\n        usgs_id\n            the station id\n        t_range\n            the time range, for example, [\"2015-01-01\", \"2022-01-01\"]\n\n        Returns\n        -------\n        np.array\n            streamflow data of one station for a given time range\n        \"\"\"\n        logging.debug(\"reading %s streamflow data after 2015\", usgs_id)\n        gage_id_df = self.sites\n        huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n        usgs_file = os.path.join(\n            self.data_source_description[\"CAMELS_FLOW_AFTER2015_DIR\"],\n            huc,\n            usgs_id + \"_streamflow_qc.txt\",\n        )\n        data_temp = pd.read_csv(usgs_file, sep=\",\", header=None, skiprows=1)\n        obs = data_temp[4].values\n        obs[obs &lt; 0] = np.nan\n        t_lst = hydro_time.t_range_days(t_range)\n        nt = t_lst.shape[0]\n        return (\n            self._read_usgs_gage_for_some(nt, data_temp, t_lst, obs)\n            if len(obs) != nt\n            else obs\n        )\n\n    def read_target_cols(\n        self,\n        gage_id_lst: Union[list, np.array] = None,\n        t_range: list = None,\n        target_cols: Union[list, np.array] = None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        read target values; for CAMELS, they are streamflows\n\n        default target_cols is an one-value list\n        Notice: the unit of target outputs in different regions are not totally same\n\n        Parameters\n        ----------\n        gage_id_lst\n            station ids\n        t_range\n            the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n        target_cols\n            the default is None, but we neea at least one default target.\n            For CAMELS-US, it is [\"usgsFlow\"];\n        kwargs\n            some other params if needed\n\n        Returns\n        -------\n        np.array\n            streamflow data, 3-dim [station, time, streamflow]\n        \"\"\"\n        if target_cols is None:\n            return np.array([])\n        else:\n            nf = len(target_cols)\n        t_range_list = hydro_time.t_range_days(t_range)\n        nt = t_range_list.shape[0]\n        y = np.full([len(gage_id_lst), nt, nf], np.nan)\n        for k in tqdm(\n            range(len(gage_id_lst)), desc=\"Read streamflow data of CAMELS-US\"\n        ):\n            for j in range(len(target_cols)):\n                if target_cols[j] == \"ET\":\n                    data_et = self.read_camels_us_model_output_data(\n                        gage_id_lst[k : k + 1], t_range, [\"ET\"]\n                    )\n                    y[k, :, j : j + 1] = data_et\n                else:\n                    data_obs = self._read_augmented_camels_streamflow(\n                        gage_id_lst, t_range, t_range_list, k\n                    )\n                    y[k, :, j] = data_obs\n        return y\n\n    def _read_augmented_camels_streamflow(self, gage_id_lst, t_range, t_range_list, k):\n        dt150101 = hydro_time.t2str(\"2015-01-01\")\n        if t_range_list[-1] &gt; dt150101 and t_range_list[0] &lt; dt150101:\n            # latest streamflow data in CAMELS is 2014/12/31\n            data_obs_after_2015 = self.read_camels_streamflow(\n                gage_id_lst[k], [\"2015-01-01\", t_range[1]]\n            )\n            data_obs_before_2015 = self.read_usgs_gage(\n                gage_id_lst[k], [t_range[0], \"2015-01-01\"]\n            )\n            return np.concatenate((data_obs_before_2015, data_obs_after_2015))\n        elif t_range_list[-1] &lt;= dt150101:\n            return self.read_usgs_gage(gage_id_lst[k], t_range)\n        else:\n            return self.read_camels_streamflow(gage_id_lst[k], t_range)\n\n    def read_camels_us_model_output_data(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        forcing_type=\"daymet\",\n    ) -&gt; np.array:\n        \"\"\"\n        Read model output data of CAMELS-US, including SWE, PRCP, RAIM, TAIR, PET, ET, MOD_RUN, OBS_RUN\n        Date starts from 1980-10-01 to 2014-12-31\n\n        Parameters\n        ----------\n        gage_id_lst : list\n            the station id list\n        var_lst : list\n            the variable list\n        t_range : list\n            the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n        forcing_type : str, optional\n            by default \"daymet\"\n        \"\"\"\n        t_range_list = hydro_time.t_range_days(t_range)\n        model_out_put_var_lst = [\n            \"SWE\",\n            \"PRCP\",\n            \"RAIM\",\n            \"TAIR\",\n            \"PET\",\n            \"ET\",\n            \"MOD_RUN\",\n            \"OBS_RUN\",\n        ]\n        if not set(var_lst).issubset(set(model_out_put_var_lst)):\n            raise RuntimeError(\"not in this list\")\n        nt = t_range_list.shape[0]\n        chosen_camels_mods = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n        count = 0\n        for usgs_id in tqdm(gage_id_lst, desc=\"Read model output data of CAMELS-US\"):\n            gage_id_df = self.sites\n            huc02_ = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n            file_path_dir = os.path.join(\n                self.data_source_dir,\n                \"basin_timeseries_v1p2_modelOutput_\" + forcing_type,\n                \"model_output_\" + forcing_type,\n                \"model_output\",\n                \"flow_timeseries\",\n                forcing_type,\n                huc02_,\n            )\n            sac_random_seeds = [\n                \"05\",\n                \"11\",\n                \"27\",\n                \"33\",\n                \"48\",\n                \"59\",\n                \"66\",\n                \"72\",\n                \"80\",\n                \"94\",\n            ]\n            files = [\n                os.path.join(\n                    file_path_dir, usgs_id + \"_\" + random_seed + \"_model_output.txt\"\n                )\n                for random_seed in sac_random_seeds\n            ]\n            results = []\n            for file in files:\n                result = pd.read_csv(file, sep=\"\\s+\")\n                df_date = result[[\"YR\", \"MNTH\", \"DY\"]]\n                df_date.columns = [\"year\", \"month\", \"day\"]\n                date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n                [c, ind1, ind2] = np.intersect1d(\n                    date, t_range_list, return_indices=True\n                )\n                results.append(result[var_lst].values[ind1])\n            result_np = np.array(results)\n            chosen_camels_mods[count, ind2, :] = np.mean(result_np, axis=0)\n            count = count + 1\n        return chosen_camels_mods\n\n    def read_forcing_gage(self, usgs_id, var_lst, t_range_list, forcing_type=\"daymet\"):\n        # data_source = daymet or maurer or nldas\n        logging.debug(\"reading %s forcing data\", usgs_id)\n        gage_id_df = self.sites\n        huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n\n        data_folder = self.data_source_description[\"CAMELS_FORCING_DIR\"]\n        temp_s = \"cida\" if forcing_type == \"daymet\" else forcing_type\n        data_file = os.path.join(\n            data_folder,\n            forcing_type,\n            huc,\n            f\"{usgs_id}_lump_{temp_s}_forcing_leap.txt\",\n        )\n        data_temp = pd.read_csv(data_file, sep=r\"\\s+\", header=None, skiprows=4)\n        forcing_lst = [\n            \"Year\",\n            \"Mnth\",\n            \"Day\",\n            \"Hr\",\n            \"dayl\",\n            \"prcp\",\n            \"srad\",\n            \"swe\",\n            \"tmax\",\n            \"tmin\",\n            \"vp\",\n        ]\n        df_date = data_temp[[0, 1, 2]]\n        df_date.columns = [\"year\", \"month\", \"day\"]\n        date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n\n        nf = len(var_lst)\n        [c, ind1, ind2] = np.intersect1d(date, t_range_list, return_indices=True)\n        nt = c.shape[0]\n        out = np.full([nt, nf], np.nan)\n\n        for k in range(nf):\n            ind = forcing_lst.index(var_lst[k])\n            out[ind2, k] = data_temp[ind].values[ind1]\n        return out\n\n    def read_relevant_cols(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        forcing_type=\"daymet\",\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Read forcing data\n\n        Parameters\n        ----------\n        gage_id_lst\n            station ids\n        t_range\n            the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n        var_lst\n            forcing variable types\n        forcing_type\n            now only for CAMELS-US, there are three types: daymet, nldas, maurer\n        Returns\n        -------\n        np.array\n            forcing data\n        \"\"\"\n        t_range_list = hydro_time.t_range_days(t_range)\n        nt = t_range_list.shape[0]\n        x = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n        for k in tqdm(range(len(gage_id_lst)), desc=\"Read forcing data of CAMELS-US\"):\n            if \"PET\" in var_lst:\n                pet_idx = var_lst.index(\"PET\")\n                data_pet = self.read_camels_us_model_output_data(\n                    gage_id_lst[k : k + 1], t_range, [\"PET\"]\n                )\n                x[k, :, pet_idx : pet_idx + 1] = data_pet\n                no_pet_var_lst = [x for x in var_lst if x != \"PET\"]\n                data = self.read_forcing_gage(\n                    gage_id_lst[k],\n                    no_pet_var_lst,\n                    t_range_list,\n                    forcing_type=forcing_type,\n                )\n                var_indices = [var_lst.index(var) for var in no_pet_var_lst]\n                x[k : k + 1, :, var_indices] = data\n            else:\n                data = self.read_forcing_gage(\n                    gage_id_lst[k],\n                    var_lst,\n                    t_range_list,\n                    forcing_type=forcing_type,\n                )\n                x[k, :, :] = data\n        return x\n\n    def read_attr_all(self):\n        data_folder = self.data_source_description[\"CAMELS_ATTR_DIR\"]\n        key_lst = self.data_source_description[\"CAMELS_ATTR_KEY_LST\"]\n        f_dict = {}\n        var_dict = {}\n        var_lst = []\n        out_lst = []\n        gage_dict = self.sites\n        camels_str = \"camels_\"\n        sep_ = \";\"\n        for key in key_lst:\n            data_file = os.path.join(data_folder, camels_str + key + \".txt\")\n            if self.region == \"GB\":\n                data_file = os.path.join(\n                    data_folder, camels_str + key + \"_attributes.csv\"\n                )\n            elif self.region == \"CC\":\n                data_file = os.path.join(data_folder, key + \".csv\")\n            data_temp = pd.read_csv(data_file, sep=sep_)\n            var_lst_temp = list(data_temp.columns[1:])\n            var_dict[key] = var_lst_temp\n            var_lst.extend(var_lst_temp)\n            k = 0\n            gage_id_key = \"gauge_id\"\n            if self.region == \"CC\":\n                gage_id_key = \"gage_id\"\n            n_gage = len(gage_dict[gage_id_key].values)\n            out_temp = np.full([n_gage, len(var_lst_temp)], np.nan)\n            for field in var_lst_temp:\n                if is_string_dtype(data_temp[field]):\n                    value, ref = pd.factorize(data_temp[field], sort=True)\n                    out_temp[:, k] = value\n                    f_dict[field] = ref.tolist()\n                elif is_numeric_dtype(data_temp[field]):\n                    out_temp[:, k] = data_temp[field].values\n                k = k + 1\n            out_lst.append(out_temp)\n        out = np.concatenate(out_lst, 1)\n        return out, var_lst, var_dict, f_dict\n\n    def read_attr_all_yr(self):\n        var_lst = self.get_constant_cols().tolist()\n        gage_id_lst = self.read_object_ids()\n        # for factorized data, we need factorize all gages' data to keep the factorized number same all the time\n        n_gage = len(self.read_object_ids())\n        c = np.full([n_gage, len(var_lst)], np.nan, dtype=object)\n        for k in range(n_gage):\n            attr_file = os.path.join(\n                self.data_source_description[\"CAMELS_ATTR_DIR\"],\n                gage_id_lst[k],\n                \"attributes.json\",\n            )\n            attr_data = hydro_file.unserialize_json_ordered(attr_file)\n            for j in range(len(var_lst)):\n                c[k, j] = attr_data[var_lst[j]]\n        data_temp = pd.DataFrame(c, columns=var_lst)\n        out_temp = np.full([n_gage, len(var_lst)], np.nan)\n        f_dict = {}\n        k = 0\n        for field in var_lst:\n            if field in [\"high_prec_timing\", \"low_prec_timing\"]:\n                # string type\n                value, ref = pd.factorize(data_temp[field], sort=True)\n                out_temp[:, k] = value\n                f_dict[field] = ref.tolist()\n            else:\n                out_temp[:, k] = data_temp[field].values\n            k = k + 1\n        # keep same format with CAMELS_US\n        return out_temp, var_lst, None, f_dict\n\n    def read_constant_cols(\n        self, gage_id_lst=None, var_lst=None, is_return_dict=False, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Read Attributes data\n\n        Parameters\n        ----------\n        gage_id_lst\n            station ids\n        var_lst\n            attribute variable types\n        is_return_dict\n            if true, return var_dict and f_dict for CAMELS_US\n        Returns\n        -------\n        Union[tuple, np.array]\n            if attr var type is str, return factorized data.\n            When we need to know what a factorized value represents, we need return a tuple;\n            otherwise just return an array\n        \"\"\"\n        attr_all, var_lst_all, var_dict, f_dict = self.read_attr_all()\n        ind_var = [var_lst_all.index(var) for var in var_lst]\n        id_lst_all = self.read_object_ids()\n        # Notice the sequence of station ids ! Some id_lst_all are not sorted, so don't use np.intersect1d\n        ind_grid = [id_lst_all.tolist().index(tmp) for tmp in gage_id_lst]\n        temp = attr_all[ind_grid, :]\n        out = temp[:, ind_var]\n        return (out, var_dict, f_dict) if is_return_dict else out\n\n    def read_area(self, gage_id_lst) -&gt; np.ndarray:\n        return self.read_attr_xrdataset(gage_id_lst, [\"area_gages2\"])\n\n    def read_mean_prcp(self, gage_id_lst, unit=\"mm/d\") -&gt; xr.Dataset:\n        \"\"\"Read mean precipitation data\n\n        Parameters\n        ----------\n        gage_id_lst : list\n            station ids\n        unit : str, optional\n            the unit of mean_prcp, by default \"mm/d\"\n\n        Returns\n        -------\n        xr.Dataset\n            TODO: now only support CAMELS-US\n\n        Raises\n        ------\n        NotImplementedError\n            some regions are not supported\n        ValueError\n            unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\n        \"\"\"\n        data = self.read_attr_xrdataset(gage_id_lst, [\"p_mean\"])\n        if unit in [\"mm/d\", \"mm/day\"]:\n            converted_data = data\n        elif unit in [\"mm/h\", \"mm/hour\"]:\n            converted_data = data / 24\n        elif unit in [\"mm/3h\", \"mm/3hour\"]:\n            converted_data = data / 8\n        elif unit in [\"mm/8d\", \"mm/8day\"]:\n            converted_data = data * 8\n        else:\n            raise ValueError(\n                \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n            )\n        return converted_data\n\n    def cache_forcing_np_json(self):\n        \"\"\"\n        Save all daymet basin-forcing data in a numpy array file in the cache directory.\n\n        Because it takes much time to read data from txt files,\n        it is a good way to cache data as a numpy file to speed up the reading.\n        In addition, we need a document to explain the meaning of all dimensions.\n\n        TODO: now only support CAMELS-US\n        \"\"\"\n        cache_npy_file = CACHE_DIR.joinpath(\"camels_daymet_forcing.npy\")\n        json_file = CACHE_DIR.joinpath(\"camels_daymet_forcing.json\")\n        variables = self.get_relevant_cols()\n        basins = self.sites[\"gauge_id\"].values\n        daymet_t_range = [\"1980-01-01\", \"2015-01-01\"]\n        times = [\n            hydro_time.t2str(tmp)\n            for tmp in hydro_time.t_range_days(daymet_t_range).tolist()\n        ]\n        data_info = collections.OrderedDict(\n            {\n                \"dim\": [\"basin\", \"time\", \"variable\"],\n                \"basin\": basins.tolist(),\n                \"time\": times,\n                \"variable\": variables.tolist(),\n            }\n        )\n        with open(json_file, \"w\") as FP:\n            json.dump(data_info, FP, indent=4)\n        data = self.read_relevant_cols(\n            gage_id_lst=basins.tolist(),\n            t_range=daymet_t_range,\n            var_lst=variables.tolist(),\n        )\n        np.save(cache_npy_file, data)\n\n    def cache_streamflow_np_json(self):\n        \"\"\"\n        Save all basins' streamflow data in a numpy array file in the cache directory\n\n        TODO: now only support CAMELS-US\n        \"\"\"\n        cache_npy_file = CACHE_DIR.joinpath(\"camels_streamflow.npy\")\n        json_file = CACHE_DIR.joinpath(\"camels_streamflow.json\")\n        variables = self.get_target_cols()\n        basins = self.sites[\"gauge_id\"].values\n        t_range = [\"1980-01-01\", \"2015-01-01\"]\n        times = [\n            hydro_time.t2str(tmp) for tmp in hydro_time.t_range_days(t_range).tolist()\n        ]\n        data_info = collections.OrderedDict(\n            {\n                \"dim\": [\"basin\", \"time\", \"variable\"],\n                \"basin\": basins.tolist(),\n                \"time\": times,\n                \"variable\": variables.tolist(),\n            }\n        )\n        with open(json_file, \"w\") as FP:\n            json.dump(data_info, FP, indent=4)\n        data = self.read_target_cols(\n            gage_id_lst=basins,\n            t_range=t_range,\n            target_cols=variables,\n        )\n        np.save(cache_npy_file, data)\n\n    def cache_attributes_xrdataset(self):\n        \"\"\"Convert all the attributes to a single dataframe and save as a netcdf file\n        TODO: now only support CAMELS-US\n\n        Returns\n        -------\n        None\n        \"\"\"\n        # NOTICE: although it seems that we don't use pint_xarray, we have to import this package\n        import pint_xarray\n\n        attr_files = self.data_source_dir.glob(\"camels_*.txt\")\n        attrs = {\n            f.stem.split(\"_\")[1]: pd.read_csv(\n                f, sep=\";\", index_col=0, dtype={\"huc_02\": str, \"gauge_id\": str}\n            )\n            for f in attr_files\n        }\n\n        attrs_df = pd.concat(attrs.values(), axis=1)\n\n        # fix station names\n        def fix_station_nm(station_nm):\n            name = station_nm.title().rsplit(\" \", 1)\n            name[0] = name[0] if name[0][-1] == \",\" else f\"{name[0]},\"\n            name[1] = name[1].replace(\".\", \"\")\n            return \" \".join(\n                (name[0], name[1].upper() if len(name[1]) == 2 else name[1].title())\n            )\n\n        attrs_df[\"gauge_name\"] = [fix_station_nm(n) for n in attrs_df[\"gauge_name\"]]\n        obj_cols = attrs_df.columns[attrs_df.dtypes == \"object\"]\n        for c in obj_cols:\n            attrs_df[c] = attrs_df[c].str.strip().astype(str)\n\n        # transform categorical variables to numeric\n        categorical_mappings = {}\n        for column in attrs_df.columns:\n            if attrs_df[column].dtype == \"object\":\n                attrs_df[column] = attrs_df[column].astype(\"category\")\n                categorical_mappings[column] = dict(\n                    enumerate(attrs_df[column].cat.categories)\n                )\n                attrs_df[column] = attrs_df[column].cat.codes\n\n        # unify id to basin\n        attrs_df.index.name = \"basin\"\n        # We use xarray dataset to cache all data\n        ds_from_df = attrs_df.to_xarray()\n        units_dict = {\n            \"gauge_lat\": \"degree\",\n            \"gauge_lon\": \"degree\",\n            \"elev_mean\": \"m\",\n            \"slope_mean\": \"m/km\",\n            \"area_gages2\": \"km^2\",\n            \"area_geospa_fabric\": \"km^2\",\n            \"geol_1st_class\": \"dimensionless\",\n            \"glim_1st_class_frac\": \"dimensionless\",\n            \"geol_2nd_class\": \"dimensionless\",\n            \"glim_2nd_class_frac\": \"dimensionless\",\n            \"carbonate_rocks_frac\": \"dimensionless\",\n            \"geol_porostiy\": \"dimensionless\",\n            \"geol_permeability\": \"m^2\",\n            \"frac_forest\": \"dimensionless\",\n            \"lai_max\": \"dimensionless\",\n            \"lai_diff\": \"dimensionless\",\n            \"gvf_max\": \"dimensionless\",\n            \"gvf_diff\": \"dimensionless\",\n            \"dom_land_cover_frac\": \"dimensionless\",\n            \"dom_land_cover\": \"dimensionless\",\n            \"root_depth_50\": \"m\",\n            \"root_depth_99\": \"m\",\n            \"q_mean\": \"mm/day\",\n            \"runoff_ratio\": \"dimensionless\",\n            \"slope_fdc\": \"dimensionless\",\n            \"baseflow_index\": \"dimensionless\",\n            \"stream_elas\": \"dimensionless\",\n            \"q5\": \"mm/day\",\n            \"q95\": \"mm/day\",\n            \"high_q_freq\": \"day/year\",\n            \"high_q_dur\": \"day\",\n            \"low_q_freq\": \"day/year\",\n            \"low_q_dur\": \"day\",\n            \"zero_q_freq\": \"percent\",\n            \"hfd_mean\": \"dimensionless\",\n            \"soil_depth_pelletier\": \"m\",\n            \"soil_depth_statsgo\": \"m\",\n            \"soil_porosity\": \"dimensionless\",\n            \"soil_conductivity\": \"cm/hr\",\n            \"max_water_content\": \"m\",\n            \"sand_frac\": \"percent\",\n            \"silt_frac\": \"percent\",\n            \"clay_frac\": \"percent\",\n            \"water_frac\": \"percent\",\n            \"organic_frac\": \"percent\",\n            \"other_frac\": \"percent\",\n            \"p_mean\": \"mm/day\",\n            \"pet_mean\": \"mm/day\",\n            \"p_seasonality\": \"dimensionless\",\n            \"frac_snow\": \"dimensionless\",\n            \"aridity\": \"dimensionless\",\n            \"high_prec_freq\": \"days/year\",\n            \"high_prec_dur\": \"day\",\n            \"high_prec_timing\": \"dimensionless\",\n            \"low_prec_freq\": \"days/year\",\n            \"low_prec_dur\": \"day\",\n            \"low_prec_timing\": \"dimensionless\",\n            \"huc_02\": \"dimensionless\",\n            \"gauge_name\": \"dimensionless\",\n        }\n\n        # Assign units to the variables in the Dataset\n        for var_name in units_dict:\n            if var_name in ds_from_df.data_vars:\n                ds_from_df[var_name].attrs[\"units\"] = units_dict[var_name]\n\n        # Assign categorical mappings to the variables in the Dataset\n        for column in ds_from_df.data_vars:\n            if column in categorical_mappings:\n                mapping_str = categorical_mappings[column]\n                ds_from_df[column].attrs[\"category_mapping\"] = str(mapping_str)\n        ds_from_df.to_netcdf(self.cache_dir.joinpath(self._attributes_cache_filename))\n\n    def cache_timeseries_xrdataset(self):\n        warnings.warn(\"Check you units of all variables\")\n        ds_streamflow = self._cache_streamflow_xrdataset()\n        ds_forcing = self._cache_forcing_xrdataset()\n        ds = xr.merge([ds_streamflow, ds_forcing])\n        ds.to_netcdf(self.cache_dir.joinpath(self._timeseries_cache_filename))\n\n    def _cache_streamflow_xrdataset(self):\n        \"\"\"Save all basins' streamflow data in a netcdf file in the cache directory\n\n        TODO: ONLY SUPPORT CAMELS-US now\n        \"\"\"\n        cache_npy_file = self.cache_dir.joinpath(\"camels_streamflow.npy\")\n        json_file = self.cache_dir.joinpath(\"camels_streamflow.json\")\n        if (not os.path.isfile(cache_npy_file)) or (not os.path.isfile(json_file)):\n            self.cache_streamflow_np_json()\n        streamflow = np.load(cache_npy_file)\n        with open(json_file, \"r\") as fp:\n            streamflow_dict = json.load(fp, object_pairs_hook=collections.OrderedDict)\n        import pint_xarray\n\n        basins = streamflow_dict[\"basin\"]\n        times = pd.date_range(\n            streamflow_dict[\"time\"][0], periods=len(streamflow_dict[\"time\"])\n        )\n        return xr.Dataset(\n            {\n                \"streamflow\": (\n                    [\"basin\", \"time\"],\n                    streamflow[:, :, 0],\n                    {\"units\": self.streamflow_unit},\n                ),\n                \"ET\": (\n                    [\"basin\", \"time\"],\n                    streamflow[:, :, 1],\n                    {\"units\": \"mm/day\"},\n                ),\n            },\n            coords={\n                \"basin\": basins,\n                \"time\": times,\n            },\n        )\n\n    def _cache_forcing_xrdataset(self):\n        \"\"\"Save all daymet basin-forcing data in a netcdf file in the cache directory.\n\n        TODO: ONLY SUPPORT CAMELS-US now\n        \"\"\"\n        cache_npy_file = self.cache_dir.joinpath(\"camels_daymet_forcing.npy\")\n        json_file = self.cache_dir.joinpath(\"camels_daymet_forcing.json\")\n        if (not os.path.isfile(cache_npy_file)) or (not os.path.isfile(json_file)):\n            self.cache_forcing_np_json()\n        daymet_forcing = np.load(cache_npy_file)\n        with open(json_file, \"r\") as fp:\n            daymet_forcing_dict = json.load(\n                fp, object_pairs_hook=collections.OrderedDict\n            )\n        import pint_xarray\n\n        basins = daymet_forcing_dict[\"basin\"]\n        times = pd.date_range(\n            daymet_forcing_dict[\"time\"][0], periods=len(daymet_forcing_dict[\"time\"])\n        )\n        variables = daymet_forcing_dict[\"variable\"]\n        # All units' names are from Pint https://github.com/hgrecco/pint/blob/master/pint/default_en.txt\n        # final is PET's unit. PET comes from the model output of CAMELS-US\n        units = [\"s\", \"mm/day\", \"W/m^2\", \"mm\", \"\u00b0C\", \"\u00b0C\", \"Pa\", \"mm/day\"]\n        return xr.Dataset(\n            data_vars={\n                **{\n                    variables[i]: (\n                        [\"basin\", \"time\"],\n                        daymet_forcing[:, :, i],\n                        {\"units\": units[i]},\n                    )\n                    for i in range(len(variables))\n                }\n            },\n            coords={\n                \"basin\": basins,\n                \"time\": times,\n            },\n            attrs={\"forcing_type\": \"daymet\"},\n        )\n\n    @property\n    def streamflow_unit(self):\n        return \"foot^3/s\"\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.cache_attributes_xrdataset","title":"<code>cache_attributes_xrdataset()</code>","text":"<p>Convert all the attributes to a single dataframe and save as a netcdf file TODO: now only support CAMELS-US</p>"},{"location":"hydrodataset/#hydrodataset.Camels.cache_attributes_xrdataset--returns","title":"Returns","text":"<p>None</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def cache_attributes_xrdataset(self):\n    \"\"\"Convert all the attributes to a single dataframe and save as a netcdf file\n    TODO: now only support CAMELS-US\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # NOTICE: although it seems that we don't use pint_xarray, we have to import this package\n    import pint_xarray\n\n    attr_files = self.data_source_dir.glob(\"camels_*.txt\")\n    attrs = {\n        f.stem.split(\"_\")[1]: pd.read_csv(\n            f, sep=\";\", index_col=0, dtype={\"huc_02\": str, \"gauge_id\": str}\n        )\n        for f in attr_files\n    }\n\n    attrs_df = pd.concat(attrs.values(), axis=1)\n\n    # fix station names\n    def fix_station_nm(station_nm):\n        name = station_nm.title().rsplit(\" \", 1)\n        name[0] = name[0] if name[0][-1] == \",\" else f\"{name[0]},\"\n        name[1] = name[1].replace(\".\", \"\")\n        return \" \".join(\n            (name[0], name[1].upper() if len(name[1]) == 2 else name[1].title())\n        )\n\n    attrs_df[\"gauge_name\"] = [fix_station_nm(n) for n in attrs_df[\"gauge_name\"]]\n    obj_cols = attrs_df.columns[attrs_df.dtypes == \"object\"]\n    for c in obj_cols:\n        attrs_df[c] = attrs_df[c].str.strip().astype(str)\n\n    # transform categorical variables to numeric\n    categorical_mappings = {}\n    for column in attrs_df.columns:\n        if attrs_df[column].dtype == \"object\":\n            attrs_df[column] = attrs_df[column].astype(\"category\")\n            categorical_mappings[column] = dict(\n                enumerate(attrs_df[column].cat.categories)\n            )\n            attrs_df[column] = attrs_df[column].cat.codes\n\n    # unify id to basin\n    attrs_df.index.name = \"basin\"\n    # We use xarray dataset to cache all data\n    ds_from_df = attrs_df.to_xarray()\n    units_dict = {\n        \"gauge_lat\": \"degree\",\n        \"gauge_lon\": \"degree\",\n        \"elev_mean\": \"m\",\n        \"slope_mean\": \"m/km\",\n        \"area_gages2\": \"km^2\",\n        \"area_geospa_fabric\": \"km^2\",\n        \"geol_1st_class\": \"dimensionless\",\n        \"glim_1st_class_frac\": \"dimensionless\",\n        \"geol_2nd_class\": \"dimensionless\",\n        \"glim_2nd_class_frac\": \"dimensionless\",\n        \"carbonate_rocks_frac\": \"dimensionless\",\n        \"geol_porostiy\": \"dimensionless\",\n        \"geol_permeability\": \"m^2\",\n        \"frac_forest\": \"dimensionless\",\n        \"lai_max\": \"dimensionless\",\n        \"lai_diff\": \"dimensionless\",\n        \"gvf_max\": \"dimensionless\",\n        \"gvf_diff\": \"dimensionless\",\n        \"dom_land_cover_frac\": \"dimensionless\",\n        \"dom_land_cover\": \"dimensionless\",\n        \"root_depth_50\": \"m\",\n        \"root_depth_99\": \"m\",\n        \"q_mean\": \"mm/day\",\n        \"runoff_ratio\": \"dimensionless\",\n        \"slope_fdc\": \"dimensionless\",\n        \"baseflow_index\": \"dimensionless\",\n        \"stream_elas\": \"dimensionless\",\n        \"q5\": \"mm/day\",\n        \"q95\": \"mm/day\",\n        \"high_q_freq\": \"day/year\",\n        \"high_q_dur\": \"day\",\n        \"low_q_freq\": \"day/year\",\n        \"low_q_dur\": \"day\",\n        \"zero_q_freq\": \"percent\",\n        \"hfd_mean\": \"dimensionless\",\n        \"soil_depth_pelletier\": \"m\",\n        \"soil_depth_statsgo\": \"m\",\n        \"soil_porosity\": \"dimensionless\",\n        \"soil_conductivity\": \"cm/hr\",\n        \"max_water_content\": \"m\",\n        \"sand_frac\": \"percent\",\n        \"silt_frac\": \"percent\",\n        \"clay_frac\": \"percent\",\n        \"water_frac\": \"percent\",\n        \"organic_frac\": \"percent\",\n        \"other_frac\": \"percent\",\n        \"p_mean\": \"mm/day\",\n        \"pet_mean\": \"mm/day\",\n        \"p_seasonality\": \"dimensionless\",\n        \"frac_snow\": \"dimensionless\",\n        \"aridity\": \"dimensionless\",\n        \"high_prec_freq\": \"days/year\",\n        \"high_prec_dur\": \"day\",\n        \"high_prec_timing\": \"dimensionless\",\n        \"low_prec_freq\": \"days/year\",\n        \"low_prec_dur\": \"day\",\n        \"low_prec_timing\": \"dimensionless\",\n        \"huc_02\": \"dimensionless\",\n        \"gauge_name\": \"dimensionless\",\n    }\n\n    # Assign units to the variables in the Dataset\n    for var_name in units_dict:\n        if var_name in ds_from_df.data_vars:\n            ds_from_df[var_name].attrs[\"units\"] = units_dict[var_name]\n\n    # Assign categorical mappings to the variables in the Dataset\n    for column in ds_from_df.data_vars:\n        if column in categorical_mappings:\n            mapping_str = categorical_mappings[column]\n            ds_from_df[column].attrs[\"category_mapping\"] = str(mapping_str)\n    ds_from_df.to_netcdf(self.cache_dir.joinpath(self._attributes_cache_filename))\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.cache_forcing_np_json","title":"<code>cache_forcing_np_json()</code>","text":"<p>Save all daymet basin-forcing data in a numpy array file in the cache directory.</p> <p>Because it takes much time to read data from txt files, it is a good way to cache data as a numpy file to speed up the reading. In addition, we need a document to explain the meaning of all dimensions.</p> <p>TODO: now only support CAMELS-US</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def cache_forcing_np_json(self):\n    \"\"\"\n    Save all daymet basin-forcing data in a numpy array file in the cache directory.\n\n    Because it takes much time to read data from txt files,\n    it is a good way to cache data as a numpy file to speed up the reading.\n    In addition, we need a document to explain the meaning of all dimensions.\n\n    TODO: now only support CAMELS-US\n    \"\"\"\n    cache_npy_file = CACHE_DIR.joinpath(\"camels_daymet_forcing.npy\")\n    json_file = CACHE_DIR.joinpath(\"camels_daymet_forcing.json\")\n    variables = self.get_relevant_cols()\n    basins = self.sites[\"gauge_id\"].values\n    daymet_t_range = [\"1980-01-01\", \"2015-01-01\"]\n    times = [\n        hydro_time.t2str(tmp)\n        for tmp in hydro_time.t_range_days(daymet_t_range).tolist()\n    ]\n    data_info = collections.OrderedDict(\n        {\n            \"dim\": [\"basin\", \"time\", \"variable\"],\n            \"basin\": basins.tolist(),\n            \"time\": times,\n            \"variable\": variables.tolist(),\n        }\n    )\n    with open(json_file, \"w\") as FP:\n        json.dump(data_info, FP, indent=4)\n    data = self.read_relevant_cols(\n        gage_id_lst=basins.tolist(),\n        t_range=daymet_t_range,\n        var_lst=variables.tolist(),\n    )\n    np.save(cache_npy_file, data)\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.cache_streamflow_np_json","title":"<code>cache_streamflow_np_json()</code>","text":"<p>Save all basins' streamflow data in a numpy array file in the cache directory</p> <p>TODO: now only support CAMELS-US</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def cache_streamflow_np_json(self):\n    \"\"\"\n    Save all basins' streamflow data in a numpy array file in the cache directory\n\n    TODO: now only support CAMELS-US\n    \"\"\"\n    cache_npy_file = CACHE_DIR.joinpath(\"camels_streamflow.npy\")\n    json_file = CACHE_DIR.joinpath(\"camels_streamflow.json\")\n    variables = self.get_target_cols()\n    basins = self.sites[\"gauge_id\"].values\n    t_range = [\"1980-01-01\", \"2015-01-01\"]\n    times = [\n        hydro_time.t2str(tmp) for tmp in hydro_time.t_range_days(t_range).tolist()\n    ]\n    data_info = collections.OrderedDict(\n        {\n            \"dim\": [\"basin\", \"time\", \"variable\"],\n            \"basin\": basins.tolist(),\n            \"time\": times,\n            \"variable\": variables.tolist(),\n        }\n    )\n    with open(json_file, \"w\") as FP:\n        json.dump(data_info, FP, indent=4)\n    data = self.read_target_cols(\n        gage_id_lst=basins,\n        t_range=t_range,\n        target_cols=variables,\n    )\n    np.save(cache_npy_file, data)\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.download_data_source","title":"<code>download_data_source()</code>","text":"<p>Download the required zip files</p> <p>Now we only support CAMELS-US's downloading. For others, please download it manually, and put all files of a dataset in one directory. For example, all files of CAMELS_AUS should be put in \"camels_aus\"</p>"},{"location":"hydrodataset/#hydrodataset.Camels.download_data_source--returns","title":"Returns","text":"<p>None</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def download_data_source(self) -&gt; None:\n    \"\"\"\n    Download the required zip files\n\n    Now we only support CAMELS-US's downloading.\n    For others, please download it manually,\n    and put all files of a dataset in one directory.\n    For example, all files of CAMELS_AUS should be put in \"camels_aus\"\n\n    Returns\n    -------\n    None\n    \"\"\"\n    camels_config = self.data_source_description\n    self.data_source_dir.mkdir(exist_ok=True)\n    links = camels_config[\"CAMELS_DOWNLOAD_URL_LST\"]\n    for url in links:\n        fzip = Path(self.data_source_dir, url.rsplit(\"/\", 1)[1])\n        if fzip.exists():\n            with urlopen(url) as response:\n                if int(response.info()[\"Content-length\"]) != fzip.stat().st_size:\n                    fzip.unlink()\n    to_dl = [\n        url\n        for url in links\n        if not Path(self.data_source_dir, url.rsplit(\"/\", 1)[1]).exists()\n    ]\n    hydro_file.download_zip_files(to_dl, self.data_source_dir)\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.get_constant_cols","title":"<code>get_constant_cols()</code>","text":"<p>all readable attrs in CAMELS</p>"},{"location":"hydrodataset/#hydrodataset.Camels.get_constant_cols--returns","title":"Returns","text":"<p>np.array     attribute types</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def get_constant_cols(self) -&gt; np.ndarray:\n    \"\"\"\n    all readable attrs in CAMELS\n\n    Returns\n    -------\n    np.array\n        attribute types\n    \"\"\"\n    data_folder = self.data_source_description[\"CAMELS_ATTR_DIR\"]\n    return self._get_constant_cols_some(data_folder, \"camels_\", \".txt\", \";\")\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.get_relevant_cols","title":"<code>get_relevant_cols()</code>","text":"<p>all readable forcing types</p>"},{"location":"hydrodataset/#hydrodataset.Camels.get_relevant_cols--returns","title":"Returns","text":"<p>np.array     forcing types</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def get_relevant_cols(self) -&gt; np.ndarray:\n    \"\"\"\n    all readable forcing types\n\n    Returns\n    -------\n    np.array\n        forcing types\n    \"\"\"\n    # PET is from model_output file in CAMELS-US\n    return np.array([\"dayl\", \"prcp\", \"srad\", \"swe\", \"tmax\", \"tmin\", \"vp\", \"PET\"])\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.get_target_cols","title":"<code>get_target_cols()</code>","text":"<p>For CAMELS, the target vars are streamflows</p>"},{"location":"hydrodataset/#hydrodataset.Camels.get_target_cols--returns","title":"Returns","text":"<p>np.array     streamflow types</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def get_target_cols(self) -&gt; np.ndarray:\n    \"\"\"\n    For CAMELS, the target vars are streamflows\n\n    Returns\n    -------\n    np.array\n        streamflow types\n    \"\"\"\n    return np.array([\"usgsFlow\", \"ET\"])\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.read_camels_streamflow","title":"<code>read_camels_streamflow(usgs_id, t_range)</code>","text":"<p>read streamflow data of a station for date after 2015 from CAMELS-US</p> <p>The streamflow data is downloaded from USGS website by HyRivers tools</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_camels_streamflow--parameters","title":"Parameters","text":"<p>usgs_id     the station id t_range     the time range, for example, [\"2015-01-01\", \"2022-01-01\"]</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_camels_streamflow--returns","title":"Returns","text":"<p>np.array     streamflow data of one station for a given time range</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_camels_streamflow(self, usgs_id, t_range):\n    \"\"\"\n    read streamflow data of a station for date after 2015 from CAMELS-US\n\n    The streamflow data is downloaded from USGS website by HyRivers tools\n\n    Parameters\n    ----------\n    usgs_id\n        the station id\n    t_range\n        the time range, for example, [\"2015-01-01\", \"2022-01-01\"]\n\n    Returns\n    -------\n    np.array\n        streamflow data of one station for a given time range\n    \"\"\"\n    logging.debug(\"reading %s streamflow data after 2015\", usgs_id)\n    gage_id_df = self.sites\n    huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n    usgs_file = os.path.join(\n        self.data_source_description[\"CAMELS_FLOW_AFTER2015_DIR\"],\n        huc,\n        usgs_id + \"_streamflow_qc.txt\",\n    )\n    data_temp = pd.read_csv(usgs_file, sep=\",\", header=None, skiprows=1)\n    obs = data_temp[4].values\n    obs[obs &lt; 0] = np.nan\n    t_lst = hydro_time.t_range_days(t_range)\n    nt = t_lst.shape[0]\n    return (\n        self._read_usgs_gage_for_some(nt, data_temp, t_lst, obs)\n        if len(obs) != nt\n        else obs\n    )\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.read_camels_us_model_output_data","title":"<code>read_camels_us_model_output_data(gage_id_lst=None, t_range=None, var_lst=None, forcing_type='daymet')</code>","text":"<p>Read model output data of CAMELS-US, including SWE, PRCP, RAIM, TAIR, PET, ET, MOD_RUN, OBS_RUN Date starts from 1980-10-01 to 2014-12-31</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_camels_us_model_output_data--parameters","title":"Parameters","text":"<p>gage_id_lst : list     the station id list var_lst : list     the variable list t_range : list     the time range, for example, [\"1990-01-01\", \"2000-01-01\"] forcing_type : str, optional     by default \"daymet\"</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_camels_us_model_output_data(\n    self,\n    gage_id_lst: list = None,\n    t_range: list = None,\n    var_lst: list = None,\n    forcing_type=\"daymet\",\n) -&gt; np.array:\n    \"\"\"\n    Read model output data of CAMELS-US, including SWE, PRCP, RAIM, TAIR, PET, ET, MOD_RUN, OBS_RUN\n    Date starts from 1980-10-01 to 2014-12-31\n\n    Parameters\n    ----------\n    gage_id_lst : list\n        the station id list\n    var_lst : list\n        the variable list\n    t_range : list\n        the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n    forcing_type : str, optional\n        by default \"daymet\"\n    \"\"\"\n    t_range_list = hydro_time.t_range_days(t_range)\n    model_out_put_var_lst = [\n        \"SWE\",\n        \"PRCP\",\n        \"RAIM\",\n        \"TAIR\",\n        \"PET\",\n        \"ET\",\n        \"MOD_RUN\",\n        \"OBS_RUN\",\n    ]\n    if not set(var_lst).issubset(set(model_out_put_var_lst)):\n        raise RuntimeError(\"not in this list\")\n    nt = t_range_list.shape[0]\n    chosen_camels_mods = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n    count = 0\n    for usgs_id in tqdm(gage_id_lst, desc=\"Read model output data of CAMELS-US\"):\n        gage_id_df = self.sites\n        huc02_ = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n        file_path_dir = os.path.join(\n            self.data_source_dir,\n            \"basin_timeseries_v1p2_modelOutput_\" + forcing_type,\n            \"model_output_\" + forcing_type,\n            \"model_output\",\n            \"flow_timeseries\",\n            forcing_type,\n            huc02_,\n        )\n        sac_random_seeds = [\n            \"05\",\n            \"11\",\n            \"27\",\n            \"33\",\n            \"48\",\n            \"59\",\n            \"66\",\n            \"72\",\n            \"80\",\n            \"94\",\n        ]\n        files = [\n            os.path.join(\n                file_path_dir, usgs_id + \"_\" + random_seed + \"_model_output.txt\"\n            )\n            for random_seed in sac_random_seeds\n        ]\n        results = []\n        for file in files:\n            result = pd.read_csv(file, sep=\"\\s+\")\n            df_date = result[[\"YR\", \"MNTH\", \"DY\"]]\n            df_date.columns = [\"year\", \"month\", \"day\"]\n            date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n            [c, ind1, ind2] = np.intersect1d(\n                date, t_range_list, return_indices=True\n            )\n            results.append(result[var_lst].values[ind1])\n        result_np = np.array(results)\n        chosen_camels_mods[count, ind2, :] = np.mean(result_np, axis=0)\n        count = count + 1\n    return chosen_camels_mods\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.read_constant_cols","title":"<code>read_constant_cols(gage_id_lst=None, var_lst=None, is_return_dict=False, **kwargs)</code>","text":"<p>Read Attributes data</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_constant_cols--parameters","title":"Parameters","text":"<p>gage_id_lst     station ids var_lst     attribute variable types is_return_dict     if true, return var_dict and f_dict for CAMELS_US Returns</p> <p>Union[tuple, np.array]     if attr var type is str, return factorized data.     When we need to know what a factorized value represents, we need return a tuple;     otherwise just return an array</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_constant_cols(\n    self, gage_id_lst=None, var_lst=None, is_return_dict=False, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"\n    Read Attributes data\n\n    Parameters\n    ----------\n    gage_id_lst\n        station ids\n    var_lst\n        attribute variable types\n    is_return_dict\n        if true, return var_dict and f_dict for CAMELS_US\n    Returns\n    -------\n    Union[tuple, np.array]\n        if attr var type is str, return factorized data.\n        When we need to know what a factorized value represents, we need return a tuple;\n        otherwise just return an array\n    \"\"\"\n    attr_all, var_lst_all, var_dict, f_dict = self.read_attr_all()\n    ind_var = [var_lst_all.index(var) for var in var_lst]\n    id_lst_all = self.read_object_ids()\n    # Notice the sequence of station ids ! Some id_lst_all are not sorted, so don't use np.intersect1d\n    ind_grid = [id_lst_all.tolist().index(tmp) for tmp in gage_id_lst]\n    temp = attr_all[ind_grid, :]\n    out = temp[:, ind_var]\n    return (out, var_dict, f_dict) if is_return_dict else out\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.read_mean_prcp","title":"<code>read_mean_prcp(gage_id_lst, unit='mm/d')</code>","text":"<p>Read mean precipitation data</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_mean_prcp--parameters","title":"Parameters","text":"<p>gage_id_lst : list     station ids unit : str, optional     the unit of mean_prcp, by default \"mm/d\"</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_mean_prcp--returns","title":"Returns","text":"<p>xr.Dataset     TODO: now only support CAMELS-US</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_mean_prcp--raises","title":"Raises","text":"<p>NotImplementedError     some regions are not supported ValueError     unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_mean_prcp(self, gage_id_lst, unit=\"mm/d\") -&gt; xr.Dataset:\n    \"\"\"Read mean precipitation data\n\n    Parameters\n    ----------\n    gage_id_lst : list\n        station ids\n    unit : str, optional\n        the unit of mean_prcp, by default \"mm/d\"\n\n    Returns\n    -------\n    xr.Dataset\n        TODO: now only support CAMELS-US\n\n    Raises\n    ------\n    NotImplementedError\n        some regions are not supported\n    ValueError\n        unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\n    \"\"\"\n    data = self.read_attr_xrdataset(gage_id_lst, [\"p_mean\"])\n    if unit in [\"mm/d\", \"mm/day\"]:\n        converted_data = data\n    elif unit in [\"mm/h\", \"mm/hour\"]:\n        converted_data = data / 24\n    elif unit in [\"mm/3h\", \"mm/3hour\"]:\n        converted_data = data / 8\n    elif unit in [\"mm/8d\", \"mm/8day\"]:\n        converted_data = data * 8\n    else:\n        raise ValueError(\n            \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n        )\n    return converted_data\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.read_object_ids","title":"<code>read_object_ids(**kwargs)</code>","text":"<p>read station ids</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_object_ids--parameters","title":"Parameters","text":"<p>**kwargs     optional params if needed</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_object_ids--returns","title":"Returns","text":"<p>np.array     gage/station ids</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_object_ids(self, **kwargs) -&gt; np.ndarray:\n    \"\"\"\n    read station ids\n\n    Parameters\n    ----------\n    **kwargs\n        optional params if needed\n\n    Returns\n    -------\n    np.array\n        gage/station ids\n    \"\"\"\n    return np.sort(self.sites[\"gauge_id\"].values)\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.read_relevant_cols","title":"<code>read_relevant_cols(gage_id_lst=None, t_range=None, var_lst=None, forcing_type='daymet', **kwargs)</code>","text":"<p>Read forcing data</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_relevant_cols--parameters","title":"Parameters","text":"<p>gage_id_lst     station ids t_range     the time range, for example, [\"1990-01-01\", \"2000-01-01\"] var_lst     forcing variable types forcing_type     now only for CAMELS-US, there are three types: daymet, nldas, maurer Returns</p> <p>np.array     forcing data</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_relevant_cols(\n    self,\n    gage_id_lst: list = None,\n    t_range: list = None,\n    var_lst: list = None,\n    forcing_type=\"daymet\",\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Read forcing data\n\n    Parameters\n    ----------\n    gage_id_lst\n        station ids\n    t_range\n        the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n    var_lst\n        forcing variable types\n    forcing_type\n        now only for CAMELS-US, there are three types: daymet, nldas, maurer\n    Returns\n    -------\n    np.array\n        forcing data\n    \"\"\"\n    t_range_list = hydro_time.t_range_days(t_range)\n    nt = t_range_list.shape[0]\n    x = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n    for k in tqdm(range(len(gage_id_lst)), desc=\"Read forcing data of CAMELS-US\"):\n        if \"PET\" in var_lst:\n            pet_idx = var_lst.index(\"PET\")\n            data_pet = self.read_camels_us_model_output_data(\n                gage_id_lst[k : k + 1], t_range, [\"PET\"]\n            )\n            x[k, :, pet_idx : pet_idx + 1] = data_pet\n            no_pet_var_lst = [x for x in var_lst if x != \"PET\"]\n            data = self.read_forcing_gage(\n                gage_id_lst[k],\n                no_pet_var_lst,\n                t_range_list,\n                forcing_type=forcing_type,\n            )\n            var_indices = [var_lst.index(var) for var in no_pet_var_lst]\n            x[k : k + 1, :, var_indices] = data\n        else:\n            data = self.read_forcing_gage(\n                gage_id_lst[k],\n                var_lst,\n                t_range_list,\n                forcing_type=forcing_type,\n            )\n            x[k, :, :] = data\n    return x\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.read_site_info","title":"<code>read_site_info()</code>","text":"<p>Read the basic information of gages in a CAMELS dataset</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_site_info--returns","title":"Returns","text":"<p>pd.DataFrame     basic info of gages</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_site_info(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Read the basic information of gages in a CAMELS dataset\n\n    Returns\n    -------\n    pd.DataFrame\n        basic info of gages\n    \"\"\"\n    camels_file = self.data_source_description[\"CAMELS_GAUGE_FILE\"]\n    return pd.read_csv(camels_file, sep=\";\", dtype={\"gauge_id\": str, \"huc_02\": str})\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.read_target_cols","title":"<code>read_target_cols(gage_id_lst=None, t_range=None, target_cols=None, **kwargs)</code>","text":"<p>read target values; for CAMELS, they are streamflows</p> <p>default target_cols is an one-value list Notice: the unit of target outputs in different regions are not totally same</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_target_cols--parameters","title":"Parameters","text":"<p>gage_id_lst     station ids t_range     the time range, for example, [\"1990-01-01\", \"2000-01-01\"] target_cols     the default is None, but we neea at least one default target.     For CAMELS-US, it is [\"usgsFlow\"]; kwargs     some other params if needed</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_target_cols--returns","title":"Returns","text":"<p>np.array     streamflow data, 3-dim [station, time, streamflow]</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_target_cols(\n    self,\n    gage_id_lst: Union[list, np.array] = None,\n    t_range: list = None,\n    target_cols: Union[list, np.array] = None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    read target values; for CAMELS, they are streamflows\n\n    default target_cols is an one-value list\n    Notice: the unit of target outputs in different regions are not totally same\n\n    Parameters\n    ----------\n    gage_id_lst\n        station ids\n    t_range\n        the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n    target_cols\n        the default is None, but we neea at least one default target.\n        For CAMELS-US, it is [\"usgsFlow\"];\n    kwargs\n        some other params if needed\n\n    Returns\n    -------\n    np.array\n        streamflow data, 3-dim [station, time, streamflow]\n    \"\"\"\n    if target_cols is None:\n        return np.array([])\n    else:\n        nf = len(target_cols)\n    t_range_list = hydro_time.t_range_days(t_range)\n    nt = t_range_list.shape[0]\n    y = np.full([len(gage_id_lst), nt, nf], np.nan)\n    for k in tqdm(\n        range(len(gage_id_lst)), desc=\"Read streamflow data of CAMELS-US\"\n    ):\n        for j in range(len(target_cols)):\n            if target_cols[j] == \"ET\":\n                data_et = self.read_camels_us_model_output_data(\n                    gage_id_lst[k : k + 1], t_range, [\"ET\"]\n                )\n                y[k, :, j : j + 1] = data_et\n            else:\n                data_obs = self._read_augmented_camels_streamflow(\n                    gage_id_lst, t_range, t_range_list, k\n                )\n                y[k, :, j] = data_obs\n    return y\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.read_usgs_gage","title":"<code>read_usgs_gage(usgs_id, t_range)</code>","text":"<p>read streamflow data of a station for date before 2015-01-01 from CAMELS-US</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_usgs_gage--parameters","title":"Parameters","text":"<p>usgs_id     the station id t_range     the time range, for example, [\"1990-01-01\", \"2000-01-01\"]</p>"},{"location":"hydrodataset/#hydrodataset.Camels.read_usgs_gage--returns","title":"Returns","text":"<p>np.array     streamflow data of one station for a given time range</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def read_usgs_gage(self, usgs_id, t_range):\n    \"\"\"\n    read streamflow data of a station for date before 2015-01-01 from CAMELS-US\n\n    Parameters\n    ----------\n    usgs_id\n        the station id\n    t_range\n        the time range, for example, [\"1990-01-01\", \"2000-01-01\"]\n\n    Returns\n    -------\n    np.array\n        streamflow data of one station for a given time range\n    \"\"\"\n    logging.debug(\"reading %s streamflow data before 2015\", usgs_id)\n    gage_id_df = self.sites\n    huc = gage_id_df[gage_id_df[\"gauge_id\"] == usgs_id][\"huc_02\"].values[0]\n    usgs_file = os.path.join(\n        self.data_source_description[\"CAMELS_FLOW_DIR\"],\n        huc,\n        usgs_id + \"_streamflow_qc.txt\",\n    )\n    data_temp = pd.read_csv(usgs_file, sep=r\"\\s+\", header=None)\n    obs = data_temp[4].values\n    obs[obs &lt; 0] = np.nan\n    t_lst = hydro_time.t_range_days(t_range)\n    nt = t_lst.shape[0]\n    return (\n        self._read_usgs_gage_for_some(nt, data_temp, t_lst, obs)\n        if len(obs) != nt\n        else obs\n    )\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.Camels.set_data_source_describe","title":"<code>set_data_source_describe()</code>","text":"<p>the files in the dataset and their location in file system</p>"},{"location":"hydrodataset/#hydrodataset.Camels.set_data_source_describe--returns","title":"Returns","text":"<p>collections.OrderedDict     the description for a CAMELS dataset</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def set_data_source_describe(self) -&gt; collections.OrderedDict:\n    \"\"\"\n    the files in the dataset and their location in file system\n\n    Returns\n    -------\n    collections.OrderedDict\n        the description for a CAMELS dataset\n    \"\"\"\n    camels_db = self.data_source_dir\n    return self._set_data_source_camelsus_describe(camels_db)\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.CamelsUs","title":"<code>CamelsUs</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELS_US dataset class.</p> <p>This class is a wrapper around the CAMELS_US class from the <code>aqua_fetch</code> package. It standardizes the dataset into a NetCDF format for easy use with hydrological models. It also includes custom logic to read the PET variable from model output files.</p> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>class CamelsUs(HydroDataset):\n    \"\"\"CAMELS_US dataset class.\n\n    This class is a wrapper around the CAMELS_US class from the `aqua_fetch` package.\n    It standardizes the dataset into a NetCDF format for easy use with hydrological models.\n    It also includes custom logic to read the PET variable from model output files.\n    \"\"\"\n\n    def __init__(self, data_path, region=None, download=False):\n        \"\"\"Initialize CAMELS_US dataset.\n\n        Args:\n            data_path: Path to the CAMELS_US data directory. This is where the data will be stored.\n            region: Geographic region identifier (optional, defaults to US).\n            download: Whether to download data automatically (not used, handled by aqua_fetch).\n        \"\"\"\n        super().__init__(data_path)\n        self.region = \"US\" if region is None else region\n        self.aqua_fetch = CAMELS_US(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_us_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_us_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1980-01-01\", \"2014-12-31\"]\n\n    def _dynamic_features(self) -&gt; list:\n        \"\"\"\n        Overrides the base method to include 'PET' as a dynamic feature.\n        \"\"\"\n        # Get the default features from the parent class (from aquafetch)\n        features = super()._dynamic_features()\n        # Add the custom PET and ET variables\n        features.extend([\"PET\", \"ET\"])\n        return features\n\n    def read_camels_us_model_output_data(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        forcing_type=\"daymet\",\n    ) -&gt; np.array:\n        \"\"\"\n        Read model output data of CAMELS-US, including PET.\n        This is a legacy function migrated from the old camels.py.\n        \"\"\"\n        # Fetch HUC codes for the requested basins on-the-fly\n        try:\n            huc_ds = self.read_attr_xrdataset(\n                gage_id_lst=gage_id_lst, var_lst=[\"huc_02\"], to_numeric=False\n            )\n            huc_df = huc_ds.to_dataframe()\n        except Exception as e:\n            raise RuntimeError(\n                f\"Could not read HUC attributes to get model output data: {e}\"\n            )\n\n        t_range_list = pd.date_range(start=t_range[0], end=t_range[1], freq=\"D\").values\n        model_out_put_var_lst = [\n            \"SWE\",\n            \"PRCP\",\n            \"RAIM\",\n            \"TAIR\",\n            \"PET\",\n            \"ET\",\n            \"MOD_RUN\",\n            \"OBS_RUN\",\n        ]\n        if not set(var_lst).issubset(set(model_out_put_var_lst)):\n            raise RuntimeError(\n                f\"Requested variables not in model output list: {var_lst}\"\n            )\n\n        nt = len(t_range_list)\n        chosen_camels_mods = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n\n        for i, usgs_id in enumerate(\n            tqdm(gage_id_lst, desc=\"Read model output data (PET and ET) for CAMELS-US\")\n        ):\n            try:\n                huc02_ = huc_df.loc[usgs_id, \"huc_02\"]\n                # Convert to 2-digit string with leading zeros if needed\n                huc02_ = f\"{int(huc02_):02d}\"\n            except KeyError:\n                print(\n                    f\"Warning: No HUC attribute found for {usgs_id}, skipping PET and ET reading.\"\n                )\n                continue\n\n            # Construct path to model output files\n            file_path_dir = os.path.join(\n                self.data_source_dir,\n                \"CAMELS_US\",\n                \"basin_timeseries_v1p2_modelOutput_\" + forcing_type,\n                \"model_output_\" + forcing_type,\n                \"model_output\",\n                \"flow_timeseries\",\n                forcing_type,\n                huc02_,\n            )\n\n            if not os.path.isdir(file_path_dir):\n                # This warning is kept for cases where the directory might be missing for a valid HUC\n                # print(f\"Warning: Model output directory not found: {file_path_dir}\")\n                continue\n\n            sac_random_seeds = [\n                \"05\",\n                \"11\",\n                \"27\",\n                \"33\",\n                \"48\",\n                \"59\",\n                \"66\",\n                \"72\",\n                \"80\",\n                \"94\",\n            ]\n            files = [\n                os.path.join(file_path_dir, f\"{usgs_id}_{seed}_model_output.txt\")\n                for seed in sac_random_seeds\n            ]\n\n            results = []\n            for file in files:\n                if not os.path.exists(file):\n                    continue\n                try:\n                    result = pd.read_csv(file, sep=r\"\\s+\")\n                    df_date = result[[\"YR\", \"MNTH\", \"DY\"]]\n                    df_date.columns = [\"year\", \"month\", \"day\"]\n                    date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n\n                    c, ind1, ind2 = np.intersect1d(\n                        date, t_range_list, return_indices=True\n                    )\n                    if len(c) &gt; 0:\n                        temp_data = np.full([nt, len(var_lst)], np.nan)\n                        temp_data[ind2, :] = result[var_lst].values[ind1]\n                        results.append(temp_data)\n                except Exception as e:\n                    print(f\"Warning: Failed to read {file}: {e}\")\n\n            if results:\n                result_np = np.array(results)\n                # Calculate mean across different random seeds\n                with np.errstate(\n                    invalid=\"ignore\"\n                ):  # Ignore warnings from all-NaN slices\n                    chosen_camels_mods[i, :, :] = np.nanmean(result_np, axis=0)\n\n        return chosen_camels_mods\n\n    def cache_timeseries_xrdataset(self):\n        \"\"\"\n        Overrides the base method to create a complete cache file including PET.\n\n        This method first calls the parent implementation to create the base cache\n        from aquafetch data, then reads the custom PET data and merges it into the\n        same cache file.\n        \"\"\"\n        # First, create the base cache file using the parent method\n        print(\"Creating base time-series cache from aquafetch...\")\n        super().cache_timeseries_xrdataset()\n\n        # Now, read the PET and ET data for all basins for the default time range\n        print(\"Reading PET and ET data to add to the cache...\")\n        gage_id_lst = self.read_object_ids().tolist()\n        model_output_data = self.read_camels_us_model_output_data(\n            gage_id_lst=gage_id_lst, t_range=self.default_t_range, var_lst=[\"PET\", \"ET\"]\n        )\n\n        cache_file = self.cache_dir.joinpath(self._timeseries_cache_filename)\n\n        # Use a with statement to ensure the dataset is closed before writing\n        with xr.open_dataset(cache_file) as ds:\n            print(f\"Variables in base cache: {list(ds.data_vars.keys())}\")\n            # Create xarray.DataArrays for PET and ET\n            pet_da = xr.DataArray(\n                model_output_data[:, :, 0],  # PET data\n                coords={\"basin\": gage_id_lst, \"time\": ds.time},\n                dims=[\"basin\", \"time\"],\n                attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n                name=\"PET\",\n            )\n            et_da = xr.DataArray(\n                model_output_data[:, :, 1],  # ET data\n                coords={\"basin\": gage_id_lst, \"time\": ds.time},\n                dims=[\"basin\", \"time\"],\n                attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n                name=\"ET\",\n            )\n            # Merge PET and ET into the main dataset\n            # Load the dataset into memory to avoid issues with lazy loading\n            merged_ds = ds.load().merge(pet_da).merge(et_da)\n\n        # Now that the original file is closed, we can safely overwrite it\n        print(\"Saving final cache file with merged PET and ET data...\")\n        print(f\"Variables in merged dataset: {list(merged_ds.data_vars.keys())}\")\n        merged_ds.to_netcdf(cache_file, mode=\"w\")\n        print(f\"Successfully saved final cache to: {cache_file}\")\n\n    _subclass_static_definitions = {\n        \"huc_02\": {\"specific_name\": \"huc_02\", \"unit\": \"dimensionless\"},\n        \"gauge_lat\": {\"specific_name\": \"lat\", \"unit\": \"degree\"},\n        \"gauge_lon\": {\"specific_name\": \"long\", \"unit\": \"degree\"},\n        \"elev_mean\": {\"specific_name\": \"elev_mean\", \"unit\": \"m\"},\n        \"slope_mean\": {\"specific_name\": \"slope_mkm1\", \"unit\": \"m/km\"},\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"geol_1st_class\": {\"specific_name\": \"geol_1st_class\", \"unit\": \"dimensionless\"},\n        \"geol_2nd_class\": {\"specific_name\": \"geol_2nd_class\", \"unit\": \"dimensionless\"},\n        \"geol_porostiy\": {\"specific_name\": \"geol_porostiy\", \"unit\": \"dimensionless\"},\n        \"geol_permeability\": {\"specific_name\": \"geol_permeability\", \"unit\": \"m^2\"},\n        \"frac_forest\": {\"specific_name\": \"frac_forest\", \"unit\": \"dimensionless\"},\n        \"lai_max\": {\"specific_name\": \"lai_max\", \"unit\": \"dimensionless\"},\n        \"lai_diff\": {\"specific_name\": \"lai_diff\", \"unit\": \"dimensionless\"},\n        \"dom_land_cover_frac\": {\n            \"specific_name\": \"dom_land_cover_frac\",\n            \"unit\": \"dimensionless\",\n        },\n        \"dom_land_cover\": {\"specific_name\": \"dom_land_cover\", \"unit\": \"dimensionless\"},\n        \"root_depth_50\": {\"specific_name\": \"root_depth_50\", \"unit\": \"m\"},\n        \"root_depth_99\": {\"specific_name\": \"root_depth_99\", \"unit\": \"m\"},\n        \"soil_depth_statsgo\": {\"specific_name\": \"soil_depth_statsgo\", \"unit\": \"m\"},\n        \"soil_porosity\": {\"specific_name\": \"soil_porosity\", \"unit\": \"dimensionless\"},\n        \"soil_conductivity\": {\"specific_name\": \"soil_conductivity\", \"unit\": \"cm/hr\"},\n        \"max_water_content\": {\"specific_name\": \"max_water_content\", \"unit\": \"m\"},\n        \"pet_mean\": {\"specific_name\": \"pet_mean\", \"unit\": \"mm/day\"},\n    }\n    _dynamic_variable_mapping = {\n        StandardVariable.STREAMFLOW: {\n            \"default_source\": \"usgs\",\n            \"sources\": {\"usgs\": {\"specific_name\": \"q_cms_obs\", \"unit\": \"m^3/s\"}},\n        },\n        # TODO: For maurer and nldas, we have not checked the specific names and units.\n        StandardVariable.PRECIPITATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"pcp_mm\", \"unit\": \"mm/day\"},\n                \"maurer\": {\"specific_name\": \"prcp_maurer\", \"unit\": \"mm/day\"},\n                \"nldas\": {\"specific_name\": \"prcp_nldas\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MAX: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"airtemp_c_max\", \"unit\": \"\u00b0C\"},\n                \"maurer\": {\"specific_name\": \"tmax_maurer\", \"unit\": \"\u00b0C\"},\n                \"nldas\": {\"specific_name\": \"tmax_nldas\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.TEMPERATURE_MIN: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"airtemp_c_min\", \"unit\": \"\u00b0C\"},\n                \"maurer\": {\"specific_name\": \"tmin_maurer\", \"unit\": \"\u00b0C\"},\n                \"nldas\": {\"specific_name\": \"tmin_nldas\", \"unit\": \"\u00b0C\"},\n            },\n        },\n        StandardVariable.DAYLIGHT_DURATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"dayl\", \"unit\": \"s\"},\n                \"maurer\": {\"specific_name\": \"dayl_maurer\", \"unit\": \"s\"},\n                \"nldas\": {\"specific_name\": \"dayl_nldas\", \"unit\": \"s\"},\n            },\n        },\n        StandardVariable.SOLAR_RADIATION: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"solrad_wm2\", \"unit\": \"W/m^2\"},\n                \"maurer\": {\"specific_name\": \"srad_maurer\", \"unit\": \"W/m^2\"},\n                \"nldas\": {\"specific_name\": \"srad_nldas\", \"unit\": \"W/m^2\"},\n            },\n        },\n        StandardVariable.SNOW_WATER_EQUIVALENT: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"swe_mm\", \"unit\": \"mm/day\"},\n                \"maurer\": {\"specific_name\": \"swe_maurer\", \"unit\": \"mm/day\"},\n                \"nldas\": {\"specific_name\": \"swe_nldas\", \"unit\": \"mm/day\"},\n            },\n        },\n        StandardVariable.VAPOR_PRESSURE: {\n            \"default_source\": \"daymet\",\n            \"sources\": {\n                \"daymet\": {\"specific_name\": \"vp_hpa\", \"unit\": \"hPa\"},\n                \"maurer\": {\"specific_name\": \"vp_maurer\", \"unit\": \"hPa\"},\n                \"nldas\": {\"specific_name\": \"vp_nldas\", \"unit\": \"hPa\"},\n            },\n        },\n        StandardVariable.POTENTIAL_EVAPOTRANSPIRATION: {\n            \"default_source\": \"sac-sma\",\n            \"sources\": {\"sac-sma\": {\"specific_name\": \"PET\", \"unit\": \"mm/day\"}},\n        },\n        StandardVariable.EVAPOTRANSPIRATION: {\n            \"default_source\": \"sac-sma\",\n            \"sources\": {\"sac-sma\": {\"specific_name\": \"ET\", \"unit\": \"mm/day\"}},\n        },\n    }\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.CamelsUs.__init__","title":"<code>__init__(data_path, region=None, download=False)</code>","text":"<p>Initialize CAMELS_US dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <p>Path to the CAMELS_US data directory. This is where the data will be stored.</p> required <code>region</code> <p>Geographic region identifier (optional, defaults to US).</p> <code>None</code> <code>download</code> <p>Whether to download data automatically (not used, handled by aqua_fetch).</p> <code>False</code> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>def __init__(self, data_path, region=None, download=False):\n    \"\"\"Initialize CAMELS_US dataset.\n\n    Args:\n        data_path: Path to the CAMELS_US data directory. This is where the data will be stored.\n        region: Geographic region identifier (optional, defaults to US).\n        download: Whether to download data automatically (not used, handled by aqua_fetch).\n    \"\"\"\n    super().__init__(data_path)\n    self.region = \"US\" if region is None else region\n    self.aqua_fetch = CAMELS_US(data_path)\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.CamelsUs.cache_timeseries_xrdataset","title":"<code>cache_timeseries_xrdataset()</code>","text":"<p>Overrides the base method to create a complete cache file including PET.</p> <p>This method first calls the parent implementation to create the base cache from aquafetch data, then reads the custom PET data and merges it into the same cache file.</p> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>def cache_timeseries_xrdataset(self):\n    \"\"\"\n    Overrides the base method to create a complete cache file including PET.\n\n    This method first calls the parent implementation to create the base cache\n    from aquafetch data, then reads the custom PET data and merges it into the\n    same cache file.\n    \"\"\"\n    # First, create the base cache file using the parent method\n    print(\"Creating base time-series cache from aquafetch...\")\n    super().cache_timeseries_xrdataset()\n\n    # Now, read the PET and ET data for all basins for the default time range\n    print(\"Reading PET and ET data to add to the cache...\")\n    gage_id_lst = self.read_object_ids().tolist()\n    model_output_data = self.read_camels_us_model_output_data(\n        gage_id_lst=gage_id_lst, t_range=self.default_t_range, var_lst=[\"PET\", \"ET\"]\n    )\n\n    cache_file = self.cache_dir.joinpath(self._timeseries_cache_filename)\n\n    # Use a with statement to ensure the dataset is closed before writing\n    with xr.open_dataset(cache_file) as ds:\n        print(f\"Variables in base cache: {list(ds.data_vars.keys())}\")\n        # Create xarray.DataArrays for PET and ET\n        pet_da = xr.DataArray(\n            model_output_data[:, :, 0],  # PET data\n            coords={\"basin\": gage_id_lst, \"time\": ds.time},\n            dims=[\"basin\", \"time\"],\n            attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n            name=\"PET\",\n        )\n        et_da = xr.DataArray(\n            model_output_data[:, :, 1],  # ET data\n            coords={\"basin\": gage_id_lst, \"time\": ds.time},\n            dims=[\"basin\", \"time\"],\n            attrs={\"units\": \"mm/day\", \"source\": \"SAC-SMA Model Output\"},\n            name=\"ET\",\n        )\n        # Merge PET and ET into the main dataset\n        # Load the dataset into memory to avoid issues with lazy loading\n        merged_ds = ds.load().merge(pet_da).merge(et_da)\n\n    # Now that the original file is closed, we can safely overwrite it\n    print(\"Saving final cache file with merged PET and ET data...\")\n    print(f\"Variables in merged dataset: {list(merged_ds.data_vars.keys())}\")\n    merged_ds.to_netcdf(cache_file, mode=\"w\")\n    print(f\"Successfully saved final cache to: {cache_file}\")\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.CamelsUs.read_camels_us_model_output_data","title":"<code>read_camels_us_model_output_data(gage_id_lst=None, t_range=None, var_lst=None, forcing_type='daymet')</code>","text":"<p>Read model output data of CAMELS-US, including PET. This is a legacy function migrated from the old camels.py.</p> Source code in <code>hydrodataset/camels_us.py</code> <pre><code>def read_camels_us_model_output_data(\n    self,\n    gage_id_lst: list = None,\n    t_range: list = None,\n    var_lst: list = None,\n    forcing_type=\"daymet\",\n) -&gt; np.array:\n    \"\"\"\n    Read model output data of CAMELS-US, including PET.\n    This is a legacy function migrated from the old camels.py.\n    \"\"\"\n    # Fetch HUC codes for the requested basins on-the-fly\n    try:\n        huc_ds = self.read_attr_xrdataset(\n            gage_id_lst=gage_id_lst, var_lst=[\"huc_02\"], to_numeric=False\n        )\n        huc_df = huc_ds.to_dataframe()\n    except Exception as e:\n        raise RuntimeError(\n            f\"Could not read HUC attributes to get model output data: {e}\"\n        )\n\n    t_range_list = pd.date_range(start=t_range[0], end=t_range[1], freq=\"D\").values\n    model_out_put_var_lst = [\n        \"SWE\",\n        \"PRCP\",\n        \"RAIM\",\n        \"TAIR\",\n        \"PET\",\n        \"ET\",\n        \"MOD_RUN\",\n        \"OBS_RUN\",\n    ]\n    if not set(var_lst).issubset(set(model_out_put_var_lst)):\n        raise RuntimeError(\n            f\"Requested variables not in model output list: {var_lst}\"\n        )\n\n    nt = len(t_range_list)\n    chosen_camels_mods = np.full([len(gage_id_lst), nt, len(var_lst)], np.nan)\n\n    for i, usgs_id in enumerate(\n        tqdm(gage_id_lst, desc=\"Read model output data (PET and ET) for CAMELS-US\")\n    ):\n        try:\n            huc02_ = huc_df.loc[usgs_id, \"huc_02\"]\n            # Convert to 2-digit string with leading zeros if needed\n            huc02_ = f\"{int(huc02_):02d}\"\n        except KeyError:\n            print(\n                f\"Warning: No HUC attribute found for {usgs_id}, skipping PET and ET reading.\"\n            )\n            continue\n\n        # Construct path to model output files\n        file_path_dir = os.path.join(\n            self.data_source_dir,\n            \"CAMELS_US\",\n            \"basin_timeseries_v1p2_modelOutput_\" + forcing_type,\n            \"model_output_\" + forcing_type,\n            \"model_output\",\n            \"flow_timeseries\",\n            forcing_type,\n            huc02_,\n        )\n\n        if not os.path.isdir(file_path_dir):\n            # This warning is kept for cases where the directory might be missing for a valid HUC\n            # print(f\"Warning: Model output directory not found: {file_path_dir}\")\n            continue\n\n        sac_random_seeds = [\n            \"05\",\n            \"11\",\n            \"27\",\n            \"33\",\n            \"48\",\n            \"59\",\n            \"66\",\n            \"72\",\n            \"80\",\n            \"94\",\n        ]\n        files = [\n            os.path.join(file_path_dir, f\"{usgs_id}_{seed}_model_output.txt\")\n            for seed in sac_random_seeds\n        ]\n\n        results = []\n        for file in files:\n            if not os.path.exists(file):\n                continue\n            try:\n                result = pd.read_csv(file, sep=r\"\\s+\")\n                df_date = result[[\"YR\", \"MNTH\", \"DY\"]]\n                df_date.columns = [\"year\", \"month\", \"day\"]\n                date = pd.to_datetime(df_date).values.astype(\"datetime64[D]\")\n\n                c, ind1, ind2 = np.intersect1d(\n                    date, t_range_list, return_indices=True\n                )\n                if len(c) &gt; 0:\n                    temp_data = np.full([nt, len(var_lst)], np.nan)\n                    temp_data[ind2, :] = result[var_lst].values[ind1]\n                    results.append(temp_data)\n            except Exception as e:\n                print(f\"Warning: Failed to read {file}: {e}\")\n\n        if results:\n            result_np = np.array(results)\n            # Calculate mean across different random seeds\n            with np.errstate(\n                invalid=\"ignore\"\n            ):  # Ignore warnings from all-NaN slices\n                chosen_camels_mods[i, :, :] = np.nanmean(result_np, axis=0)\n\n    return chosen_camels_mods\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.CamelshKr","title":"<code>CamelshKr</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>CAMELSH_KR dataset class extending RainfallRunoff.</p> <p>This class provides access to the CAMELSH_KR dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/camelsh_kr.py</code> <pre><code>class CamelshKr(HydroDataset):\n    \"\"\"CAMELSH_KR dataset class extending RainfallRunoff.\n\n    This class provides access to the CAMELSH_KR dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(self, data_path, region=None, download=False, cache_path=None):\n        \"\"\"Initialize CAMELSH_KR dataset.\n\n        Args:\n            data_path: Path to the CAMELSH_KR data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n            cache_path: Path to the cache directory\n        \"\"\"\n        super().__init__(data_path, cache_path=cache_path)\n        self.region = region\n        self.download = download\n        # In aqua_fetch, CAMELS_SK is the alias of CAMELSH_KR\n        self.aqua_fetch = CAMELS_SK(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"camels_sk_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"camels_sk_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"2000-01-01\", \"2019-12-31\"]\n\n    def _get_attribute_units(self):\n        return {\n            # \u5730\u5f62\u7279\u5f81\n            \"dis_m3_\": \"m^3/s\",\n            \"run_mm_\": \"millimeter\",\n            \"inu_pc_\": \"percent\",\n            \"lka_pc_\": \"1e-1 * percent\",\n            \"lkv_mc_\": \"1e6 * m^3\",\n            \"rev_mc_\": \"1e6 * m^3\",\n            \"dor_pc_\": \"percent (x10)\",\n            \"ria_ha_\": \"hectares\",\n            \"riv_tc_\": \"1e3 * m^3\",\n            \"gwt_cm_\": \"centimeter\",\n            \"ele_mt_\": \"meter\",\n            \"slp_dg_\": \"1e-1 * degree\",\n            \"sgr_dk_\": \"decimeter/km\",\n            \"clz_cl_\": \"dimensionless\",\n            \"cls_cl_\": \"dimensionless\",\n            \"tmp_dc_\": \"degree_Celsius\",\n            \"pre_mm_\": \"millimeters\",\n            \"pet_mm_\": \"millimeters\",\n            \"aet_mm_\": \"millimeters\",\n            \"ari_ix_\": \"1e-2\",\n            \"cmi_ix_\": \"1e-2\",\n            \"snw_pc_\": \"percent\",\n            \"glc_cl_\": \"dimensionless\",\n            \"glc_pc_\": \"percent\",\n            \"pnv_cl_\": \"dimensionless\",\n            \"pnv_pc_\": \"percent\",\n            \"wet_cl_\": \"dimensionless\",\n            \"wet_pc_\": \"percent\",\n            \"for_pc_\": \"percent\",\n            \"crp_pc_\": \"percent\",\n            \"pst_pc_\": \"percent\",\n            \"ire_pc_\": \"percent\",\n            \"gla_pc_\": \"percent\",\n            \"prm_pc_\": \"percent\",\n            \"pac_pc_\": \"percent\",\n            \"tbi_cl_\": \"dimensionless\",\n            \"tec_cl_\": \"dimensionless\",\n            \"fmh_cl_\": \"dimensionless\",\n            \"fec_cl_\": \"dimensionless\",\n            \"cly_pc_\": \"percent\",\n            \"slt_pc_\": \"percent\",\n            \"snd_pc_\": \"percent\",\n            \"soc_th_\": \"tonne/hectare\",\n            \"swc_pc_\": \"percent\",\n            \"lit_cl_\": \"dimensionless\",\n            \"kar_pc_\": \"percent\",\n            \"ero_kh_\": \"kg/hectare/year\",\n            \"pop_ct_\": \"1e3\",\n            \"ppd_pk_\": \"1/km^2\",\n            \"urb_pc_\": \"percent\",\n            \"nli_ix_\": \"1e-2\",\n            \"rdd_mk_\": \"meter/km^2\",\n            \"hft_ix_\": \"1e-1\",\n            \"gad_id_\": \"dimensionless\",\n            \"gdp_ud_\": \"dimensionless\",\n            \"hdi_ix_\": \"1e-3\",\n        }\n\n    def _get_timeseries_units(self):\n        return [\n            \"unknown\",  # total_precipitation\n            \"unknown\",  # temperature_2m\n            \"unknown\",  # dewpoint_temperature_2m\n            \"unknown\",  # snow_cover\n            \"unknown\",  # snow_depth\n            \"unknown\",  # potential_evaporation\n            \"unknown\",  # u_component_of_wind_10m\n            \"unknown\",  # v_component_of_wind_10m\n            \"unknown\",  # surface_pressure\n            \"unknown\",  # surface_net_thermal_radiation\n            \"unknown\",  # surface_net_solar_radiation\n            \"unknown\",  # precip_obs\n            \"unknown\",  # air_temp_obs\n            \"unknown\",  # wind_dir_obs\n            \"unknown\",  # wind_sp_obs\n            \"unknown\",  # q_cms_obs\n            \"unknown\",  # water_level\n        ]\n\n    def read_mean_prcp(self, gage_id_lst, unit=\"mm/d\") -&gt; xr.Dataset:\n        data_output_ds_ = self.read_attr_xrdataset(\n            gage_id_lst,\n            [\"p_mean\"],\n        )\n        data_output_ds_ = data_output_ds_.rename(\n            {\"STAID\": \"basin\"}\n        )  # \u91cd\u547d\u540d STAID \u4e3a basin\n        return data_output_ds_\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.CamelshKr.__init__","title":"<code>__init__(data_path, region=None, download=False, cache_path=None)</code>","text":"<p>Initialize CAMELSH_KR dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <p>Path to the CAMELSH_KR data directory</p> required <code>region</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> <code>cache_path</code> <p>Path to the cache directory</p> <code>None</code> Source code in <code>hydrodataset/camelsh_kr.py</code> <pre><code>def __init__(self, data_path, region=None, download=False, cache_path=None):\n    \"\"\"Initialize CAMELSH_KR dataset.\n\n    Args:\n        data_path: Path to the CAMELSH_KR data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n        cache_path: Path to the cache directory\n    \"\"\"\n    super().__init__(data_path, cache_path=cache_path)\n    self.region = region\n    self.download = download\n    # In aqua_fetch, CAMELS_SK is the alias of CAMELSH_KR\n    self.aqua_fetch = CAMELS_SK(data_path)\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset","title":"<code>HydroDataset</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An interface for Hydrological Dataset</p> <p>For unit, we use Pint package's unit system -- unit registry</p>"},{"location":"hydrodataset/#hydrodataset.HydroDataset--parameters","title":"Parameters","text":"<p>ABC : type description</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>class HydroDataset(ABC):\n    \"\"\"An interface for Hydrological Dataset\n\n    For unit, we use Pint package's unit system -- unit registry\n\n    Parameters\n    ----------\n    ABC : _type_\n        _description_\n    \"\"\"\n\n    # A unified definition for static variables, including name mapping and units\n    _base_static_definitions = {\n        \"area\": {\"specific_name\": \"area_km2\", \"unit\": \"km^2\"},\n        \"p_mean\": {\"specific_name\": \"p_mean\", \"unit\": \"mm/day\"},\n    }\n    # variable name map for timeseries\n    _dynamic_variable_mapping = {}\n\n    def __init__(self, data_path, cache_path=None):\n        self.data_source_dir = Path(ROOT_DIR, data_path)\n        if not self.data_source_dir.is_dir():\n            self.data_source_dir.mkdir(parents=True)\n        if cache_path is None:\n            self.cache_dir = Path(CACHE_DIR)\n        else:\n            self.cache_dir = Path(cache_path)\n        if not self.cache_dir.is_dir():\n            self.cache_dir.mkdir(parents=True)\n\n        # Merge static variable definitions\n        self._static_variable_definitions = self._base_static_definitions.copy()\n        if hasattr(self.__class__, \"_subclass_static_definitions\"):\n            self._static_variable_definitions.update(self._subclass_static_definitions)\n\n    def get_name(self):\n        raise NotImplementedError\n\n    def set_data_source_describe(self):\n        raise NotImplementedError\n\n    def download_data_source(self):\n        raise NotImplementedError\n\n    def is_data_ready(self):\n        raise NotImplementedError\n\n    def read_object_ids(self) -&gt; np.ndarray:\n        \"\"\"Read watershed station ID list.\"\"\"\n        if hasattr(self, \"aqua_fetch\"):\n            stations_list = self.aqua_fetch.stations()\n            return np.sort(np.array(stations_list))\n        raise NotImplementedError\n\n    def read_target_cols(\n        self, gage_id_lst=None, t_range=None, target_cols=None, **kwargs\n    ) -&gt; np.ndarray:\n        raise NotImplementedError\n\n    def read_relevant_cols(\n        self, gage_id_lst=None, t_range=None, var_lst=None, forcing_type=None, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"3d data (site_num * time_length * var_num), time-series data\"\"\"\n        raise NotImplementedError\n\n    def read_constant_cols(\n        self, gage_id_lst=None, var_lst=None, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"2d data (site_num * var_num), non-time-series data\"\"\"\n        raise NotImplementedError\n\n    def read_other_cols(\n        self, object_ids=None, other_cols: dict = None, **kwargs\n    ) -&gt; dict:\n        \"\"\"some data which cannot be easily treated as constant vars or time-series with same length as relevant vars\n        CONVENTION: other_cols is a dict, where each item is also a dict with all params in it\n        \"\"\"\n        raise NotImplementedError\n\n    def get_constant_cols(self) -&gt; np.ndarray:\n        \"\"\"the constant cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def get_relevant_cols(self) -&gt; np.ndarray:\n        \"\"\"the relevant cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def get_target_cols(self) -&gt; np.ndarray:\n        \"\"\"the target cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def get_other_cols(self) -&gt; dict:\n        \"\"\"the other cols in this data_source\"\"\"\n        raise NotImplementedError\n\n    def _dynamic_features(self) -&gt; list:\n        \"\"\"the dynamic features in this data_source\"\"\"\n        if hasattr(self, \"aqua_fetch\"):\n            original_features = self.aqua_fetch.dynamic_features\n            return self._clean_feature_names(original_features)\n        raise NotImplementedError\n\n    @staticmethod\n    def _clean_feature_names(feature_names):\n        \"\"\"Clean feature names to be compatible with NetCDF format and our internal standard.\n\n        The cleaning process follows these steps:\n        1. Remove units in parentheses (along with any preceding whitespace)\n           e.g., 'Prcp(mm/day)' -&gt; 'Prcp' or 'Temp (\u00b0C)' -&gt; 'Temp'\n        2. Convert all characters to lowercase\n           e.g., 'Prcp' -&gt; 'prcp'\n        3. Remove any remaining invalid characters (only keep a-z, 0-9, and _)\n           This ensures NetCDF variable naming compliance\n\n        Args:\n            feature_names (list or pd.Index): Original feature names that may contain\n                units and special characters\n\n        Returns:\n            list: Cleaned feature names with only lowercase letters, numbers, and underscores\n\n        Examples:\n            &gt;&gt;&gt; _clean_feature_names(['Prcp(mm/day)_daymet', 'Temp (\u00b0C)'])\n            ['prcp_daymet', 'temp']\n        \"\"\"\n        if not isinstance(feature_names, pd.Index):\n            feature_names = pd.Index(feature_names)\n\n        # Remove units in parentheses, then convert to lowercase\n        cleaned_names = feature_names.str.replace(\n            r\"\\s*\\([^)]*\\)\", \"\", regex=True\n        ).str.lower()\n        # Replace any remaining invalid characters\n        cleaned_names = cleaned_names.str.replace(r\"\"\"[^a-z0-9_]\"\"\", \"\", regex=True)\n        return cleaned_names.tolist()\n\n    def _static_features(self) -&gt; list:\n        \"\"\"the static features in this data_source\"\"\"\n        if hasattr(self, \"aqua_fetch\"):\n            original_features = self.aqua_fetch.static_features\n            return self._clean_feature_names(original_features)\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def _attributes_cache_filename(self):\n        pass\n\n    @property\n    @abstractmethod\n    def _timeseries_cache_filename(self):\n        pass\n\n    @property\n    @abstractmethod\n    def default_t_range(self):\n        pass\n\n    def cache_timeseries_xrdataset(self):\n        if hasattr(self, \"aqua_fetch\"):\n            # Build a lookup map from specific name to unit\n            unit_lookup = {}\n            if hasattr(self, \"_dynamic_variable_mapping\"):\n                for (\n                    std_name,\n                    mapping_info,\n                ) in self._dynamic_variable_mapping.items():\n                    for source, source_info in mapping_info[\"sources\"].items():\n                        unit_lookup[source_info[\"specific_name\"]] = source_info[\"unit\"]\n\n            gage_id_lst = self.read_object_ids().tolist()\n            original_var_lst = self.aqua_fetch.dynamic_features\n            cleaned_var_lst = self._clean_feature_names(original_var_lst)\n            # Create a mapping from original variable names to cleaned names\n            # to ensure correct correspondence even if list order changes\n            var_name_mapping = dict(zip(original_var_lst, cleaned_var_lst))\n\n            batch_data = self.aqua_fetch.fetch_stations_features(\n                stations=gage_id_lst,\n                dynamic_features=original_var_lst,\n                static_features=None,\n                st=self.default_t_range[0],\n                en=self.default_t_range[1],\n                as_dataframe=False,\n            )\n\n            dynamic_data = (\n                batch_data[1] if isinstance(batch_data, tuple) else batch_data\n            )\n\n            new_data_vars = {}\n            time_coord = dynamic_data.coords[\"time\"]\n\n            # Process only the variables that exist in the data source\n            # Subclasses can add additional variables in their override methods\n            for original_var in tqdm(\n                original_var_lst,\n                desc=\"Processing variables\",\n                total=len(original_var_lst),\n            ):\n                cleaned_var = var_name_mapping[original_var]\n                var_data = []\n                for station in gage_id_lst:\n                    if station in dynamic_data.data_vars:\n                        station_data = dynamic_data[station].sel(\n                            dynamic_features=original_var\n                        )\n                        if \"dynamic_features\" in station_data.coords:\n                            station_data = station_data.drop(\"dynamic_features\")\n                        var_data.append(station_data)\n\n                if var_data:\n                    combined = xr.concat(var_data, dim=\"basin\")\n                    combined[\"basin\"] = gage_id_lst\n                    combined.attrs[\"units\"] = unit_lookup.get(cleaned_var, \"unknown\")\n                    new_data_vars[cleaned_var] = combined\n\n            new_ds = xr.Dataset(\n                data_vars=new_data_vars,\n                coords={\n                    \"basin\": gage_id_lst,\n                    \"time\": time_coord,\n                },\n            )\n\n            batch_filepath = self.cache_dir.joinpath(self._timeseries_cache_filename)\n            batch_filepath.parent.mkdir(parents=True, exist_ok=True)\n            new_ds.to_netcdf(batch_filepath)\n            print(f\"\u6210\u529f\u4fdd\u5b58\u5230: {batch_filepath}\")\n        else:\n            raise NotImplementedError\n\n    def _assign_units_to_dataset(self, ds, units_map):\n        def get_unit_by_prefix(var_name):\n            for prefix, unit in units_map.items():\n                if var_name.startswith(prefix):\n                    return unit\n            return None\n\n        def get_unit(var_name):\n            prefix_unit = get_unit_by_prefix(var_name)\n            if prefix_unit:\n                return prefix_unit\n            return \"undefined\"\n\n        for var in ds.data_vars:\n            unit = get_unit(var)\n            ds[var].attrs[\"units\"] = unit\n            if unit == \"class\":\n                ds[var].attrs[\"description\"] = \"Classification code\"\n        return ds\n\n        return ds\n\n    def _get_attribute_units(self) -&gt; dict:\n        \"\"\"Builds a unit dictionary from the static variable definitions.\"\"\"\n        return {\n            info[\"specific_name\"]: info[\"unit\"]\n            for std_name, info in self._static_variable_definitions.items()\n        }\n\n    def cache_attributes_xrdataset(self):\n        if hasattr(self, \"aqua_fetch\"):\n            df_attr = self.aqua_fetch.fetch_static_features()\n            print(df_attr.columns)\n            # Clean column names using the unified method\n            df_attr.columns = self._clean_feature_names(df_attr.columns)\n            # Remove duplicate columns if any (keep first occurrence)\n            if df_attr.columns.duplicated().any():\n                df_attr = df_attr.loc[:, ~df_attr.columns.duplicated()]\n            # Ensure index is string type for basin IDs\n            df_attr.index = df_attr.index.astype(str)\n            ds_attr = df_attr.to_xarray()\n            # Check if the coordinate is named 'basin', if not rename it\n            coord_names = list(ds_attr.dims.keys())\n            if len(coord_names) &gt; 0 and coord_names[0] != \"basin\":\n                ds_attr = ds_attr.rename({coord_names[0]: \"basin\"})\n            units_map = self._get_attribute_units()\n            ds_attr = self._assign_units_to_dataset(ds_attr, units_map)\n            ds_attr.to_netcdf(self.cache_dir.joinpath(self._attributes_cache_filename))\n        else:\n            raise NotImplementedError\n\n    def read_attr_xrdataset(\n        self,\n        gage_id_lst: list = None,\n        var_lst: list = None,\n        to_numeric: bool = True,\n        **kwargs,\n    ) -&gt; xr.Dataset:\n        \"\"\"Reads attribute data for a list of basins using standardized variable names.\n\n        Args:\n            gage_id_lst: A list of basin identifiers.\n            var_lst: A list of **standard** attribute names to retrieve.\n                If None, all available static features will be returned.\n            to_numeric: If True, converts all non-numeric variables to numeric codes\n                and stores the original labels in the variable's attributes.\n                Defaults to True.\n\n        Returns:\n            An xarray Dataset containing the attribute data for the requested basins,\n            with variables named using the standard names.\n        \"\"\"\n        if var_lst is None:\n            var_lst = self.get_available_static_features()\n\n        # 1. Translate standard names to dataset-specific names\n        target_vars_to_fetch = []\n        rename_map = {}\n        for std_name in var_lst:\n            if std_name not in self._static_variable_definitions:\n                raise ValueError(\n                    f\"'{std_name}' is not a recognized standard static variable.\"\n                )\n            actual_var_name = self._static_variable_definitions[std_name][\n                \"specific_name\"\n            ]\n            target_vars_to_fetch.append(actual_var_name)\n            rename_map[actual_var_name] = std_name\n\n        # 2. Read data from cache using actual variable names\n        attr_cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n        try:\n            attr_ds = xr.open_dataset(attr_cache_file)\n        except FileNotFoundError:\n            self.cache_attributes_xrdataset()\n            attr_ds = xr.open_dataset(attr_cache_file)\n\n        # 3. Select variables and basins\n        ds_subset = attr_ds[target_vars_to_fetch]\n        if gage_id_lst is not None:\n            gage_id_lst = [str(gid) for gid in gage_id_lst]\n            ds_selected = ds_subset.sel(basin=gage_id_lst)\n        else:\n            ds_selected = ds_subset\n\n        # 4. Rename to standard names\n        final_ds = ds_selected.rename(rename_map)\n\n        if not to_numeric:\n            return final_ds\n\n        # 5. If to_numeric is True, perform conversion\n        converted_ds = xr.Dataset(coords=final_ds.coords)\n        for var_name, da in final_ds.data_vars.items():\n            if np.issubdtype(da.dtype, np.number):\n                converted_ds[var_name] = da\n            else:\n                # Assumes string-like array that needs factorizing\n                numeric_vals, labels = pd.factorize(da.values, sort=True)\n                new_da = xr.DataArray(\n                    numeric_vals,\n                    coords=da.coords,\n                    dims=da.dims,\n                    name=da.name,\n                    attrs=da.attrs,  # Preserve original attributes\n                )\n                new_da.attrs[\"labels\"] = labels.tolist()\n                converted_ds[var_name] = new_da\n        return converted_ds\n\n    def read_ts_xrdataset(\n        self,\n        gage_id_lst: list = None,\n        t_range: list = None,\n        var_lst: list = None,\n        sources: dict = None,\n        **kwargs,\n    ):\n        if (\n            not hasattr(self, \"_dynamic_variable_mapping\")\n            or not self._dynamic_variable_mapping\n        ):\n            raise NotImplementedError(\n                \"This dataset does not support the standardized variable mapping.\"\n            )\n\n        if var_lst is None:\n            var_lst = list(self._dynamic_variable_mapping.keys())\n\n        if t_range is None:\n            t_range = self.default_t_range\n\n        target_vars_to_fetch = []\n        rename_map = {}\n\n        for std_name in var_lst:\n            if std_name not in self._dynamic_variable_mapping:\n                raise ValueError(\n                    f\"'{std_name}' is not a recognized standard variable for this dataset.\"\n                )\n\n            mapping_info = self._dynamic_variable_mapping[std_name]\n\n            # Determine which source(s) to use and if they were explicitly requested\n            is_explicit_source = sources and std_name in sources\n            sources_to_use = []\n            if is_explicit_source:\n                provided_sources = sources[std_name]\n                if isinstance(provided_sources, list):\n                    sources_to_use.extend(provided_sources)\n                else:\n                    sources_to_use.append(provided_sources)\n            else:\n                sources_to_use.append(mapping_info[\"default_source\"])\n\n            # A suffix is only needed if the user explicitly requested multiple sources\n            needs_suffix = is_explicit_source and len(sources_to_use) &gt; 1\n            for source in sources_to_use:\n                if source not in mapping_info[\"sources\"]:\n                    raise ValueError(\n                        f\"Source '{source}' is not available for variable '{std_name}'.\"\n                    )\n\n                actual_var_name = mapping_info[\"sources\"][source][\"specific_name\"]\n                target_vars_to_fetch.append(actual_var_name)\n                output_name = f\"{std_name}_{source}\" if needs_suffix else std_name\n                rename_map[actual_var_name] = output_name\n\n        # Read data from cache using actual variable names\n        ts_cache_file = self.cache_dir.joinpath(self._timeseries_cache_filename)\n\n        if not os.path.isfile(ts_cache_file):\n            self.cache_timeseries_xrdataset()\n\n        ts = xr.open_dataset(ts_cache_file)\n        missing_vars = [v for v in target_vars_to_fetch if v not in ts.data_vars]\n        if missing_vars:\n            # To provide a better error message, map back to standard names\n            reverse_rename_map = {v: k for k, v in rename_map.items()}\n            missing_std_vars = [reverse_rename_map.get(v, v) for v in missing_vars]\n            raise ValueError(\n                f\"The following variables are missing from the cache file: {missing_std_vars}\"\n            )\n\n        ds_subset = ts[target_vars_to_fetch]\n        ds_selected = ds_subset.sel(\n            basin=gage_id_lst, time=slice(t_range[0], t_range[1])\n        )\n        final_ds = ds_selected.rename(rename_map)\n        return final_ds\n\n    def get_available_dynamic_features(self) -&gt; dict:\n        \"\"\"\n        Returns a dictionary of available standard dynamic feature names\n        and their possible sources.\n        \"\"\"\n        if (\n            not hasattr(self, \"_dynamic_variable_mapping\")\n            or not self._dynamic_variable_mapping\n        ):\n            return {}\n\n        feature_info = {}\n        for std_name, mapping_info in self._dynamic_variable_mapping.items():\n            feature_info[std_name] = {\n                \"default_source\": mapping_info.get(\"default_source\"),\n                \"available_sources\": list(mapping_info.get(\"sources\", {}).keys()),\n            }\n        return feature_info\n\n    def get_available_static_features(self) -&gt; list:\n        \"\"\"Returns a list of available standard static feature names.\"\"\"\n        return list(self._static_variable_definitions.keys())\n\n    @property\n    def available_static_features(self) -&gt; list:\n        \"\"\"Returns a list of available static attribute names.\"\"\"\n        return self.get_available_static_features()\n\n    @property\n    def available_dynamic_features(self) -&gt; dict:\n        \"\"\"Returns a dictionary of available dynamic feature names and their possible sources.\"\"\"\n        return self.get_available_dynamic_features()\n\n    def read_area(self, gage_id_lst: list[str]) -&gt; xr.Dataset:\n        \"\"\"Reads the catchment area for a list of basins.\n\n        Args:\n            gage_id_lst: A list of basin identifiers for which to retrieve the area.\n\n        Returns:\n            An xarray Dataset containing the area data for the requested basins.\n        \"\"\"\n        data_ds = self.read_attr_xrdataset(gage_id_lst=gage_id_lst, var_lst=[\"area\"])\n        return data_ds\n\n    def read_mean_prcp(self, gage_id_lst: list[str], unit: str = \"mm/d\") -&gt; xr.Dataset:\n        \"\"\"Reads the mean daily precipitation for a list of basins, with unit conversion.\n\n        Args:\n            gage_id_lst: A list of basin identifiers.\n            unit: The desired unit for the output precipitation. Defaults to \"mm/d\".\n                Supported units: ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h',\n                'mm/3hour', 'mm/8d', 'mm/8day'].\n\n        Returns:\n            An xarray Dataset containing the mean precipitation data in the specified units.\n\n        Raises:\n            ValueError: If an unsupported unit is provided.\n        \"\"\"\n        prcp_var_name = \"p_mean\"\n        data_ds = self.read_attr_xrdataset(\n            gage_id_lst=gage_id_lst, var_lst=[prcp_var_name]\n        )\n        # No conversion needed\n        if unit in [\"mm/d\", \"mm/day\"]:\n            return data_ds\n\n        # Conversion needed, create a new dataset\n        converted_ds = data_ds.copy()\n        # After renaming, the variable in the dataset is now the standard name\n        if unit in [\"mm/h\", \"mm/hour\"]:\n            converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 24\n        elif unit in [\"mm/3h\", \"mm/3hour\"]:\n            converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 8\n        elif unit in [\"mm/8d\", \"mm/8day\"]:\n            converted_ds[prcp_var_name] = data_ds[prcp_var_name] * 8\n        else:\n            raise ValueError(\n                \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n            )\n        return converted_ds\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.available_dynamic_features","title":"<code>available_dynamic_features</code>  <code>property</code>","text":"<p>Returns a dictionary of available dynamic feature names and their possible sources.</p>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.available_static_features","title":"<code>available_static_features</code>  <code>property</code>","text":"<p>Returns a list of available static attribute names.</p>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.get_available_dynamic_features","title":"<code>get_available_dynamic_features()</code>","text":"<p>Returns a dictionary of available standard dynamic feature names and their possible sources.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_available_dynamic_features(self) -&gt; dict:\n    \"\"\"\n    Returns a dictionary of available standard dynamic feature names\n    and their possible sources.\n    \"\"\"\n    if (\n        not hasattr(self, \"_dynamic_variable_mapping\")\n        or not self._dynamic_variable_mapping\n    ):\n        return {}\n\n    feature_info = {}\n    for std_name, mapping_info in self._dynamic_variable_mapping.items():\n        feature_info[std_name] = {\n            \"default_source\": mapping_info.get(\"default_source\"),\n            \"available_sources\": list(mapping_info.get(\"sources\", {}).keys()),\n        }\n    return feature_info\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.get_available_static_features","title":"<code>get_available_static_features()</code>","text":"<p>Returns a list of available standard static feature names.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_available_static_features(self) -&gt; list:\n    \"\"\"Returns a list of available standard static feature names.\"\"\"\n    return list(self._static_variable_definitions.keys())\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.get_constant_cols","title":"<code>get_constant_cols()</code>","text":"<p>the constant cols in this data_source</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_constant_cols(self) -&gt; np.ndarray:\n    \"\"\"the constant cols in this data_source\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.get_other_cols","title":"<code>get_other_cols()</code>","text":"<p>the other cols in this data_source</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_other_cols(self) -&gt; dict:\n    \"\"\"the other cols in this data_source\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.get_relevant_cols","title":"<code>get_relevant_cols()</code>","text":"<p>the relevant cols in this data_source</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_relevant_cols(self) -&gt; np.ndarray:\n    \"\"\"the relevant cols in this data_source\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.get_target_cols","title":"<code>get_target_cols()</code>","text":"<p>the target cols in this data_source</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def get_target_cols(self) -&gt; np.ndarray:\n    \"\"\"the target cols in this data_source\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.read_area","title":"<code>read_area(gage_id_lst)</code>","text":"<p>Reads the catchment area for a list of basins.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list[str]</code> <p>A list of basin identifiers for which to retrieve the area.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the area data for the requested basins.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_area(self, gage_id_lst: list[str]) -&gt; xr.Dataset:\n    \"\"\"Reads the catchment area for a list of basins.\n\n    Args:\n        gage_id_lst: A list of basin identifiers for which to retrieve the area.\n\n    Returns:\n        An xarray Dataset containing the area data for the requested basins.\n    \"\"\"\n    data_ds = self.read_attr_xrdataset(gage_id_lst=gage_id_lst, var_lst=[\"area\"])\n    return data_ds\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.read_attr_xrdataset","title":"<code>read_attr_xrdataset(gage_id_lst=None, var_lst=None, to_numeric=True, **kwargs)</code>","text":"<p>Reads attribute data for a list of basins using standardized variable names.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list</code> <p>A list of basin identifiers.</p> <code>None</code> <code>var_lst</code> <code>list</code> <p>A list of standard attribute names to retrieve. If None, all available static features will be returned.</p> <code>None</code> <code>to_numeric</code> <code>bool</code> <p>If True, converts all non-numeric variables to numeric codes and stores the original labels in the variable's attributes. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the attribute data for the requested basins,</p> <code>Dataset</code> <p>with variables named using the standard names.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_attr_xrdataset(\n    self,\n    gage_id_lst: list = None,\n    var_lst: list = None,\n    to_numeric: bool = True,\n    **kwargs,\n) -&gt; xr.Dataset:\n    \"\"\"Reads attribute data for a list of basins using standardized variable names.\n\n    Args:\n        gage_id_lst: A list of basin identifiers.\n        var_lst: A list of **standard** attribute names to retrieve.\n            If None, all available static features will be returned.\n        to_numeric: If True, converts all non-numeric variables to numeric codes\n            and stores the original labels in the variable's attributes.\n            Defaults to True.\n\n    Returns:\n        An xarray Dataset containing the attribute data for the requested basins,\n        with variables named using the standard names.\n    \"\"\"\n    if var_lst is None:\n        var_lst = self.get_available_static_features()\n\n    # 1. Translate standard names to dataset-specific names\n    target_vars_to_fetch = []\n    rename_map = {}\n    for std_name in var_lst:\n        if std_name not in self._static_variable_definitions:\n            raise ValueError(\n                f\"'{std_name}' is not a recognized standard static variable.\"\n            )\n        actual_var_name = self._static_variable_definitions[std_name][\n            \"specific_name\"\n        ]\n        target_vars_to_fetch.append(actual_var_name)\n        rename_map[actual_var_name] = std_name\n\n    # 2. Read data from cache using actual variable names\n    attr_cache_file = self.cache_dir.joinpath(self._attributes_cache_filename)\n    try:\n        attr_ds = xr.open_dataset(attr_cache_file)\n    except FileNotFoundError:\n        self.cache_attributes_xrdataset()\n        attr_ds = xr.open_dataset(attr_cache_file)\n\n    # 3. Select variables and basins\n    ds_subset = attr_ds[target_vars_to_fetch]\n    if gage_id_lst is not None:\n        gage_id_lst = [str(gid) for gid in gage_id_lst]\n        ds_selected = ds_subset.sel(basin=gage_id_lst)\n    else:\n        ds_selected = ds_subset\n\n    # 4. Rename to standard names\n    final_ds = ds_selected.rename(rename_map)\n\n    if not to_numeric:\n        return final_ds\n\n    # 5. If to_numeric is True, perform conversion\n    converted_ds = xr.Dataset(coords=final_ds.coords)\n    for var_name, da in final_ds.data_vars.items():\n        if np.issubdtype(da.dtype, np.number):\n            converted_ds[var_name] = da\n        else:\n            # Assumes string-like array that needs factorizing\n            numeric_vals, labels = pd.factorize(da.values, sort=True)\n            new_da = xr.DataArray(\n                numeric_vals,\n                coords=da.coords,\n                dims=da.dims,\n                name=da.name,\n                attrs=da.attrs,  # Preserve original attributes\n            )\n            new_da.attrs[\"labels\"] = labels.tolist()\n            converted_ds[var_name] = new_da\n    return converted_ds\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.read_constant_cols","title":"<code>read_constant_cols(gage_id_lst=None, var_lst=None, **kwargs)</code>","text":"<p>2d data (site_num * var_num), non-time-series data</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_constant_cols(\n    self, gage_id_lst=None, var_lst=None, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"2d data (site_num * var_num), non-time-series data\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.read_mean_prcp","title":"<code>read_mean_prcp(gage_id_lst, unit='mm/d')</code>","text":"<p>Reads the mean daily precipitation for a list of basins, with unit conversion.</p> <p>Parameters:</p> Name Type Description Default <code>gage_id_lst</code> <code>list[str]</code> <p>A list of basin identifiers.</p> required <code>unit</code> <code>str</code> <p>The desired unit for the output precipitation. Defaults to \"mm/d\". Supported units: ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day'].</p> <code>'mm/d'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the mean precipitation data in the specified units.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported unit is provided.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_mean_prcp(self, gage_id_lst: list[str], unit: str = \"mm/d\") -&gt; xr.Dataset:\n    \"\"\"Reads the mean daily precipitation for a list of basins, with unit conversion.\n\n    Args:\n        gage_id_lst: A list of basin identifiers.\n        unit: The desired unit for the output precipitation. Defaults to \"mm/d\".\n            Supported units: ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h',\n            'mm/3hour', 'mm/8d', 'mm/8day'].\n\n    Returns:\n        An xarray Dataset containing the mean precipitation data in the specified units.\n\n    Raises:\n        ValueError: If an unsupported unit is provided.\n    \"\"\"\n    prcp_var_name = \"p_mean\"\n    data_ds = self.read_attr_xrdataset(\n        gage_id_lst=gage_id_lst, var_lst=[prcp_var_name]\n    )\n    # No conversion needed\n    if unit in [\"mm/d\", \"mm/day\"]:\n        return data_ds\n\n    # Conversion needed, create a new dataset\n    converted_ds = data_ds.copy()\n    # After renaming, the variable in the dataset is now the standard name\n    if unit in [\"mm/h\", \"mm/hour\"]:\n        converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 24\n    elif unit in [\"mm/3h\", \"mm/3hour\"]:\n        converted_ds[prcp_var_name] = data_ds[prcp_var_name] / 8\n    elif unit in [\"mm/8d\", \"mm/8day\"]:\n        converted_ds[prcp_var_name] = data_ds[prcp_var_name] * 8\n    else:\n        raise ValueError(\n            \"unit must be one of ['mm/d', 'mm/day', 'mm/h', 'mm/hour', 'mm/3h', 'mm/3hour', 'mm/8d', 'mm/8day']\"\n        )\n    return converted_ds\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.read_object_ids","title":"<code>read_object_ids()</code>","text":"<p>Read watershed station ID list.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_object_ids(self) -&gt; np.ndarray:\n    \"\"\"Read watershed station ID list.\"\"\"\n    if hasattr(self, \"aqua_fetch\"):\n        stations_list = self.aqua_fetch.stations()\n        return np.sort(np.array(stations_list))\n    raise NotImplementedError\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.read_other_cols","title":"<code>read_other_cols(object_ids=None, other_cols=None, **kwargs)</code>","text":"<p>some data which cannot be easily treated as constant vars or time-series with same length as relevant vars CONVENTION: other_cols is a dict, where each item is also a dict with all params in it</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_other_cols(\n    self, object_ids=None, other_cols: dict = None, **kwargs\n) -&gt; dict:\n    \"\"\"some data which cannot be easily treated as constant vars or time-series with same length as relevant vars\n    CONVENTION: other_cols is a dict, where each item is also a dict with all params in it\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.HydroDataset.read_relevant_cols","title":"<code>read_relevant_cols(gage_id_lst=None, t_range=None, var_lst=None, forcing_type=None, **kwargs)</code>","text":"<p>3d data (site_num * time_length * var_num), time-series data</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>def read_relevant_cols(\n    self, gage_id_lst=None, t_range=None, var_lst=None, forcing_type=None, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"3d data (site_num * time_length * var_num), time-series data\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.LamahCe","title":"<code>LamahCe</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>LamaHCE dataset class extending RainfallRunoff.</p> <p>This class provides access to the LamaHCE dataset, which contains hourly hydrological and meteorological data for various watersheds.</p> <p>Attributes:</p> Name Type Description <code>region</code> <p>Geographic region identifier</p> <code>download</code> <p>Whether to download data automatically</p> <code>ds_description</code> <p>Dictionary containing dataset file paths</p> Source code in <code>hydrodataset/lamah_ce.py</code> <pre><code>class LamahCe(HydroDataset):\n    \"\"\"LamaHCE dataset class extending RainfallRunoff.\n\n    This class provides access to the LamaHCE dataset, which contains hourly\n    hydrological and meteorological data for various watersheds.\n\n    Attributes:\n        region: Geographic region identifier\n        download: Whether to download data automatically\n        ds_description: Dictionary containing dataset file paths\n    \"\"\"\n\n    def __init__(self, data_path, region=None, download=False, cache_path=None):\n        \"\"\"Initialize LamaHCE dataset.\n\n        Args:\n            data_path: Path to the LamaHCE data directory\n            region: Geographic region identifier (optional)\n            download: Whether to download data automatically (default: False)\n            cache_path: Path to the cache directory\n        \"\"\"\n        super().__init__(data_path, cache_path=cache_path)\n        self.region = region\n        self.download = download\n        self.aqua_fetch = LamaHCE(data_path)\n\n    @property\n    def _attributes_cache_filename(self):\n        return \"lamahce_attributes.nc\"\n\n    @property\n    def _timeseries_cache_filename(self):\n        return \"lamahce_timeseries.nc\"\n\n    @property\n    def default_t_range(self):\n        return [\"1981-01-01\", \"2019-12-31\"]\n\n    def _get_attribute_units(self):\n        return {\n            # \u5730\u5f62\u7279\u5f81\n            \"dis_m3_\": \"m^3/s\",\n            \"run_mm_\": \"millimeter\",\n            \"inu_pc_\": \"percent\",\n            \"lka_pc_\": \"1e-1 * percent\",\n            \"lkv_mc_\": \"1e6 * m^3\",\n            \"rev_mc_\": \"1e6 * m^3\",\n            \"dor_pc_\": \"percent (x10)\",\n            \"ria_ha_\": \"hectares\",\n            \"riv_tc_\": \"1e3 * m^3\",\n            \"gwt_cm_\": \"centimeter\",\n            \"ele_mt_\": \"meter\",\n            \"slp_dg_\": \"1e-1 * degree\",\n            \"sgr_dk_\": \"decimeter/km\",\n            \"clz_cl_\": \"dimensionless\",\n            \"cls_cl_\": \"dimensionless\",\n            \"tmp_dc_\": \"degree_Celsius\",\n            \"pre_mm_\": \"millimeters\",\n            \"pet_mm_\": \"millimeters\",\n            \"aet_mm_\": \"millimeters\",\n            \"ari_ix_\": \"1e-2\",\n            \"cmi_ix_\": \"1e-2\",\n            \"snw_pc_\": \"percent\",\n            \"glc_cl_\": \"dimensionless\",\n            \"glc_pc_\": \"percent\",\n            \"pnv_cl_\": \"dimensionless\",\n            \"pnv_pc_\": \"percent\",\n            \"wet_cl_\": \"dimensionless\",\n            \"wet_pc_\": \"percent\",\n            \"for_pc_\": \"percent\",\n            \"crp_pc_\": \"percent\",\n            \"pst_pc_\": \"percent\",\n            \"ire_pc_\": \"percent\",\n            \"gla_pc_\": \"percent\",\n            \"prm_pc_\": \"percent\",\n            \"pac_pc_\": \"percent\",\n            \"tbi_cl_\": \"dimensionless\",\n            \"tec_cl_\": \"dimensionless\",\n            \"fmh_cl_\": \"dimensionless\",\n            \"fec_cl_\": \"dimensionless\",\n            \"cly_pc_\": \"percent\",\n            \"slt_pc_\": \"percent\",\n            \"snd_pc_\": \"percent\",\n            \"soc_th_\": \"tonne/hectare\",\n            \"swc_pc_\": \"percent\",\n            \"lit_cl_\": \"dimensionless\",\n            \"kar_pc_\": \"percent\",\n            \"ero_kh_\": \"kg/hectare/year\",\n            \"pop_ct_\": \"1e3\",\n            \"ppd_pk_\": \"1/km^2\",\n            \"urb_pc_\": \"percent\",\n            \"nli_ix_\": \"1e-2\",\n            \"rdd_mk_\": \"meter/km^2\",\n            \"hft_ix_\": \"1e-1\",\n            \"gad_id_\": \"dimensionless\",\n            \"gdp_ud_\": \"dimensionless\",\n            \"hdi_ix_\": \"1e-3\",\n        }\n\n    def _get_timeseries_units(self):\n        return [\n            \"mm^3/s\",  # q_cms_obs ML/day-&gt;mm^3/s,\u00d71.1574 \u00d7 10\u207b2\n            \"ML/day\",  # streamflow_MLd\n            \"mm/day\",  # q_mm_obs\n            \"mm/day\",  # aet_mm_silo_morton\n            \"mm/day\",  # aet_mm_silo_morton_point\n            \"mm/day\",  # et_morton_wet_SILO\n            \"mm/day\",  # aet_mm_silo_short_crop\n            \"mm/day\",  # aet_mm_silo_tall_crop\n            \"mm/day\",  # evap_morton_lake_SILO\n            \"mm/day\",  # evap_pan_SILO\n            \"mm/day\",  # evap_syn_SILO\n            \"mm/day\",  # pcp_mm_agcd\n            \"mm/day\",  # pcp_mm_silo\n            \"mm^2/d^2 \",  # precipitation_var_AGCD\n            \"\u00b0C\",  # airtemp_C_agcd_max\n            \"\u00b0C\",  # airtemp_C_agcd_min\n            \"hPa\",  # vp_hpa_agcd_h09\n            \"hPa\",  # vp_hpa_agcd_h15\n            \"hPa\",  # mslp_SILO\n            \"MJ/m\u00b2\",  # solrad_wm2_silo\n            \"%\",  # rh_%_silo_tmax\n            \"%\",  # rh_%_silo_tmin\n            \"\u00b0C\",  # airtemp_C_silo_max\n            \"\u00b0C\",  # airtemp_C_silo_min\n            \"hPa\",  # vp_deficit_SILO\n            \"hPa\",  # vp_hpa_silo\n            \"\u00b0C\",  # airtemp_C_mean_silo\n            \"\u00b0C\",  # airtemp_C_mean_agcd\n        ]\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.LamahCe.__init__","title":"<code>__init__(data_path, region=None, download=False, cache_path=None)</code>","text":"<p>Initialize LamaHCE dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <p>Path to the LamaHCE data directory</p> required <code>region</code> <p>Geographic region identifier (optional)</p> <code>None</code> <code>download</code> <p>Whether to download data automatically (default: False)</p> <code>False</code> <code>cache_path</code> <p>Path to the cache directory</p> <code>None</code> Source code in <code>hydrodataset/lamah_ce.py</code> <pre><code>def __init__(self, data_path, region=None, download=False, cache_path=None):\n    \"\"\"Initialize LamaHCE dataset.\n\n    Args:\n        data_path: Path to the LamaHCE data directory\n        region: Geographic region identifier (optional)\n        download: Whether to download data automatically (default: False)\n        cache_path: Path to the cache directory\n    \"\"\"\n    super().__init__(data_path, cache_path=cache_path)\n    self.region = region\n    self.download = download\n    self.aqua_fetch = LamaHCE(data_path)\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets","title":"<code>MultiDatasets</code>","text":"<p>               Bases: <code>HydroDataset</code></p> <p>A data source class for multiple datasets</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>class MultiDatasets(HydroDataset):\n    \"\"\"A data source class for multiple datasets\"\"\"\n\n    def __init__(\n        self,\n        data_path: list,\n        download=False,\n        datasets: list = None,\n        regions: list = None,\n    ):\n        \"\"\"\n\n        Parameters\n        ----------\n        data_path:\n            the paths of all necessary data\n        download:\n            if True, download the dataset\n        datasets:\n            a list with multiple datasets\n        regions\n            a list with multiple regions, each region corresponds to a dataset\n        \"\"\"\n        if regions is None:\n            regions = [\"US\"]\n        if type(regions) != list:\n            regions = [regions]\n        if type(data_path) != list:\n            data_path = [data_path]\n        if not set(datasets).issubset(set(DATASETS)):\n            raise NotImplementedError(\"We only support \" + DATASETS + \" now\")\n        if not set(regions).issubset(set(REGIONS)):\n            raise NotImplementedError(\"We only support \" + REGIONS + \" now\")\n        if len(data_path) != len(regions):\n            raise RuntimeError(\"Please choose directory for each region\")\n        for one_path in data_path:\n            super().__init__(one_path)\n        self.data_path = data_path\n        self.datasets = datasets\n        self.regions = regions\n        self.data_source_description = self.set_data_source_describe()\n        if download:\n            self.download_data_source()\n        self.sites = self.read_site_info()\n        self.site_region_dict = self.read_site_id_dict()\n        self.streamflow_dict = self.get_target_dict()\n        self.forcing_dict = self.get_relevant_dict()\n        self.attr_dict = self.get_constant_dict()\n        self.str_attr_name = [\"high_prec_timing\", \"low_prec_timing\", \"geol_1st_class\"]\n\n    def get_name(self):\n        return \"MULTI_DATASETS\"\n\n    def set_data_source_describe(self) -&gt; collections.OrderedDict:\n        describe = collections.OrderedDict({})\n        for i in range(len(self.regions)):\n            region = DATASETS_DICT[self.datasets[i]](\n                self.data_path[i], False, region=self.regions[i]\n            )\n            describe = collections.OrderedDict(\n                **describe, **{self.regions[i]: region.data_source_description}\n            )\n        return describe\n\n    def download_data_source(self):\n        for i in range(len(self.regions)):\n            DATASETS_DICT[self.datasets[i]](\n                self.data_path[i], True, region=self.regions[i]\n            )\n\n    def read_site_info(self) -&gt; collections.OrderedDict:\n        \"\"\"\n        Read the basic information of gages in multiple datasets\n\n        Returns\n        -------\n        collections.OrderedDict\n            basic info of gages for different regions\n        \"\"\"\n        site_info = collections.OrderedDict({})\n        for i in range(len(self.datasets)):\n            region = DATASETS_DICT[self.datasets[i]](\n                self.data_path[i], False, region=self.regions[i]\n            )\n            site_info = collections.OrderedDict(\n                **site_info, **{self.regions[i]: region.read_site_info()}\n            )\n        return site_info\n\n    def read_object_ids(self, object_params=None) -&gt; np.array:\n        region_id_lst = []\n        for i in range(len(self.regions)):\n            dataset = DATASETS_DICT[self.datasets[i]](\n                self.data_path[i], False, region=self.regions[i]\n            )\n            region_id_lst = region_id_lst + dataset.read_object_ids().tolist()\n        region_id_arr = np.array(region_id_lst)\n        if np.unique(region_id_arr).size != region_id_arr.size:\n            raise RuntimeError(\n                \"Same id for different sites, Error! Please check your chosen gages\"\n            )\n        return region_id_arr\n\n    def read_site_id_dict(self):\n        site_id = collections.OrderedDict({})\n        for i in range(len(self.regions)):\n            region = DATASETS_DICT[self.datasets[i]](\n                self.data_path[i], False, region=self.regions[i]\n            )\n            site_id = collections.OrderedDict(\n                **site_id, **{self.regions[i]: region.read_object_ids()}\n            )\n        return site_id\n\n    def read_target_cols(\n        self, object_ids=None, t_range_list=None, target_cols=None, **kwargs\n    ) -&gt; np.array:\n        \"\"\"\n        Read streamflow data\n\n        We unify the unit of streamflow to ft3/s because first our data interface used in models are set for CAMELS-US.\n\n        Parameters\n        ----------\n        object_ids\n            sites\n        t_range_list\n            time range\n        target_cols\n            streamflow variable\n        kwargs\n            optional parameters\n\n        Returns\n        -------\n        np.array\n            streamflow data for given sites\n        \"\"\"\n        nt = hydro_time.t_range_days(t_range_list).shape[0]\n        flow = np.full((len(object_ids), nt, len(target_cols)), np.nan)\n        for i in range(len(self.regions)):\n            region_now = self.regions[i]\n            dataset = DATASETS_DICT[self.datasets[i]](\n                self.data_path[i], False, region=region_now\n            )\n            flow_dict_map = {target_cols[0]: self.streamflow_dict[region_now]}\n            in_region_flow_tuple = [\n                (k, value) for k, (key, value) in enumerate(flow_dict_map.items())\n            ]\n            sites_tuple = [\n                (k, object_ids[k])\n                for k in range(len(object_ids))\n                if object_ids[k] in self.site_region_dict[region_now]\n            ]\n            sites_in_this_region = [a_tuple[1] for a_tuple in sites_tuple]\n            in_region_flow = [index_attr[1] for index_attr in in_region_flow_tuple]\n            flow_this_region = dataset.read_target_cols(\n                sites_in_this_region, t_range_list, in_region_flow\n            )\n            sites_idx = [a_tuple[0] for a_tuple in sites_tuple]\n            flow[sites_idx, :, :] = flow_this_region\n        return flow\n\n    def read_relevant_cols(\n        self, object_ids=None, t_range_list=None, relevant_cols=None, **kwargs\n    ) -&gt; np.array:\n        \"\"\"\n        Read data of common forcing variables between given regions\n\n        Parameters\n        ----------\n        object_ids\n            sites\n        t_range_list\n            time range\n        relevant_cols\n            common forcing variables for given regions\n        kwargs\n            optional parameters\n\n        Returns\n        -------\n        np.array\n            forcing data\n        \"\"\"\n        nt = hydro_time.t_range_days(t_range_list).shape[0]\n        forcings = np.full((len(object_ids), nt, len(relevant_cols)), np.nan)\n        for i in range(len(self.regions)):\n            region_now = self.regions[i]\n            dataset = DATASETS_DICT[self.datasets[i]](\n                self.data_path[i], False, region=region_now\n            )\n            region_forcing_all = dataset.get_relevant_cols()\n            common_forcing_all = self.get_relevant_cols()\n            j_index = [\n                common_forcing_all.tolist().index(relevant_cols[ii])\n                for ii in range(len(relevant_cols))\n            ]\n            forcing_dict_lst = [\n                {relevant_cols[j]: self.forcing_dict[region_now][j_index[j]]}\n                for j in range(len(relevant_cols))\n            ]\n            forcing_dict_map = reduce(\n                lambda x, y: collections.OrderedDict(**x, **y), forcing_dict_lst\n            )\n            in_region_forcing_tuple = [\n                (k, value)\n                for k, (key, value) in enumerate(forcing_dict_map.items())\n                if value in region_forcing_all\n            ]\n            not_in_region_forcing_tuple = [\n                (k, value)\n                for k, (key, value) in enumerate(forcing_dict_map.items())\n                if value not in region_forcing_all\n            ]\n            sites_tuple = [\n                (k, object_ids[k])\n                for k in range(len(object_ids))\n                if object_ids[k] in self.site_region_dict[region_now]\n            ]\n            sites_in_this_region = [a_tuple[1] for a_tuple in sites_tuple]\n            in_region_forcing = [\n                index_attr[1] for index_attr in in_region_forcing_tuple\n            ]\n            forcing1 = dataset.read_relevant_cols(\n                sites_in_this_region, t_range_list, in_region_forcing\n            )\n            forcing2 = self.read_not_included_forcings(\n                sites_in_this_region,\n                t_range_list,\n                [index_attr[1] for index_attr in not_in_region_forcing_tuple],\n                dataset,\n            )\n            forcing1_idx = [\n                index_forcing[0] for index_forcing in in_region_forcing_tuple\n            ]\n            forcing2_idx = [\n                index_forcing[0] for index_forcing in not_in_region_forcing_tuple\n            ]\n            sites_idx = [a_tuple[0] for a_tuple in sites_tuple]\n            forcing_this_region = np.empty(\n                (len(sites_idx), nt, len(forcing1_idx) + len(forcing2_idx))\n            )\n            forcing_this_region[:, :, forcing1_idx] = forcing1\n            forcing_this_region[:, :, forcing2_idx] = forcing2\n            for j in range(len(relevant_cols)):\n                # unit of srad in AUS is MJ/m2(/day) while others are W/m2\n                if region_now == \"AUS\" and relevant_cols[j] == \"srad\":\n                    forcing_this_region[:, :, j] = (\n                        forcing_this_region[:, :, j] * 1e6 / (24 * 3600)\n                    )\n                # unit of vp in AUS is hPa while others are Pa\n                if region_now == \"AUS\" and relevant_cols[j] == \"vp\":\n                    forcing_this_region[:, :, j] = forcing_this_region[:, :, j] * 100\n            forcings[sites_idx, :, :] = forcing_this_region\n        return forcings\n\n    @staticmethod\n    def read_not_included_forcings(\n        sites_id, t_range_list, forcing_cols, dataset\n    ) -&gt; np.array:\n        \"\"\"\n        read forcings not included in the dataset of the region\n\n        Parameters\n        ----------\n        sites_id\n\n        t_range_list\n\n        forcing_cols\n\n        dataset\n            data source\n\n\n        Returns\n        -------\n        np.array\n            forcings not included in original dataset\n        \"\"\"\n        nt = hydro_time.t_range_days(t_range_list).shape[0]\n        forcing_data = np.empty([len(sites_id), nt, len(forcing_cols)])\n        for i in range(len(forcing_cols)):\n            if forcing_cols[i] == \"\":\n                forcing_data[:, :, i] = np.full((len(sites_id), nt), np.nan)\n            elif forcing_cols[i] == \"PET\" and dataset.get_name() == \"CAMELS_US\":\n                forcing_data[:, :, i : i + 1] = (\n                    dataset.read_camels_us_model_output_data(\n                        sites_id, t_range_list, [\"PET\"]\n                    )\n                )\n            elif forcing_cols[i] == \"PET_A\" and dataset.get_name() == \"LamaH_CE\":\n                forcing_data[:, :, i : i + 1] = (\n                    dataset.read_lamah_hydro_model_time_series(\n                        sites_id, t_range_list, [\"PET_A\"]\n                    )\n                )\n        return forcing_data\n\n    def read_constant_cols(\n        self, object_ids=None, constant_cols: list = None, **kwargs\n    ) -&gt; np.array:\n        \"\"\"\n        Read data of common attribute variables between given regions\n\n        Parameters\n        ----------\n        object_ids\n            sites\n        constant_cols\n            common attribute variables for given regions\n        kwargs\n            optional parameters\n\n        Returns\n        -------\n        np.array\n            attribute data\n        \"\"\"\n        attrs = np.full((len(object_ids), len(constant_cols)), np.nan)\n        str_attr_dict = {}\n        for constant_col in constant_cols:\n            if constant_col in self.str_attr_name:\n                str_attr_dict = {\n                    **str_attr_dict,\n                    **{constant_col: np.empty(len(object_ids)).astype(str)},\n                }\n        for i in range(len(self.regions)):\n            region_now = self.regions[i]\n            dataset = DATASETS_DICT[self.datasets[i]](\n                self.data_path[i], False, region=region_now\n            )\n            region_attr_all = dataset.get_constant_cols()\n            common_attr_all = self.get_constant_cols()\n            j_index = [\n                common_attr_all.tolist().index(constant_col_)\n                for constant_col_ in constant_cols\n            ]\n            constant_dict_lst = [\n                {constant_cols[j]: self.attr_dict[region_now][j_index[j]]}\n                for j in range(len(constant_cols))\n            ]\n            constant_dict_map = reduce(\n                lambda x, y: collections.OrderedDict(**x, **y), constant_dict_lst\n            )\n            in_region_attr_tuple = [\n                (k, value)\n                for k, (key, value) in enumerate(constant_dict_map.items())\n                if value in region_attr_all\n            ]\n            not_in_region_attr_tuple = [\n                (k, value)\n                for k, (key, value) in enumerate(constant_dict_map.items())\n                if value not in region_attr_all\n            ]\n            sites_tuple = [\n                (k, object_ids[k])\n                for k in range(len(object_ids))\n                if object_ids[k] in self.site_region_dict[region_now]\n            ]\n            sites_in_this_region = [a_tuple[1] for a_tuple in sites_tuple]\n            in_region_attr = [index_attr[1] for index_attr in in_region_attr_tuple]\n            if not set(in_region_attr).issubset(set(region_attr_all)):\n                raise NotImplementedError(\n                    \"Wrong name for attributes, please check your input for attributes\"\n                )\n            attrs1, var_dict, f_dict = dataset.read_constant_cols(\n                sites_in_this_region, in_region_attr, is_return_dict=True\n            )\n            attrs2 = self.read_not_included_attrs(\n                sites_in_this_region,\n                [index_attr[1] for index_attr in not_in_region_attr_tuple],\n                dataset,\n            )\n            attrs1_idx = [index_attr[0] for index_attr in in_region_attr_tuple]\n            attrs2_idx = [index_attr[0] for index_attr in not_in_region_attr_tuple]\n            sites_idx = [a_tuple[0] for a_tuple in sites_tuple]\n            attrs_this_region = np.empty(\n                (len(sites_idx), len(attrs1_idx) + len(attrs2_idx))\n            )\n            attrs_this_region[:, attrs1_idx] = attrs1\n            attrs_this_region[:, attrs2_idx] = attrs2\n            for j in range(len(constant_cols)):\n                # restore to str\n                if constant_cols[j] in self.str_attr_name:\n                    # self.str_attr_name = [\"high_prec_timing\", \"low_prec_timing\", \"geol_1st_class\"]\n                    restore_attr = np.array(\n                        [\n                            (\n                                \"\"\n                                if np.isnan(tmp)\n                                else f_dict[list(constant_dict_lst[j].values())[0]][\n                                    int(tmp)\n                                ]\n                            )\n                            for tmp in attrs_this_region[:, j]\n                        ]\n                    )\n                    str_attr_dict[constant_cols[j]][sites_idx] = restore_attr\n                # unit of slope_mean in AUS is % while others are m/km\n                if region_now == \"AUS\" and constant_cols[j] == \"slope_mean\":\n                    attrs_this_region[:, j] = attrs_this_region[:, j] * 10.0\n                # unit of soil_depth in BR is cm while others are m\n                if region_now == \"BR\" and constant_cols[j] == \"soil_depth\":\n                    attrs_this_region[:, j] = attrs_this_region[:, j] / 100.0\n                # unit of soil_conductivity in AUS is mm/h while others are cm/h\n                if region_now == \"AUS\" and constant_cols[j] == \"soil_conductivity\":\n                    attrs_this_region[:, j] = attrs_this_region[:, j] / 10.0\n                # frac of gc_dom in CE need to be calculated\n                if region_now == \"CE\" and constant_cols[j] == \"geol_1st_class_frac\":\n                    geo_types_names_in_ce = [\n                        \"gc_ig_fra\",\n                        \"gc_mt_fra\",\n                        \"gc_pa_fra\",\n                        \"gc_pb_fra\",\n                        \"gc_pi_fra\",\n                        \"gc_py_fra\",\n                        \"gc_sc_fra\",\n                        \"gc_sm_fra\",\n                        \"gc_ss_fra\",\n                        \"gc_su_fra\",\n                        \"gc_va_fra\",\n                        \"gc_vb_fra\",\n                        \"gc_wb_fra\",\n                    ]\n                    geo_fracs = dataset.read_constant_cols(\n                        sites_in_this_region, geo_types_names_in_ce\n                    )\n                    attrs_this_region[:, j] = np.array(\n                        [\n                            geo_fracs[\n                                k,\n                                geo_types_names_in_ce.index(\n                                    \"gc_\" + restore_attr[k] + \"_fra\"\n                                ),\n                            ]\n                            for k in range(len(restore_attr))\n                        ]\n                    )\n                # the type of geol_1st_class in GB are set \"unknown\", so all fractions are set to 100%\n                if region_now == \"GB\" and constant_cols[j] == \"geol_1st_class_frac\":\n                    attrs_this_region[:, j] = np.full(\n                        attrs_this_region[:, j].shape, 1.0\n                    )\n            attrs[sites_idx, :] = attrs_this_region\n        for j in range(len(constant_cols)):\n            # trans str attr to number\n            if constant_cols[j] in self.str_attr_name:\n                if constant_cols[j] == \"geol_1st_class\":\n                    geo_class_dict = {\n                        \"\": \"unknown\",\n                        \"Acid plutonic rocks\": \"pa\",\n                        \"Acid volcanic rocks\": \"va\",\n                        \"Basic plutonic rocks\": \"pb\",\n                        \"Basic volcanic rocks\": \"vb\",\n                        \"CARBNATESED\": \"sc\",\n                        \"Carbonate sedimentary rocks\": \"sc\",\n                        \"IGNEOUS\": \"igneous\",\n                        \"Ice and glaciers\": \"ig\",\n                        \"Intermediate plutonic rocks\": \"pi\",\n                        \"Intermediate volcanic rocks\": \"vi\",\n                        \"METAMORPH\": \"mt\",\n                        \"Metamorphics\": \"mt\",\n                        \"Mixed sedimentary rocks\": \"sm\",\n                        \"OTHERSED\": \"othersed\",\n                        \"Pyroclastics\": \"py\",\n                        \"SEDVOLC\": \"sm\",\n                        \"SILICSED\": \"ss\",\n                        \"Siliciclastic sedimentary rocks\": \"ss\",\n                        \"UNCONSOLDTED\": \"su\",\n                        \"Unconsolidated sediments\": \"su\",\n                        \"Water bodies\": \"wb\",\n                        \"acid_plutonic_rocks\": \"pa\",\n                        \"acid_volcanic_rocks\": \"va\",\n                        \"basic_volcanic_rocks\": \"vb\",\n                        \"carbonate_sedimentary_rocks\": \"sc\",\n                        \"intermediate_volcanic_rocks\": \"vi\",\n                        \"metamorphics\": \"mt\",\n                        \"mixed_sedimentary_rocks\": \"sm\",\n                        \"mt\": \"mt\",\n                        \"pa\": \"pa\",\n                        \"pi\": \"pi\",\n                        \"pyroclastics\": \"py\",\n                        \"sc\": \"sc\",\n                        \"siliciclastic_sedimentary_rocks\": \"ss\",\n                        \"sm\": \"sm\",\n                        \"ss\": \"ss\",\n                        \"su\": \"su\",\n                        \"unconsolidated_sediments\": \"su\",\n                        \"vb\": \"vb\",\n                    }\n                    str_attr_dict[constant_cols[j]] = np.array(\n                        [geo_class_dict[tmp] for tmp in str_attr_dict[constant_cols[j]]]\n                    )\n                value, ref = pd.factorize(str_attr_dict[constant_cols[j]], sort=True)\n                attrs[:, j] = value\n\n        return attrs\n\n    @staticmethod\n    def read_not_included_attrs(sites_id, attr_cols, dataset) -&gt; np.array:\n        \"\"\"\n        read attributes not included in the dataset of the region\n\n        Parameters\n        ----------\n        sites_id\n\n        attr_cols\n\n        dataset\n            a dataset class instance\n\n\n        Returns\n        -------\n        np.array\n            attrs not included in original dataset\n        \"\"\"\n        attr_data = np.empty([len(sites_id), len(attr_cols)])\n        for i in range(len(attr_cols)):\n            if attr_cols[i] == \"\":\n                attr_data[:, i] = np.full(len(sites_id), np.nan)\n            elif (\n                attr_cols[i] == \"dwood_perc+ewood_perc+shrub_perc\"\n                and dataset.get_name() == \"CAMELS_GB\"\n            ):\n                three_perc = dataset.read_constant_cols(\n                    sites_id, [\"dwood_perc\", \"ewood_perc\", \"shrub_perc\"]\n                )\n                attr_data[:, i] = np.sum(three_perc, axis=1)\n        return attr_data\n\n    def get_constant_cols(self) -&gt; np.array:\n        \"\"\"\n        Only read common attribute variables between given regions\n\n        Returns\n        -------\n        np.array\n            common attribute variables for given regions\n        \"\"\"\n        return np.array(\n            [\n                \"p_mean\",\n                \"pet_mean\",\n                \"aridity\",\n                \"p_seasonality\",\n                \"frac_snow\",\n                \"high_prec_freq\",\n                \"high_prec_dur\",\n                \"high_prec_timing\",\n                \"low_prec_freq\",\n                \"low_prec_dur\",\n                \"low_prec_timing\",  # climate 0-10\n                \"elev_mean\",\n                \"slope_mean\",\n                \"area\",  # topography 11-13\n                \"forest_frac\",  # land cover 14\n                \"soil_depth\",\n                \"soil_conductivity\",\n                \"sand_frac\",\n                \"silt_frac\",\n                \"clay_frac\",  # soil 15-19\n                \"geol_1st_class\",\n                \"geol_1st_class_frac\",\n            ]\n        )  # geology; aus may have different classes for geology 20-21\n\n    def get_constant_dict(self) -&gt; collections.OrderedDict:\n        \"\"\"\n        common attribute variables, get their real names in different datasets\n\n        Returns\n        -------\n        collections.OrderedDict\n            attribute variables for all regions\n        \"\"\"\n        attr_dict = collections.OrderedDict({})\n        for i in range(len(self.regions)):\n            if self.regions[i] == \"AUS\":\n                attr_dict = collections.OrderedDict(\n                    **attr_dict,\n                    **{\n                        self.regions[i]: [\n                            \"p_mean\",\n                            \"pet_mean\",\n                            \"aridity\",\n                            \"p_seasonality\",\n                            \"frac_snow\",\n                            \"high_prec_freq\",\n                            \"high_prec_dur\",\n                            \"high_prec_timing\",\n                            \"low_prec_freq\",\n                            \"low_prec_dur\",\n                            \"low_prec_timing\",\n                            \"elev_mean\",\n                            \"mean_slope_pct\",\n                            \"catchment_area\",\n                            \"prop_forested\",\n                            \"solum_thickness\",\n                            \"ksat\",\n                            \"sanda\",\n                            \"\",\n                            \"claya\",\n                            \"geol_prim\",\n                            \"geol_prim_prop\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"BR\":\n                attr_dict = collections.OrderedDict(\n                    **attr_dict,\n                    **{\n                        self.regions[i]: [\n                            \"p_mean\",\n                            \"pet_mean\",\n                            \"aridity\",\n                            \"p_seasonality\",\n                            \"frac_snow\",\n                            \"high_prec_freq\",\n                            \"high_prec_dur\",\n                            \"high_prec_timing\",\n                            \"low_prec_freq\",\n                            \"low_prec_dur\",\n                            \"low_prec_timing\",\n                            \"elev_mean\",\n                            \"slope_mean\",\n                            \"area\",\n                            \"forest_perc\",\n                            \"bedrock_depth\",\n                            \"\",\n                            \"sand_perc\",\n                            \"silt_perc\",\n                            \"clay_perc\",\n                            \"geol_class_1st\",\n                            \"geol_class_1st_perc\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"CL\":\n                attr_dict = collections.OrderedDict(\n                    **attr_dict,\n                    **{\n                        self.regions[i]: [\n                            \"p_mean_cr2met\",\n                            \"pet_mean\",\n                            \"aridity_cr2met\",\n                            \"p_seasonality_cr2met\",\n                            \"frac_snow_cr2met\",\n                            \"high_prec_freq_cr2met\",\n                            \"high_prec_dur_cr2met\",\n                            \"high_prec_timing_cr2met\",\n                            \"low_prec_freq_cr2met\",\n                            \"low_prec_dur_cr2met\",\n                            \"low_prec_timing_cr2met\",\n                            \"elev_mean\",\n                            \"slope_mean\",\n                            \"area\",\n                            \"forest_frac\",\n                            \"\",\n                            \"\",\n                            \"\",\n                            \"\",\n                            \"\",\n                            \"geol_class_1st\",\n                            \"geol_class_1st_frac\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"GB\":\n                attr_dict = collections.OrderedDict(\n                    **attr_dict,\n                    **{\n                        self.regions[i]: [\n                            \"p_mean\",\n                            \"pet_mean\",\n                            \"aridity\",\n                            \"p_seasonality\",\n                            \"frac_snow\",\n                            \"high_prec_freq\",\n                            \"high_prec_dur\",\n                            \"high_prec_timing\",\n                            \"low_prec_freq\",\n                            \"low_prec_dur\",\n                            \"low_prec_timing\",\n                            \"elev_mean\",\n                            \"dpsbar\",\n                            \"area\",\n                            \"dwood_perc+ewood_perc+shrub_perc\",\n                            \"soil_depth_pelletier\",\n                            \"conductivity_cosby\",\n                            \"sand_perc\",\n                            \"silt_perc\",\n                            \"clay_perc\",\n                            \"\",\n                            \"\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"US\":\n                attr_dict = collections.OrderedDict(\n                    **attr_dict,\n                    **{\n                        self.regions[i]: [\n                            \"p_mean\",\n                            \"pet_mean\",\n                            \"aridity\",\n                            \"p_seasonality\",\n                            \"frac_snow\",\n                            \"high_prec_freq\",\n                            \"high_prec_dur\",\n                            \"high_prec_timing\",\n                            \"low_prec_freq\",\n                            \"low_prec_dur\",\n                            \"low_prec_timing\",\n                            \"elev_mean\",\n                            \"slope_mean\",\n                            \"area_gages2\",\n                            \"frac_forest\",\n                            \"soil_depth_pelletier\",\n                            \"soil_conductivity\",\n                            \"sand_frac\",\n                            \"silt_frac\",\n                            \"clay_frac\",\n                            \"geol_1st_class\",\n                            \"glim_1st_class_frac\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"YR\":\n                raise NotImplementedError(\n                    \"Yellow river only provided normalized streamflow data, so please don't use it now.\"\n                )\n            elif self.regions[i] == \"CA\":\n                raise NotImplementedError(\n                    \"Only a few attributes are provided in CA region, so please don't use it now.\"\n                )\n            elif self.regions[i] == \"CE\":\n                attr_dict = collections.OrderedDict(\n                    **attr_dict,\n                    **{\n                        self.regions[i]: [\n                            \"p_mean\",\n                            \"et0_mean\",\n                            \"arid_1\",\n                            \"p_season\",\n                            \"frac_snow\",\n                            \"hi_prec_fr\",\n                            \"hi_prec_du\",\n                            \"hi_prec_ti\",\n                            \"lo_prec_fr\",\n                            \"lo_prec_du\",\n                            \"lo_prec_ti\",\n                            \"elev_mean\",\n                            \"slope_mean\",\n                            \"area_calc\",\n                            \"forest_fra\",\n                            \"bedrk_dep\",\n                            \"soil_condu\",\n                            \"sand_fra\",\n                            \"silt_fra\",\n                            \"clay_fra\",\n                            \"gc_dom\",\n                            \"\",\n                        ]\n                    },\n                )\n            else:\n                raise NotImplementedError(CAMELS_NO_DATASET_ERROR_LOG)\n        return attr_dict\n\n    def get_relevant_cols(self):\n        \"\"\"\n        Only read common forcing variables between given regions\n\n        Returns\n        -------\n        np.array\n            common forcing variables for given regions\n        \"\"\"\n        return np.array(\n            [\"aet\", \"pet\", \"prcp\", \"srad\", \"swe\", \"tmax\", \"tmean\", \"tmin\", \"vp\"]\n        )\n\n    def get_relevant_dict(self) -&gt; collections.OrderedDict:\n        \"\"\"\n        common forcing variables, get their real names in different datasets\n\n        Returns\n        -------\n        collections.OrderedDict\n            forcing variables for all regions\n        \"\"\"\n        forcing_dict = collections.OrderedDict({})\n        for i in range(len(self.regions)):\n            if self.regions[i] == \"AUS\":\n                # we use SILO data here\n                forcing_dict = collections.OrderedDict(\n                    **forcing_dict,\n                    **{\n                        self.regions[i]: [\n                            \"et_morton_actual_SILO\",\n                            \"et_morton_point_SILO\",\n                            \"precipitation_SILO\",\n                            \"radiation_SILO\",\n                            \"\",\n                            \"tmax_SILO\",\n                            \"\",\n                            \"tmin_SILO\",\n                            \"vp_SILO\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"BR\":\n                forcing_dict = collections.OrderedDict(\n                    **forcing_dict,\n                    **{\n                        self.regions[i]: [\n                            \"evapotransp_gleam\",\n                            \"potential_evapotransp_gleam\",\n                            \"precipitation_chirps\",\n                            \"\",\n                            \"\",\n                            \"temperature_max_cpc\",\n                            \"temperature_mean_cpc\",\n                            \"temperature_min_cpc\",\n                            \"\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"CL\":\n                forcing_dict = collections.OrderedDict(\n                    **forcing_dict,\n                    **{\n                        self.regions[i]: [\n                            \"\",\n                            \"pet_hargreaves\",\n                            \"precip_cr2met\",\n                            \"\",\n                            \"swe\",\n                            \"tmax_cr2met\",\n                            \"tmean_cr2met\",\n                            \"tmin_cr2met\",\n                            \"\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"GB\":\n                forcing_dict = collections.OrderedDict(\n                    **forcing_dict,\n                    **{\n                        self.regions[i]: [\n                            \"\",\n                            \"pet\",\n                            \"precipitation\",\n                            \"shortwave_rad\",\n                            \"\",\n                            \"\",\n                            \"temperature\",\n                            \"\",\n                            \"\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"US\":\n                forcing_dict = collections.OrderedDict(\n                    **forcing_dict,\n                    **{\n                        self.regions[i]: [\n                            \"\",\n                            \"PET\",\n                            \"prcp\",\n                            \"srad\",\n                            \"swe\",\n                            \"tmax\",\n                            \"\",\n                            \"tmin\",\n                            \"vp\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"YR\":\n                raise NotImplementedError(\n                    \"Yellow river only provided normalized streamflow data, so please don't use it now.\"\n                )\n            elif self.regions[i] == \"CA\":\n                forcing_dict = collections.OrderedDict(\n                    **forcing_dict,\n                    **{\n                        self.regions[i]: [\n                            \"\",\n                            \"\",\n                            \"prcp\",\n                            \"\",\n                            \"\",\n                            \"tmax\",\n                            \"\",\n                            \"tmin\",\n                            \"\",\n                        ]\n                    },\n                )\n            elif self.regions[i] == \"CE\":\n                forcing_dict = collections.OrderedDict(\n                    **forcing_dict,\n                    **{\n                        self.regions[i]: [\n                            \"total_et\",\n                            \"PET_A\",\n                            \"prec\",\n                            \"surf_net_solar_rad_mean\",\n                            \"swe\",\n                            \"2m_temp_max\",\n                            \"2m_temp_mean\",\n                            \"2m_temp_min\",\n                            \"\",\n                        ]\n                    },\n                )\n            else:\n                raise NotImplementedError(CAMELS_NO_DATASET_ERROR_LOG)\n        return forcing_dict\n\n    def get_target_cols(self) -&gt; np.array:\n        \"\"\"\n        Only read common target variables -- streamflow\n\n        Returns\n        -------\n        np.array\n            common target variables for given regions\n        \"\"\"\n        # unify all \"streamflow\" names to \"streamflow\"\n        return np.array([\"streamflow\"])\n\n    def get_target_dict(self) -&gt; collections.OrderedDict:\n        \"\"\"\n        common target variable -- streamflow, get its real name in different datasets\n\n        Returns\n        -------\n        collections.OrderedDict\n            streamflow variables for all regions\n        \"\"\"\n        streamflow_dict = collections.OrderedDict({})\n        for i in range(len(self.regions)):\n            if self.regions[i] == \"US\":\n                # its unit is ft3/s\n                streamflow_dict = collections.OrderedDict(\n                    **streamflow_dict, **{self.regions[i]: \"usgsFlow\"}\n                )\n            elif self.regions[i] == \"AUS\":\n                # MLd means \"1 Megaliters Per Day\"; 1 MLd = 0.011574074074074 cubic-meters-per-second\n                streamflow_dict = collections.OrderedDict(\n                    **streamflow_dict, **{self.regions[i]: \"streamflow_MLd\"}\n                )\n            elif self.regions[i] in [\"BR\", \"CL\"]:\n                streamflow_dict = collections.OrderedDict(\n                    **streamflow_dict, **{self.regions[i]: \"streamflow_m3s\"}\n                )\n            elif self.regions[i] == \"GB\":\n                streamflow_dict = collections.OrderedDict(\n                    **streamflow_dict, **{self.regions[i]: \"discharge_vol\"}\n                )\n            elif self.regions[i] == \"YR\":\n                raise NotImplementedError(\n                    \"Yellow river only provided normalized streamflow data, so please don't use it now.\"\n                )\n            elif self.regions[i] == \"CA\":\n                # mm/day, remember to trans to m3/s\n                streamflow_dict = collections.OrderedDict(\n                    **streamflow_dict, **{self.regions[i]: \"discharge\"}\n                )\n            elif self.regions[i] == \"CE\":\n                streamflow_dict = collections.OrderedDict(\n                    **streamflow_dict, **{self.regions[i]: \"qobs\"}\n                )\n            else:\n                raise NotImplementedError(CAMELS_NO_DATASET_ERROR_LOG)\n        return streamflow_dict\n\n    def get_other_cols(self) -&gt; dict:\n        pass\n\n    def read_other_cols(self, object_ids=None, other_cols=None, **kwargs):\n        pass\n\n    def read_area(self, object_ids) -&gt; np.array:\n        return self.read_constant_cols(object_ids, [\"area\"], is_return_dict=False)\n\n    def read_mean_prep(self, object_ids) -&gt; np.array:\n        return self.read_constant_cols(object_ids, [\"p_mean\"], is_return_dict=False)\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.__init__","title":"<code>__init__(data_path, download=False, datasets=None, regions=None)</code>","text":""},{"location":"hydrodataset/#hydrodataset.MultiDatasets.__init__--parameters","title":"Parameters","text":"<p>data_path:     the paths of all necessary data download:     if True, download the dataset datasets:     a list with multiple datasets regions     a list with multiple regions, each region corresponds to a dataset</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def __init__(\n    self,\n    data_path: list,\n    download=False,\n    datasets: list = None,\n    regions: list = None,\n):\n    \"\"\"\n\n    Parameters\n    ----------\n    data_path:\n        the paths of all necessary data\n    download:\n        if True, download the dataset\n    datasets:\n        a list with multiple datasets\n    regions\n        a list with multiple regions, each region corresponds to a dataset\n    \"\"\"\n    if regions is None:\n        regions = [\"US\"]\n    if type(regions) != list:\n        regions = [regions]\n    if type(data_path) != list:\n        data_path = [data_path]\n    if not set(datasets).issubset(set(DATASETS)):\n        raise NotImplementedError(\"We only support \" + DATASETS + \" now\")\n    if not set(regions).issubset(set(REGIONS)):\n        raise NotImplementedError(\"We only support \" + REGIONS + \" now\")\n    if len(data_path) != len(regions):\n        raise RuntimeError(\"Please choose directory for each region\")\n    for one_path in data_path:\n        super().__init__(one_path)\n    self.data_path = data_path\n    self.datasets = datasets\n    self.regions = regions\n    self.data_source_description = self.set_data_source_describe()\n    if download:\n        self.download_data_source()\n    self.sites = self.read_site_info()\n    self.site_region_dict = self.read_site_id_dict()\n    self.streamflow_dict = self.get_target_dict()\n    self.forcing_dict = self.get_relevant_dict()\n    self.attr_dict = self.get_constant_dict()\n    self.str_attr_name = [\"high_prec_timing\", \"low_prec_timing\", \"geol_1st_class\"]\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_constant_cols","title":"<code>get_constant_cols()</code>","text":"<p>Only read common attribute variables between given regions</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_constant_cols--returns","title":"Returns","text":"<p>np.array     common attribute variables for given regions</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def get_constant_cols(self) -&gt; np.array:\n    \"\"\"\n    Only read common attribute variables between given regions\n\n    Returns\n    -------\n    np.array\n        common attribute variables for given regions\n    \"\"\"\n    return np.array(\n        [\n            \"p_mean\",\n            \"pet_mean\",\n            \"aridity\",\n            \"p_seasonality\",\n            \"frac_snow\",\n            \"high_prec_freq\",\n            \"high_prec_dur\",\n            \"high_prec_timing\",\n            \"low_prec_freq\",\n            \"low_prec_dur\",\n            \"low_prec_timing\",  # climate 0-10\n            \"elev_mean\",\n            \"slope_mean\",\n            \"area\",  # topography 11-13\n            \"forest_frac\",  # land cover 14\n            \"soil_depth\",\n            \"soil_conductivity\",\n            \"sand_frac\",\n            \"silt_frac\",\n            \"clay_frac\",  # soil 15-19\n            \"geol_1st_class\",\n            \"geol_1st_class_frac\",\n        ]\n    )  # geology; aus may have different classes for geology 20-21\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_constant_dict","title":"<code>get_constant_dict()</code>","text":"<p>common attribute variables, get their real names in different datasets</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_constant_dict--returns","title":"Returns","text":"<p>collections.OrderedDict     attribute variables for all regions</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def get_constant_dict(self) -&gt; collections.OrderedDict:\n    \"\"\"\n    common attribute variables, get their real names in different datasets\n\n    Returns\n    -------\n    collections.OrderedDict\n        attribute variables for all regions\n    \"\"\"\n    attr_dict = collections.OrderedDict({})\n    for i in range(len(self.regions)):\n        if self.regions[i] == \"AUS\":\n            attr_dict = collections.OrderedDict(\n                **attr_dict,\n                **{\n                    self.regions[i]: [\n                        \"p_mean\",\n                        \"pet_mean\",\n                        \"aridity\",\n                        \"p_seasonality\",\n                        \"frac_snow\",\n                        \"high_prec_freq\",\n                        \"high_prec_dur\",\n                        \"high_prec_timing\",\n                        \"low_prec_freq\",\n                        \"low_prec_dur\",\n                        \"low_prec_timing\",\n                        \"elev_mean\",\n                        \"mean_slope_pct\",\n                        \"catchment_area\",\n                        \"prop_forested\",\n                        \"solum_thickness\",\n                        \"ksat\",\n                        \"sanda\",\n                        \"\",\n                        \"claya\",\n                        \"geol_prim\",\n                        \"geol_prim_prop\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"BR\":\n            attr_dict = collections.OrderedDict(\n                **attr_dict,\n                **{\n                    self.regions[i]: [\n                        \"p_mean\",\n                        \"pet_mean\",\n                        \"aridity\",\n                        \"p_seasonality\",\n                        \"frac_snow\",\n                        \"high_prec_freq\",\n                        \"high_prec_dur\",\n                        \"high_prec_timing\",\n                        \"low_prec_freq\",\n                        \"low_prec_dur\",\n                        \"low_prec_timing\",\n                        \"elev_mean\",\n                        \"slope_mean\",\n                        \"area\",\n                        \"forest_perc\",\n                        \"bedrock_depth\",\n                        \"\",\n                        \"sand_perc\",\n                        \"silt_perc\",\n                        \"clay_perc\",\n                        \"geol_class_1st\",\n                        \"geol_class_1st_perc\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"CL\":\n            attr_dict = collections.OrderedDict(\n                **attr_dict,\n                **{\n                    self.regions[i]: [\n                        \"p_mean_cr2met\",\n                        \"pet_mean\",\n                        \"aridity_cr2met\",\n                        \"p_seasonality_cr2met\",\n                        \"frac_snow_cr2met\",\n                        \"high_prec_freq_cr2met\",\n                        \"high_prec_dur_cr2met\",\n                        \"high_prec_timing_cr2met\",\n                        \"low_prec_freq_cr2met\",\n                        \"low_prec_dur_cr2met\",\n                        \"low_prec_timing_cr2met\",\n                        \"elev_mean\",\n                        \"slope_mean\",\n                        \"area\",\n                        \"forest_frac\",\n                        \"\",\n                        \"\",\n                        \"\",\n                        \"\",\n                        \"\",\n                        \"geol_class_1st\",\n                        \"geol_class_1st_frac\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"GB\":\n            attr_dict = collections.OrderedDict(\n                **attr_dict,\n                **{\n                    self.regions[i]: [\n                        \"p_mean\",\n                        \"pet_mean\",\n                        \"aridity\",\n                        \"p_seasonality\",\n                        \"frac_snow\",\n                        \"high_prec_freq\",\n                        \"high_prec_dur\",\n                        \"high_prec_timing\",\n                        \"low_prec_freq\",\n                        \"low_prec_dur\",\n                        \"low_prec_timing\",\n                        \"elev_mean\",\n                        \"dpsbar\",\n                        \"area\",\n                        \"dwood_perc+ewood_perc+shrub_perc\",\n                        \"soil_depth_pelletier\",\n                        \"conductivity_cosby\",\n                        \"sand_perc\",\n                        \"silt_perc\",\n                        \"clay_perc\",\n                        \"\",\n                        \"\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"US\":\n            attr_dict = collections.OrderedDict(\n                **attr_dict,\n                **{\n                    self.regions[i]: [\n                        \"p_mean\",\n                        \"pet_mean\",\n                        \"aridity\",\n                        \"p_seasonality\",\n                        \"frac_snow\",\n                        \"high_prec_freq\",\n                        \"high_prec_dur\",\n                        \"high_prec_timing\",\n                        \"low_prec_freq\",\n                        \"low_prec_dur\",\n                        \"low_prec_timing\",\n                        \"elev_mean\",\n                        \"slope_mean\",\n                        \"area_gages2\",\n                        \"frac_forest\",\n                        \"soil_depth_pelletier\",\n                        \"soil_conductivity\",\n                        \"sand_frac\",\n                        \"silt_frac\",\n                        \"clay_frac\",\n                        \"geol_1st_class\",\n                        \"glim_1st_class_frac\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"YR\":\n            raise NotImplementedError(\n                \"Yellow river only provided normalized streamflow data, so please don't use it now.\"\n            )\n        elif self.regions[i] == \"CA\":\n            raise NotImplementedError(\n                \"Only a few attributes are provided in CA region, so please don't use it now.\"\n            )\n        elif self.regions[i] == \"CE\":\n            attr_dict = collections.OrderedDict(\n                **attr_dict,\n                **{\n                    self.regions[i]: [\n                        \"p_mean\",\n                        \"et0_mean\",\n                        \"arid_1\",\n                        \"p_season\",\n                        \"frac_snow\",\n                        \"hi_prec_fr\",\n                        \"hi_prec_du\",\n                        \"hi_prec_ti\",\n                        \"lo_prec_fr\",\n                        \"lo_prec_du\",\n                        \"lo_prec_ti\",\n                        \"elev_mean\",\n                        \"slope_mean\",\n                        \"area_calc\",\n                        \"forest_fra\",\n                        \"bedrk_dep\",\n                        \"soil_condu\",\n                        \"sand_fra\",\n                        \"silt_fra\",\n                        \"clay_fra\",\n                        \"gc_dom\",\n                        \"\",\n                    ]\n                },\n            )\n        else:\n            raise NotImplementedError(CAMELS_NO_DATASET_ERROR_LOG)\n    return attr_dict\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_relevant_cols","title":"<code>get_relevant_cols()</code>","text":"<p>Only read common forcing variables between given regions</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_relevant_cols--returns","title":"Returns","text":"<p>np.array     common forcing variables for given regions</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def get_relevant_cols(self):\n    \"\"\"\n    Only read common forcing variables between given regions\n\n    Returns\n    -------\n    np.array\n        common forcing variables for given regions\n    \"\"\"\n    return np.array(\n        [\"aet\", \"pet\", \"prcp\", \"srad\", \"swe\", \"tmax\", \"tmean\", \"tmin\", \"vp\"]\n    )\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_relevant_dict","title":"<code>get_relevant_dict()</code>","text":"<p>common forcing variables, get their real names in different datasets</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_relevant_dict--returns","title":"Returns","text":"<p>collections.OrderedDict     forcing variables for all regions</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def get_relevant_dict(self) -&gt; collections.OrderedDict:\n    \"\"\"\n    common forcing variables, get their real names in different datasets\n\n    Returns\n    -------\n    collections.OrderedDict\n        forcing variables for all regions\n    \"\"\"\n    forcing_dict = collections.OrderedDict({})\n    for i in range(len(self.regions)):\n        if self.regions[i] == \"AUS\":\n            # we use SILO data here\n            forcing_dict = collections.OrderedDict(\n                **forcing_dict,\n                **{\n                    self.regions[i]: [\n                        \"et_morton_actual_SILO\",\n                        \"et_morton_point_SILO\",\n                        \"precipitation_SILO\",\n                        \"radiation_SILO\",\n                        \"\",\n                        \"tmax_SILO\",\n                        \"\",\n                        \"tmin_SILO\",\n                        \"vp_SILO\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"BR\":\n            forcing_dict = collections.OrderedDict(\n                **forcing_dict,\n                **{\n                    self.regions[i]: [\n                        \"evapotransp_gleam\",\n                        \"potential_evapotransp_gleam\",\n                        \"precipitation_chirps\",\n                        \"\",\n                        \"\",\n                        \"temperature_max_cpc\",\n                        \"temperature_mean_cpc\",\n                        \"temperature_min_cpc\",\n                        \"\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"CL\":\n            forcing_dict = collections.OrderedDict(\n                **forcing_dict,\n                **{\n                    self.regions[i]: [\n                        \"\",\n                        \"pet_hargreaves\",\n                        \"precip_cr2met\",\n                        \"\",\n                        \"swe\",\n                        \"tmax_cr2met\",\n                        \"tmean_cr2met\",\n                        \"tmin_cr2met\",\n                        \"\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"GB\":\n            forcing_dict = collections.OrderedDict(\n                **forcing_dict,\n                **{\n                    self.regions[i]: [\n                        \"\",\n                        \"pet\",\n                        \"precipitation\",\n                        \"shortwave_rad\",\n                        \"\",\n                        \"\",\n                        \"temperature\",\n                        \"\",\n                        \"\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"US\":\n            forcing_dict = collections.OrderedDict(\n                **forcing_dict,\n                **{\n                    self.regions[i]: [\n                        \"\",\n                        \"PET\",\n                        \"prcp\",\n                        \"srad\",\n                        \"swe\",\n                        \"tmax\",\n                        \"\",\n                        \"tmin\",\n                        \"vp\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"YR\":\n            raise NotImplementedError(\n                \"Yellow river only provided normalized streamflow data, so please don't use it now.\"\n            )\n        elif self.regions[i] == \"CA\":\n            forcing_dict = collections.OrderedDict(\n                **forcing_dict,\n                **{\n                    self.regions[i]: [\n                        \"\",\n                        \"\",\n                        \"prcp\",\n                        \"\",\n                        \"\",\n                        \"tmax\",\n                        \"\",\n                        \"tmin\",\n                        \"\",\n                    ]\n                },\n            )\n        elif self.regions[i] == \"CE\":\n            forcing_dict = collections.OrderedDict(\n                **forcing_dict,\n                **{\n                    self.regions[i]: [\n                        \"total_et\",\n                        \"PET_A\",\n                        \"prec\",\n                        \"surf_net_solar_rad_mean\",\n                        \"swe\",\n                        \"2m_temp_max\",\n                        \"2m_temp_mean\",\n                        \"2m_temp_min\",\n                        \"\",\n                    ]\n                },\n            )\n        else:\n            raise NotImplementedError(CAMELS_NO_DATASET_ERROR_LOG)\n    return forcing_dict\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_target_cols","title":"<code>get_target_cols()</code>","text":"<p>Only read common target variables -- streamflow</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_target_cols--returns","title":"Returns","text":"<p>np.array     common target variables for given regions</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def get_target_cols(self) -&gt; np.array:\n    \"\"\"\n    Only read common target variables -- streamflow\n\n    Returns\n    -------\n    np.array\n        common target variables for given regions\n    \"\"\"\n    # unify all \"streamflow\" names to \"streamflow\"\n    return np.array([\"streamflow\"])\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_target_dict","title":"<code>get_target_dict()</code>","text":"<p>common target variable -- streamflow, get its real name in different datasets</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.get_target_dict--returns","title":"Returns","text":"<p>collections.OrderedDict     streamflow variables for all regions</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def get_target_dict(self) -&gt; collections.OrderedDict:\n    \"\"\"\n    common target variable -- streamflow, get its real name in different datasets\n\n    Returns\n    -------\n    collections.OrderedDict\n        streamflow variables for all regions\n    \"\"\"\n    streamflow_dict = collections.OrderedDict({})\n    for i in range(len(self.regions)):\n        if self.regions[i] == \"US\":\n            # its unit is ft3/s\n            streamflow_dict = collections.OrderedDict(\n                **streamflow_dict, **{self.regions[i]: \"usgsFlow\"}\n            )\n        elif self.regions[i] == \"AUS\":\n            # MLd means \"1 Megaliters Per Day\"; 1 MLd = 0.011574074074074 cubic-meters-per-second\n            streamflow_dict = collections.OrderedDict(\n                **streamflow_dict, **{self.regions[i]: \"streamflow_MLd\"}\n            )\n        elif self.regions[i] in [\"BR\", \"CL\"]:\n            streamflow_dict = collections.OrderedDict(\n                **streamflow_dict, **{self.regions[i]: \"streamflow_m3s\"}\n            )\n        elif self.regions[i] == \"GB\":\n            streamflow_dict = collections.OrderedDict(\n                **streamflow_dict, **{self.regions[i]: \"discharge_vol\"}\n            )\n        elif self.regions[i] == \"YR\":\n            raise NotImplementedError(\n                \"Yellow river only provided normalized streamflow data, so please don't use it now.\"\n            )\n        elif self.regions[i] == \"CA\":\n            # mm/day, remember to trans to m3/s\n            streamflow_dict = collections.OrderedDict(\n                **streamflow_dict, **{self.regions[i]: \"discharge\"}\n            )\n        elif self.regions[i] == \"CE\":\n            streamflow_dict = collections.OrderedDict(\n                **streamflow_dict, **{self.regions[i]: \"qobs\"}\n            )\n        else:\n            raise NotImplementedError(CAMELS_NO_DATASET_ERROR_LOG)\n    return streamflow_dict\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_constant_cols","title":"<code>read_constant_cols(object_ids=None, constant_cols=None, **kwargs)</code>","text":"<p>Read data of common attribute variables between given regions</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_constant_cols--parameters","title":"Parameters","text":"<p>object_ids     sites constant_cols     common attribute variables for given regions kwargs     optional parameters</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_constant_cols--returns","title":"Returns","text":"<p>np.array     attribute data</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def read_constant_cols(\n    self, object_ids=None, constant_cols: list = None, **kwargs\n) -&gt; np.array:\n    \"\"\"\n    Read data of common attribute variables between given regions\n\n    Parameters\n    ----------\n    object_ids\n        sites\n    constant_cols\n        common attribute variables for given regions\n    kwargs\n        optional parameters\n\n    Returns\n    -------\n    np.array\n        attribute data\n    \"\"\"\n    attrs = np.full((len(object_ids), len(constant_cols)), np.nan)\n    str_attr_dict = {}\n    for constant_col in constant_cols:\n        if constant_col in self.str_attr_name:\n            str_attr_dict = {\n                **str_attr_dict,\n                **{constant_col: np.empty(len(object_ids)).astype(str)},\n            }\n    for i in range(len(self.regions)):\n        region_now = self.regions[i]\n        dataset = DATASETS_DICT[self.datasets[i]](\n            self.data_path[i], False, region=region_now\n        )\n        region_attr_all = dataset.get_constant_cols()\n        common_attr_all = self.get_constant_cols()\n        j_index = [\n            common_attr_all.tolist().index(constant_col_)\n            for constant_col_ in constant_cols\n        ]\n        constant_dict_lst = [\n            {constant_cols[j]: self.attr_dict[region_now][j_index[j]]}\n            for j in range(len(constant_cols))\n        ]\n        constant_dict_map = reduce(\n            lambda x, y: collections.OrderedDict(**x, **y), constant_dict_lst\n        )\n        in_region_attr_tuple = [\n            (k, value)\n            for k, (key, value) in enumerate(constant_dict_map.items())\n            if value in region_attr_all\n        ]\n        not_in_region_attr_tuple = [\n            (k, value)\n            for k, (key, value) in enumerate(constant_dict_map.items())\n            if value not in region_attr_all\n        ]\n        sites_tuple = [\n            (k, object_ids[k])\n            for k in range(len(object_ids))\n            if object_ids[k] in self.site_region_dict[region_now]\n        ]\n        sites_in_this_region = [a_tuple[1] for a_tuple in sites_tuple]\n        in_region_attr = [index_attr[1] for index_attr in in_region_attr_tuple]\n        if not set(in_region_attr).issubset(set(region_attr_all)):\n            raise NotImplementedError(\n                \"Wrong name for attributes, please check your input for attributes\"\n            )\n        attrs1, var_dict, f_dict = dataset.read_constant_cols(\n            sites_in_this_region, in_region_attr, is_return_dict=True\n        )\n        attrs2 = self.read_not_included_attrs(\n            sites_in_this_region,\n            [index_attr[1] for index_attr in not_in_region_attr_tuple],\n            dataset,\n        )\n        attrs1_idx = [index_attr[0] for index_attr in in_region_attr_tuple]\n        attrs2_idx = [index_attr[0] for index_attr in not_in_region_attr_tuple]\n        sites_idx = [a_tuple[0] for a_tuple in sites_tuple]\n        attrs_this_region = np.empty(\n            (len(sites_idx), len(attrs1_idx) + len(attrs2_idx))\n        )\n        attrs_this_region[:, attrs1_idx] = attrs1\n        attrs_this_region[:, attrs2_idx] = attrs2\n        for j in range(len(constant_cols)):\n            # restore to str\n            if constant_cols[j] in self.str_attr_name:\n                # self.str_attr_name = [\"high_prec_timing\", \"low_prec_timing\", \"geol_1st_class\"]\n                restore_attr = np.array(\n                    [\n                        (\n                            \"\"\n                            if np.isnan(tmp)\n                            else f_dict[list(constant_dict_lst[j].values())[0]][\n                                int(tmp)\n                            ]\n                        )\n                        for tmp in attrs_this_region[:, j]\n                    ]\n                )\n                str_attr_dict[constant_cols[j]][sites_idx] = restore_attr\n            # unit of slope_mean in AUS is % while others are m/km\n            if region_now == \"AUS\" and constant_cols[j] == \"slope_mean\":\n                attrs_this_region[:, j] = attrs_this_region[:, j] * 10.0\n            # unit of soil_depth in BR is cm while others are m\n            if region_now == \"BR\" and constant_cols[j] == \"soil_depth\":\n                attrs_this_region[:, j] = attrs_this_region[:, j] / 100.0\n            # unit of soil_conductivity in AUS is mm/h while others are cm/h\n            if region_now == \"AUS\" and constant_cols[j] == \"soil_conductivity\":\n                attrs_this_region[:, j] = attrs_this_region[:, j] / 10.0\n            # frac of gc_dom in CE need to be calculated\n            if region_now == \"CE\" and constant_cols[j] == \"geol_1st_class_frac\":\n                geo_types_names_in_ce = [\n                    \"gc_ig_fra\",\n                    \"gc_mt_fra\",\n                    \"gc_pa_fra\",\n                    \"gc_pb_fra\",\n                    \"gc_pi_fra\",\n                    \"gc_py_fra\",\n                    \"gc_sc_fra\",\n                    \"gc_sm_fra\",\n                    \"gc_ss_fra\",\n                    \"gc_su_fra\",\n                    \"gc_va_fra\",\n                    \"gc_vb_fra\",\n                    \"gc_wb_fra\",\n                ]\n                geo_fracs = dataset.read_constant_cols(\n                    sites_in_this_region, geo_types_names_in_ce\n                )\n                attrs_this_region[:, j] = np.array(\n                    [\n                        geo_fracs[\n                            k,\n                            geo_types_names_in_ce.index(\n                                \"gc_\" + restore_attr[k] + \"_fra\"\n                            ),\n                        ]\n                        for k in range(len(restore_attr))\n                    ]\n                )\n            # the type of geol_1st_class in GB are set \"unknown\", so all fractions are set to 100%\n            if region_now == \"GB\" and constant_cols[j] == \"geol_1st_class_frac\":\n                attrs_this_region[:, j] = np.full(\n                    attrs_this_region[:, j].shape, 1.0\n                )\n        attrs[sites_idx, :] = attrs_this_region\n    for j in range(len(constant_cols)):\n        # trans str attr to number\n        if constant_cols[j] in self.str_attr_name:\n            if constant_cols[j] == \"geol_1st_class\":\n                geo_class_dict = {\n                    \"\": \"unknown\",\n                    \"Acid plutonic rocks\": \"pa\",\n                    \"Acid volcanic rocks\": \"va\",\n                    \"Basic plutonic rocks\": \"pb\",\n                    \"Basic volcanic rocks\": \"vb\",\n                    \"CARBNATESED\": \"sc\",\n                    \"Carbonate sedimentary rocks\": \"sc\",\n                    \"IGNEOUS\": \"igneous\",\n                    \"Ice and glaciers\": \"ig\",\n                    \"Intermediate plutonic rocks\": \"pi\",\n                    \"Intermediate volcanic rocks\": \"vi\",\n                    \"METAMORPH\": \"mt\",\n                    \"Metamorphics\": \"mt\",\n                    \"Mixed sedimentary rocks\": \"sm\",\n                    \"OTHERSED\": \"othersed\",\n                    \"Pyroclastics\": \"py\",\n                    \"SEDVOLC\": \"sm\",\n                    \"SILICSED\": \"ss\",\n                    \"Siliciclastic sedimentary rocks\": \"ss\",\n                    \"UNCONSOLDTED\": \"su\",\n                    \"Unconsolidated sediments\": \"su\",\n                    \"Water bodies\": \"wb\",\n                    \"acid_plutonic_rocks\": \"pa\",\n                    \"acid_volcanic_rocks\": \"va\",\n                    \"basic_volcanic_rocks\": \"vb\",\n                    \"carbonate_sedimentary_rocks\": \"sc\",\n                    \"intermediate_volcanic_rocks\": \"vi\",\n                    \"metamorphics\": \"mt\",\n                    \"mixed_sedimentary_rocks\": \"sm\",\n                    \"mt\": \"mt\",\n                    \"pa\": \"pa\",\n                    \"pi\": \"pi\",\n                    \"pyroclastics\": \"py\",\n                    \"sc\": \"sc\",\n                    \"siliciclastic_sedimentary_rocks\": \"ss\",\n                    \"sm\": \"sm\",\n                    \"ss\": \"ss\",\n                    \"su\": \"su\",\n                    \"unconsolidated_sediments\": \"su\",\n                    \"vb\": \"vb\",\n                }\n                str_attr_dict[constant_cols[j]] = np.array(\n                    [geo_class_dict[tmp] for tmp in str_attr_dict[constant_cols[j]]]\n                )\n            value, ref = pd.factorize(str_attr_dict[constant_cols[j]], sort=True)\n            attrs[:, j] = value\n\n    return attrs\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_not_included_attrs","title":"<code>read_not_included_attrs(sites_id, attr_cols, dataset)</code>  <code>staticmethod</code>","text":"<p>read attributes not included in the dataset of the region</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_not_included_attrs--parameters","title":"Parameters","text":"<p>sites_id</p> <p>attr_cols</p> <p>dataset     a dataset class instance</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_not_included_attrs--returns","title":"Returns","text":"<p>np.array     attrs not included in original dataset</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>@staticmethod\ndef read_not_included_attrs(sites_id, attr_cols, dataset) -&gt; np.array:\n    \"\"\"\n    read attributes not included in the dataset of the region\n\n    Parameters\n    ----------\n    sites_id\n\n    attr_cols\n\n    dataset\n        a dataset class instance\n\n\n    Returns\n    -------\n    np.array\n        attrs not included in original dataset\n    \"\"\"\n    attr_data = np.empty([len(sites_id), len(attr_cols)])\n    for i in range(len(attr_cols)):\n        if attr_cols[i] == \"\":\n            attr_data[:, i] = np.full(len(sites_id), np.nan)\n        elif (\n            attr_cols[i] == \"dwood_perc+ewood_perc+shrub_perc\"\n            and dataset.get_name() == \"CAMELS_GB\"\n        ):\n            three_perc = dataset.read_constant_cols(\n                sites_id, [\"dwood_perc\", \"ewood_perc\", \"shrub_perc\"]\n            )\n            attr_data[:, i] = np.sum(three_perc, axis=1)\n    return attr_data\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_not_included_forcings","title":"<code>read_not_included_forcings(sites_id, t_range_list, forcing_cols, dataset)</code>  <code>staticmethod</code>","text":"<p>read forcings not included in the dataset of the region</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_not_included_forcings--parameters","title":"Parameters","text":"<p>sites_id</p> <p>t_range_list</p> <p>forcing_cols</p> <p>dataset     data source</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_not_included_forcings--returns","title":"Returns","text":"<p>np.array     forcings not included in original dataset</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>@staticmethod\ndef read_not_included_forcings(\n    sites_id, t_range_list, forcing_cols, dataset\n) -&gt; np.array:\n    \"\"\"\n    read forcings not included in the dataset of the region\n\n    Parameters\n    ----------\n    sites_id\n\n    t_range_list\n\n    forcing_cols\n\n    dataset\n        data source\n\n\n    Returns\n    -------\n    np.array\n        forcings not included in original dataset\n    \"\"\"\n    nt = hydro_time.t_range_days(t_range_list).shape[0]\n    forcing_data = np.empty([len(sites_id), nt, len(forcing_cols)])\n    for i in range(len(forcing_cols)):\n        if forcing_cols[i] == \"\":\n            forcing_data[:, :, i] = np.full((len(sites_id), nt), np.nan)\n        elif forcing_cols[i] == \"PET\" and dataset.get_name() == \"CAMELS_US\":\n            forcing_data[:, :, i : i + 1] = (\n                dataset.read_camels_us_model_output_data(\n                    sites_id, t_range_list, [\"PET\"]\n                )\n            )\n        elif forcing_cols[i] == \"PET_A\" and dataset.get_name() == \"LamaH_CE\":\n            forcing_data[:, :, i : i + 1] = (\n                dataset.read_lamah_hydro_model_time_series(\n                    sites_id, t_range_list, [\"PET_A\"]\n                )\n            )\n    return forcing_data\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_relevant_cols","title":"<code>read_relevant_cols(object_ids=None, t_range_list=None, relevant_cols=None, **kwargs)</code>","text":"<p>Read data of common forcing variables between given regions</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_relevant_cols--parameters","title":"Parameters","text":"<p>object_ids     sites t_range_list     time range relevant_cols     common forcing variables for given regions kwargs     optional parameters</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_relevant_cols--returns","title":"Returns","text":"<p>np.array     forcing data</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def read_relevant_cols(\n    self, object_ids=None, t_range_list=None, relevant_cols=None, **kwargs\n) -&gt; np.array:\n    \"\"\"\n    Read data of common forcing variables between given regions\n\n    Parameters\n    ----------\n    object_ids\n        sites\n    t_range_list\n        time range\n    relevant_cols\n        common forcing variables for given regions\n    kwargs\n        optional parameters\n\n    Returns\n    -------\n    np.array\n        forcing data\n    \"\"\"\n    nt = hydro_time.t_range_days(t_range_list).shape[0]\n    forcings = np.full((len(object_ids), nt, len(relevant_cols)), np.nan)\n    for i in range(len(self.regions)):\n        region_now = self.regions[i]\n        dataset = DATASETS_DICT[self.datasets[i]](\n            self.data_path[i], False, region=region_now\n        )\n        region_forcing_all = dataset.get_relevant_cols()\n        common_forcing_all = self.get_relevant_cols()\n        j_index = [\n            common_forcing_all.tolist().index(relevant_cols[ii])\n            for ii in range(len(relevant_cols))\n        ]\n        forcing_dict_lst = [\n            {relevant_cols[j]: self.forcing_dict[region_now][j_index[j]]}\n            for j in range(len(relevant_cols))\n        ]\n        forcing_dict_map = reduce(\n            lambda x, y: collections.OrderedDict(**x, **y), forcing_dict_lst\n        )\n        in_region_forcing_tuple = [\n            (k, value)\n            for k, (key, value) in enumerate(forcing_dict_map.items())\n            if value in region_forcing_all\n        ]\n        not_in_region_forcing_tuple = [\n            (k, value)\n            for k, (key, value) in enumerate(forcing_dict_map.items())\n            if value not in region_forcing_all\n        ]\n        sites_tuple = [\n            (k, object_ids[k])\n            for k in range(len(object_ids))\n            if object_ids[k] in self.site_region_dict[region_now]\n        ]\n        sites_in_this_region = [a_tuple[1] for a_tuple in sites_tuple]\n        in_region_forcing = [\n            index_attr[1] for index_attr in in_region_forcing_tuple\n        ]\n        forcing1 = dataset.read_relevant_cols(\n            sites_in_this_region, t_range_list, in_region_forcing\n        )\n        forcing2 = self.read_not_included_forcings(\n            sites_in_this_region,\n            t_range_list,\n            [index_attr[1] for index_attr in not_in_region_forcing_tuple],\n            dataset,\n        )\n        forcing1_idx = [\n            index_forcing[0] for index_forcing in in_region_forcing_tuple\n        ]\n        forcing2_idx = [\n            index_forcing[0] for index_forcing in not_in_region_forcing_tuple\n        ]\n        sites_idx = [a_tuple[0] for a_tuple in sites_tuple]\n        forcing_this_region = np.empty(\n            (len(sites_idx), nt, len(forcing1_idx) + len(forcing2_idx))\n        )\n        forcing_this_region[:, :, forcing1_idx] = forcing1\n        forcing_this_region[:, :, forcing2_idx] = forcing2\n        for j in range(len(relevant_cols)):\n            # unit of srad in AUS is MJ/m2(/day) while others are W/m2\n            if region_now == \"AUS\" and relevant_cols[j] == \"srad\":\n                forcing_this_region[:, :, j] = (\n                    forcing_this_region[:, :, j] * 1e6 / (24 * 3600)\n                )\n            # unit of vp in AUS is hPa while others are Pa\n            if region_now == \"AUS\" and relevant_cols[j] == \"vp\":\n                forcing_this_region[:, :, j] = forcing_this_region[:, :, j] * 100\n        forcings[sites_idx, :, :] = forcing_this_region\n    return forcings\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_site_info","title":"<code>read_site_info()</code>","text":"<p>Read the basic information of gages in multiple datasets</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_site_info--returns","title":"Returns","text":"<p>collections.OrderedDict     basic info of gages for different regions</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def read_site_info(self) -&gt; collections.OrderedDict:\n    \"\"\"\n    Read the basic information of gages in multiple datasets\n\n    Returns\n    -------\n    collections.OrderedDict\n        basic info of gages for different regions\n    \"\"\"\n    site_info = collections.OrderedDict({})\n    for i in range(len(self.datasets)):\n        region = DATASETS_DICT[self.datasets[i]](\n            self.data_path[i], False, region=self.regions[i]\n        )\n        site_info = collections.OrderedDict(\n            **site_info, **{self.regions[i]: region.read_site_info()}\n        )\n    return site_info\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_target_cols","title":"<code>read_target_cols(object_ids=None, t_range_list=None, target_cols=None, **kwargs)</code>","text":"<p>Read streamflow data</p> <p>We unify the unit of streamflow to ft3/s because first our data interface used in models are set for CAMELS-US.</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_target_cols--parameters","title":"Parameters","text":"<p>object_ids     sites t_range_list     time range target_cols     streamflow variable kwargs     optional parameters</p>"},{"location":"hydrodataset/#hydrodataset.MultiDatasets.read_target_cols--returns","title":"Returns","text":"<p>np.array     streamflow data for given sites</p> Source code in <code>hydrodataset/multi_datasets.py</code> <pre><code>def read_target_cols(\n    self, object_ids=None, t_range_list=None, target_cols=None, **kwargs\n) -&gt; np.array:\n    \"\"\"\n    Read streamflow data\n\n    We unify the unit of streamflow to ft3/s because first our data interface used in models are set for CAMELS-US.\n\n    Parameters\n    ----------\n    object_ids\n        sites\n    t_range_list\n        time range\n    target_cols\n        streamflow variable\n    kwargs\n        optional parameters\n\n    Returns\n    -------\n    np.array\n        streamflow data for given sites\n    \"\"\"\n    nt = hydro_time.t_range_days(t_range_list).shape[0]\n    flow = np.full((len(object_ids), nt, len(target_cols)), np.nan)\n    for i in range(len(self.regions)):\n        region_now = self.regions[i]\n        dataset = DATASETS_DICT[self.datasets[i]](\n            self.data_path[i], False, region=region_now\n        )\n        flow_dict_map = {target_cols[0]: self.streamflow_dict[region_now]}\n        in_region_flow_tuple = [\n            (k, value) for k, (key, value) in enumerate(flow_dict_map.items())\n        ]\n        sites_tuple = [\n            (k, object_ids[k])\n            for k in range(len(object_ids))\n            if object_ids[k] in self.site_region_dict[region_now]\n        ]\n        sites_in_this_region = [a_tuple[1] for a_tuple in sites_tuple]\n        in_region_flow = [index_attr[1] for index_attr in in_region_flow_tuple]\n        flow_this_region = dataset.read_target_cols(\n            sites_in_this_region, t_range_list, in_region_flow\n        )\n        sites_idx = [a_tuple[0] for a_tuple in sites_tuple]\n        flow[sites_idx, :, :] = flow_this_region\n    return flow\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.StandardVariable","title":"<code>StandardVariable</code>","text":"<p>A class to hold standardized variable names as constants.</p> Source code in <code>hydrodataset/hydro_dataset.py</code> <pre><code>class StandardVariable:\n    \"\"\"A class to hold standardized variable names as constants.\"\"\"\n\n    STREAMFLOW = \"streamflow\"\n    WATER_LEVEL = \"water_level\"\n\n    PRECIPITATION = \"precipitation\"\n    CRAINF_FRAC = \"crainf_frac\"  # Fraction of total precipitation that is convective\n\n    TEMPERATURE_MAX = \"temperature_max\"\n    TEMPERATURE_MIN = \"temperature_min\"\n    TEMPERATURE_MEAN = \"temperature_mean\"\n\n    DAYLIGHT_DURATION = \"daylight_duration\"\n    RELATIVE_DAYLIGHT_DURATION = \"relative_daylight_duration\"\n\n    SOLAR_RADIATION = \"solar_radiation\"\n    LONGWAVE_SOLAR_RADIATION = \"longwave_solar_radiation\"\n\n    SNOW_WATER_EQUIVALENT = \"snow_water_equivalent\"\n\n    VAPOR_PRESSURE = \"vapor_pressure\"\n\n    SURFACE_PRESSURE = \"surface_pressure\"\n\n    WIND_SPEED = \"wind_speed\"\n    MERIDIONAL_WIND_SPEED = \"meridional_wind_speed\"\n\n    RELATIVE_HUMIDITY = \"relative_humidity\"\n    RELATIVE_HUMIDITY_MIN = \"relative_humidity_min\"\n\n    CAPE = \"cape\"  # Convective available potential energy\n\n    POTENTIAL_EVAPOTRANSPIRATION = \"potential_evapotranspiration\"\n    EVAPORATION = \"evaporation\"\n    EVAPOTRANSPIRATION = \"evapotranspiration\"\n</code></pre>"},{"location":"hydrodataset/#hydrodataset.time_intersect_dynamic_data","title":"<code>time_intersect_dynamic_data(obs, date, t_range)</code>","text":"<p>chose data from obs in the t_range</p>"},{"location":"hydrodataset/#hydrodataset.time_intersect_dynamic_data--parameters","title":"Parameters","text":"<p>obs     a np array date     all periods for obs t_range     the time range we need, such as [\"1990-01-01\",\"2000-01-01\"]</p>"},{"location":"hydrodataset/#hydrodataset.time_intersect_dynamic_data--returns","title":"Returns","text":"<p>np.array     the chosen data</p> Source code in <code>hydrodataset/camels.py</code> <pre><code>def time_intersect_dynamic_data(obs: np.array, date: np.array, t_range: list):\n    \"\"\"\n    chose data from obs in the t_range\n\n    Parameters\n    ----------\n    obs\n        a np array\n    date\n        all periods for obs\n    t_range\n        the time range we need, such as [\"1990-01-01\",\"2000-01-01\"]\n\n    Returns\n    -------\n    np.array\n        the chosen data\n    \"\"\"\n    t_lst = pd.date_range(start=t_range[0], end=t_range[1], freq=\"D\").values\n    nt = t_lst.shape[0]\n    if len(obs) != nt:\n        out = np.full([nt], np.nan)\n        [c, ind1, ind2] = np.intersect1d(date, t_lst, return_indices=True)\n        out[ind2] = obs[ind1]\n    else:\n        out = obs\n    return out\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install HydroDataset, run this command in your terminal:</p> <pre><code>pip install hydrodataset\n</code></pre> <p>This is the preferred method to install HydroDataset, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>The sources for HydroDataset can be downloaded from the Github repo.</p> <p>You can clone the public repository:</p> <pre><code>git clone git://github.com/OuyangWenyu/hydrodataset\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use HydroDataset in a project:</p> <pre><code>import hydrodataset\n</code></pre>"}]}